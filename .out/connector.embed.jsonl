{"id":"n_00d8addb8c320e6f","kind":"connector","name":"Vertex AI","fqname":"connector.Vertex AI","loc":{"line":1,"column":0,"length":139797,"begin":0,"end":139797},"file":"connector.rb","keys":["actions","connection","custom_action","custom_action_help","methods","object_definitions","pick_lists","test","title","triggers","version"],"http":{},"text":"{\n  title: 'Vertex AI',\n  version: '0.7.0',\n  \n  # ============================================\n  # CONNECTION & AUTHENTICATION\n  # ============================================\n  connection: {\n    fields: [\n      # Authentication type\n      { name: 'auth_type', label: 'Authentication type', group: 'Authentication', control_type: 'select', default: 'custom',\n        optional: false, extends_schema: true, hint: 'Select the authentication type for connecting to Google Vertex AI.',\n        options: [ ['Service account (JWT)', 'custom'], ['OAuth 2.0 (Auth code)', 'oauth2'] ]},\n      # Google Cloud Configuration\n      { name: 'project', label: 'Project ID', group: 'Google Cloud Platform', optional: false },\n      { name: 'region',  label: 'Region',     group: 'Google Cloud Platform', optional: false, control_type: 'select', \n        options: [\n          ['Global', 'global'],\n          ['US central 1', 'us-central1'],\n          ['US east 1', 'us-east1'],\n          ['US east 4', 'us-east4'],\n          ['US east 5', 'us-east5'],\n          ['US west 1', 'us-west1'],\n          ['US west 4', 'us-west4'],\n          ['US south 1', 'us-south1'],\n        ],\n        hint: 'Vertex AI region for model execution.', toggle_hint: 'Select from list',\n        toggle_field: {\n          name: 'region', label: 'Region', type: 'string', control_type: 'text', optional: false,\n          toggle_hint: 'Use custom value', hint: \"See Vertex AI locations docs for allowed regions.\" } },\n      { name: 'version', label: 'API version', group: 'Google Cloud Platform', optional: false, default: 'v1', hint: 'e.g. v1beta1' },\n      \n      # Optional Configurations\n      { name: 'vector_search_endpoint', label: 'Vector Search Endpoint', optional: true, hint: 'Public Vector Search domain host for queries' },\n      \n      # Default Behaviors\n      { name: 'default_model', label: 'Default Model', control_type: 'select', optional: true,\n        options: [\n          ['Gemini 1.5 Flash', 'gemini-1.5-flash'],\n          ['Gemini 1.5 Pro',   'gemini-1.5-pro'],\n          ['Text Embedding 004', 'text-embedding-004'],\n          ['Text Embedding Gecko', 'textembedding-gecko']\n        ] },\n      { name: 'optimization_mode', label: 'Optimization Mode', control_type: 'select', default: 'balanced',\n        options: [['Balanced', 'balanced'], ['Cost', 'cost'], ['Performance', 'performance']] },\n      { name: 'enable_caching', label: 'Enable Response Caching', control_type: 'checkbox', default: true },\n      { name: 'enable_logging', label: 'Enable Debug Logging', control_type: 'checkbox', default: false },\n      # Allow admin discovery\n      { name: 'allow_admin_discovery', label: 'Allow admin discovery of index config', group: 'Advanced', control_type: 'checkbox', default: false,\n        hint: 'When enabled, the connector may read Index/IndexEndpoint metadata to compute confidence.' }\n    ],\n    \n    authorization: {\n      type: 'multi',\n      selected: lambda do |connection|\n        connection['auth_type'] || 'custom'\n      end,\n      identity: lambda do |connection|\n        selected = connection['auth_type'] || 'custom'\n        if selected == 'oauth2'\n          begin\n            info = call('http_request',\n              connection,\n              method: 'GET',\n              url: 'https://openidconnect.googleapis.com/v1/userinfo',\n              headers: {}, # Authorization comes from apply()\n              retry_config: { max_attempts: 2, backoff: 0.5, retry_on: [429,500,502,503,504] }\n            )\n            email = info['email'] || '(no email)'\n            name  = info['name']\n            sub   = info['sub']\n            [name, email, sub].compact.join(' / ')\n          rescue\n            'OAuth2 (Google) – identity unavailable'\n          end\n        else\n          connection['service_account_email']\n        end\n      end,\n      options: {\n        oauth2: {\n          type: 'oauth2',\n          fields: [\n            { name: 'client_id', label: 'Client ID', group: 'OAuth 2.0', optional: false },\n            { name: 'client_secret', label: 'Client Secret', group: 'OAuth 2.0', optional: false, control_type: 'password' },\n            { name: 'oauth_refresh_token_ttl', label: 'Refresh token TTL (seconds)', group: 'OAuth 2.0', type: 'integer', optional: true,\n              hint: 'Used only if Google does not return refresh_token_expires_in; enables background refresh.' }\n          ],\n          # AUTH URL\n          authorization_url: lambda do |connection|\n            scopes = [\n              'https://www.googleapis.com/auth/cloud-platform',\n              'openid', 'email', 'profile' # needed for /userinfo claims\n            ].join(' ')\n\n            params = {\n              client_id: connection['client_id'],\n              response_type: 'code',\n              scope: scopes,\n              access_type: 'offline',\n              include_granted_scopes: 'true',\n              prompt: 'consent'\n            }\n\n            qs = call('to_query', params)\n            \"https://accounts.google.com/o/oauth2/v2/auth?#{qs}\"\n          end,\n          # ACQUIRE\n          acquire: lambda do |connection, auth_code|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'authorization_code',\n                code: auth_code,\n                redirect_uri: 'https://www.workato.com/oauth/callback'\n              },\n              headers: { },                 # no X-Goog-User-Project on token exchange\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            body = resp # JSON Hash\n            ttl = body['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n\n            [\n              {\n                access_token: body['access_token'],\n                refresh_token: body['refresh_token'],\n                refresh_token_expires_in: ttl\n              },\n              nil,\n              {}\n            ]\n          end,\n\n          # REFRESH\n          refresh: lambda do |connection, refresh_token|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'refresh_token',\n                refresh_token: refresh_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            {\n              access_token: resp['access_token'],\n              refresh_token: resp['refresh_token'],\n              refresh_token_expires_in: resp['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n            }.compact\n          end,\n\n          # APPLY\n          apply: lambda do |_connection, access_token|\n            headers(Authorization: \"Bearer #{access_token}\")\n          end\n        },\n        custom: {\n          type: 'custom_auth',\n          fields: [\n            { name: 'service_account_email', label: 'Service Account Email', group: 'Service Account', optional: false },\n            { name: 'client_id', label: 'Client ID', group: 'Service Account', optional: false },\n            { name: 'private_key_id', label: 'Private Key ID', group: 'Service Account', optional: false },\n            { name: 'private_key', label: 'Private Key', group: 'Service Account', optional: false, multiline: true, control_type: 'password' }\n          ],\n          acquire: lambda do |connection|\n            issued_at = Time.now.to_i\n            jwt_body_claim = {\n              'iat' => issued_at,\n              'exp' => issued_at + 3600,\n              'aud' => 'https://oauth2.googleapis.com/token',\n              'iss' => connection['service_account_email'],\n              'scope' => 'https://www.googleapis.com/auth/cloud-platform'\n            }\n            private_key = connection['private_key'].to_s.gsub('\\\\n', \"\\n\")\n            jwt_token   = workato.jwt_encode(jwt_body_claim, private_key, 'RS256', kid: connection['private_key_id'])\n\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n                assertion: jwt_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            { access_token: resp['access_token'], expires_at: (Time.now + resp['expires_in'].to_i).iso8601 }\n          end,\n          refresh_on: [401],\n          apply: lambda do |connection|\n            headers(Authorization: \"Bearer #{connection['access_token']}\")\n          end\n        }\n      }\n    },\n    \n    base_uri: lambda do |connection|\n      ver = connection['version']\n      reg = connection['region']\n\n      version = (ver && !ver.to_s.strip.empty?) ? ver.to_s : 'v1'\n      region = (reg && !reg.to_s.strip.empty?) ? reg.to_s : 'us-east4'\n\n      host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n      \"https://#{host}/#{version}/\"\n    end\n  },\n  \n  test: lambda do |connection|\n    project = connection['project']\n    region  = connection['region']\n\n    # 1) Token + API enablement (global catalog)\n    call('list_publisher_models', connection) # raises normalized errors\n\n    # 2) Regional reachability / permissions\n    url = call('build_endpoint_url', connection,\n      { 'custom_path' => \"https://{region}-aiplatform.googleapis.com/#{connection['version']}/projects/{project}/locations/{region}/endpoints\" },\n      {}\n    )\n    call('http_request', connection, method: 'GET', url: url, headers: call('build_headers', connection))\n\n    true\n  rescue => e\n    # Keep the normalized, compact message as‑is\n    error(e.message)\n  end,\n\n  # ============================================\n  # ACTIONS\n  # ============================================\n  # Listed alphabetically within each subsection.\n  actions: {\n\n    # ------------------------------------------\n    # CORE\n    # ------------------------------------------\n    # Batch Operation Action\n    batch_operation: {\n      title: 'UNIVERSAL - Batch AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'batchable_behaviors', optional: false },\n        { name: 'batch_strategy', label: 'Batch Strategy', control_type: 'select', default: 'count', options: [['By Count', 'count'], ['By Token Limit', 'tokens']] },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens ≈ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n      end,\n      # SAMPLE\n      sample_output: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n      end\n    },\n    # Universal Action\n    vertex_operation: {\n      title: 'UNIVERSAL - Vertex AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'available_behaviors', optional: false, extends_schema: true,\n          hint: 'Select the AI operation to perform' },\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select',\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          default: 'auto', optional: false, sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.'\n        },\n        { name: 'model',\n          label: 'Model',\n          group: 'Model & tuning',\n          control_type: 'select',\n          sticky: true,\n          optional: true,\n          extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"',\n          pick_list: 'models_dynamic_for_behavior', \n          pick_list_params: { behavior: 'behavior' },  # bind by field by NAME\n          toggle_hint: 'Select from list',\n          toggle_field: {\n            name: 'model',\n            label: 'Model (custom id)',\n            type: 'string',\n            control_type: 'text',\n            optional: true,\n            toggle_hint: 'Provide custom "}
{"id":"n_ed4ab15fd71232d6","kind":"connection","name":"connection","fqname":"connector.Vertex AI/connection.connection","loc":{"line":8,"column":14,"length":9433,"begin":191,"end":9624},"file":null,"keys":[],"http":{},"text":"{\n    fields: [\n      # Authentication type\n      { name: 'auth_type', label: 'Authentication type', group: 'Authentication', control_type: 'select', default: 'custom',\n        optional: false, extends_schema: true, hint: 'Select the authentication type for connecting to Google Vertex AI.',\n        options: [ ['Service account (JWT)', 'custom'], ['OAuth 2.0 (Auth code)', 'oauth2'] ]},\n      # Google Cloud Configuration\n      { name: 'project', label: 'Project ID', group: 'Google Cloud Platform', optional: false },\n      { name: 'region',  label: 'Region',     group: 'Google Cloud Platform', optional: false, control_type: 'select', \n        options: [\n          ['Global', 'global'],\n          ['US central 1', 'us-central1'],\n          ['US east 1', 'us-east1'],\n          ['US east 4', 'us-east4'],\n          ['US east 5', 'us-east5'],\n          ['US west 1', 'us-west1'],\n          ['US west 4', 'us-west4'],\n          ['US south 1', 'us-south1'],\n        ],\n        hint: 'Vertex AI region for model execution.', toggle_hint: 'Select from list',\n        toggle_field: {\n          name: 'region', label: 'Region', type: 'string', control_type: 'text', optional: false,\n          toggle_hint: 'Use custom value', hint: \"See Vertex AI locations docs for allowed regions.\" } },\n      { name: 'version', label: 'API version', group: 'Google Cloud Platform', optional: false, default: 'v1', hint: 'e.g. v1beta1' },\n      \n      # Optional Configurations\n      { name: 'vector_search_endpoint', label: 'Vector Search Endpoint', optional: true, hint: 'Public Vector Search domain host for queries' },\n      \n      # Default Behaviors\n      { name: 'default_model', label: 'Default Model', control_type: 'select', optional: true,\n        options: [\n          ['Gemini 1.5 Flash', 'gemini-1.5-flash'],\n          ['Gemini 1.5 Pro',   'gemini-1.5-pro'],\n          ['Text Embedding 004', 'text-embedding-004'],\n          ['Text Embedding Gecko', 'textembedding-gecko']\n        ] },\n      { name: 'optimization_mode', label: 'Optimization Mode', control_type: 'select', default: 'balanced',\n        options: [['Balanced', 'balanced'], ['Cost', 'cost'], ['Performance', 'performance']] },\n      { name: 'enable_caching', label: 'Enable Response Caching', control_type: 'checkbox', default: true },\n      { name: 'enable_logging', label: 'Enable Debug Logging', control_type: 'checkbox', default: false },\n      # Allow admin discovery\n      { name: 'allow_admin_discovery', label: 'Allow admin discovery of index config', group: 'Advanced', control_type: 'checkbox', default: false,\n        hint: 'When enabled, the connector may read Index/IndexEndpoint metadata to compute confidence.' }\n    ],\n    \n    authorization: {\n      type: 'multi',\n      selected: lambda do |connection|\n        connection['auth_type'] || 'custom'\n      end,\n      identity: lambda do |connection|\n        selected = connection['auth_type'] || 'custom'\n        if selected == 'oauth2'\n          begin\n            info = call('http_request',\n              connection,\n              method: 'GET',\n              url: 'https://openidconnect.googleapis.com/v1/userinfo',\n              headers: {}, # Authorization comes from apply()\n              retry_config: { max_attempts: 2, backoff: 0.5, retry_on: [429,500,502,503,504] }\n            )\n            email = info['email'] || '(no email)'\n            name  = info['name']\n            sub   = info['sub']\n            [name, email, sub].compact.join(' / ')\n          rescue\n            'OAuth2 (Google) – identity unavailable'\n          end\n        else\n          connection['service_account_email']\n        end\n      end,\n      options: {\n        oauth2: {\n          type: 'oauth2',\n          fields: [\n            { name: 'client_id', label: 'Client ID', group: 'OAuth 2.0', optional: false },\n            { name: 'client_secret', label: 'Client Secret', group: 'OAuth 2.0', optional: false, control_type: 'password' },\n            { name: 'oauth_refresh_token_ttl', label: 'Refresh token TTL (seconds)', group: 'OAuth 2.0', type: 'integer', optional: true,\n              hint: 'Used only if Google does not return refresh_token_expires_in; enables background refresh.' }\n          ],\n          # AUTH URL\n          authorization_url: lambda do |connection|\n            scopes = [\n              'https://www.googleapis.com/auth/cloud-platform',\n              'openid', 'email', 'profile' # needed for /userinfo claims\n            ].join(' ')\n\n            params = {\n              client_id: connection['client_id'],\n              response_type: 'code',\n              scope: scopes,\n              access_type: 'offline',\n              include_granted_scopes: 'true',\n              prompt: 'consent'\n            }\n\n            qs = call('to_query', params)\n            \"https://accounts.google.com/o/oauth2/v2/auth?#{qs}\"\n          end,\n          # ACQUIRE\n          acquire: lambda do |connection, auth_code|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'authorization_code',\n                code: auth_code,\n                redirect_uri: 'https://www.workato.com/oauth/callback'\n              },\n              headers: { },                 # no X-Goog-User-Project on token exchange\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            body = resp # JSON Hash\n            ttl = body['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n\n            [\n              {\n                access_token: body['access_token'],\n                refresh_token: body['refresh_token'],\n                refresh_token_expires_in: ttl\n              },\n              nil,\n              {}\n            ]\n          end,\n\n          # REFRESH\n          refresh: lambda do |connection, refresh_token|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'refresh_token',\n                refresh_token: refresh_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            {\n              access_token: resp['access_token'],\n              refresh_token: resp['refresh_token'],\n              refresh_token_expires_in: resp['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n            }.compact\n          end,\n\n          # APPLY\n          apply: lambda do |_connection, access_token|\n            headers(Authorization: \"Bearer #{access_token}\")\n          end\n        },\n        custom: {\n          type: 'custom_auth',\n          fields: [\n            { name: 'service_account_email', label: 'Service Account Email', group: 'Service Account', optional: false },\n            { name: 'client_id', label: 'Client ID', group: 'Service Account', optional: false },\n            { name: 'private_key_id', label: 'Private Key ID', group: 'Service Account', optional: false },\n            { name: 'private_key', label: 'Private Key', group: 'Service Account', optional: false, multiline: true, control_type: 'password' }\n          ],\n          acquire: lambda do |connection|\n            issued_at = Time.now.to_i\n            jwt_body_claim = {\n              'iat' => issued_at,\n              'exp' => issued_at + 3600,\n              'aud' => 'https://oauth2.googleapis.com/token',\n              'iss' => connection['service_account_email'],\n              'scope' => 'https://www.googleapis.com/auth/cloud-platform'\n            }\n            private_key = connection['private_key'].to_s.gsub('\\\\n', \"\\n\")\n            jwt_token   = workato.jwt_encode(jwt_body_claim, private_key, 'RS256', kid: connection['private_key_id'])\n\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n                assertion: jwt_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            { access_token: resp['access_token'], expires_at: (Time.now + resp['expires_in'].to_i).iso8601 }\n          end,\n          refresh_on: [401],\n          apply: lambda do |connection|\n            headers(Authorization: \"Bearer #{connection['access_token']}\")\n          end\n        }\n      }\n    },\n    \n    base_uri: lambda do |connection|\n      ver = connection['version']\n      reg = connection['region']\n\n      version = (ver && !ver.to_s.strip.empty?) ? ver.to_s : 'v1'\n      region = (reg && !reg.to_s.strip.empty?) ? reg.to_s : 'us-east4'\n\n      host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n      \"https://#{host}/#{version}/\"\n    end\n "}
{"id":"n_46f92993f54caddc","kind":"test","name":"test","fqname":"connector.Vertex AI/test.test","loc":{"line":224,"column":8,"length":678,"begin":9637,"end":10315},"file":null,"keys":[],"http":{},"text":": lambda do |connection|\n    project = connection['project']\n    region  = connection['region']\n\n    # 1) Token + API enablement (global catalog)\n    call('list_publisher_models', connection) # raises normalized errors\n\n    # 2) Regional reachability / permissions\n    url = call('build_endpoint_url', connection,\n      { 'custom_path' => \"https://{region}-aiplatform.googleapis.com/#{connection['version']}/projects/{project}/locations/{region}/endpoints\" },\n      {}\n    )\n    call('http_request', connection, method: 'GET', url: url, headers: call('build_headers', connection))\n\n    true\n  rescue => e\n    # Keep the normalized, compact message as‑is\n    error(e.message)\n "}
{"id":"n_ee3b8428eaa7274f","kind":"methods","name":"methods","fqname":"connector.Vertex AI/methods.methods","loc":{"line":811,"column":11,"length":95394,"begin":37408,"end":132802},"file":null,"keys":[],"http":{},"text":"\n  methods: {\n    # ============================================\n    # LAYER 1: CORE METHODS (Foundation)\n    # ============================================\n    \n    # Payload Building\n    build_payload: lambda do |template:, variables:, format:|\n      case format\n      \n      # Direct\n      when 'direct'\n        variables\n      \n      # Template\n      when 'template'\n        result = template.dup\n        variables.each { |k, v| result = result.gsub(\"{#{k}}\", v.to_s) }\n        result\n      \n      # Vertex prompt\n      when 'vertex_prompt'\n        payload = {\n          'contents' => [{\n            'role'  => 'user',\n            'parts' => [{ 'text' => call('apply_template', template, variables) }]\n          }],\n          'generationConfig' => call('build_generation_config', variables)\n        }.compact\n\n        sys = variables['system']\n        payload['systemInstruction'] = { 'parts' => [{ 'text' => sys }] } if sys && !sys.to_s.strip.empty?\n\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        # JSON mode (optional)\n        if variables['response_mime_type'] || variables['response_schema']\n          gc = (payload['generationConfig'] ||= {})\n          gc['responseMimeType'] = variables['response_mime_type'] if variables['response_mime_type']\n          gc['responseSchema']   = variables['response_schema']     if variables['response_schema']\n        end\n\n        payload['labels'] = variables['labels'] if variables['labels']\n        payload\n\n      when 'vertex_contents'\n        contents = Array(variables['contents']).map do |c|\n          role = c['role'] || c[:role] || 'user'\n          parts = Array(c['parts']).map do |p|\n            if p['text'] || p[:text]\n              { 'text' => p['text'] || p[:text] }\n            elsif p['inline_data'] || p[:inline_data] || p['inlineData']\n              src = p['inline_data'] || p[:inline_data] || p['inlineData']\n              { 'inlineData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'data'     => src['data'] || src[:data]\n                }.compact\n              }\n            elsif p['file_data'] || p[:file_data] || p['fileData']\n              src = p['file_data'] || p[:file_data] || p['fileData']\n              { 'fileData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'fileUri'  => src['file_uri']  || src[:file_uri]  || src['fileUri']\n                }.compact\n              }\n            else\n              {} # ignored\n            end\n          end.compact\n\n          { 'role' => role, 'parts' => parts }\n        end\n\n        payload = {\n          'contents'         => contents,\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        if variables['system']\n          payload['systemInstruction'] = { 'parts' => [{ 'text' => variables['system'] }] }\n        end\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        # JSON mode (optional)\n        gc = (payload['generationConfig'] ||= {})\n        gc['responseMimeType'] = variables['response_mime_type'] if variables['response_mime_type']\n        gc['responseSchema']   = variables['response_schema']     if variables['response_schema']\n\n        payload['labels'] = variables['labels'] if variables['labels']\n        payload\n      when 'vertex_passthrough'\n        src = variables['payload'] || variables['payload_json'] || variables['fully_formed'] || variables['request_json']\n        obj =\n          if src.is_a?(String)\n            begin\n              JSON.parse(src)\n            rescue\n              corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n              error(\"Invalid payload_json (must be valid JSON object). [corr_id=#{corr}]\")\n            end\n          else\n            src\n          end\n        error('payload_json must be a JSON object') unless obj.is_a?(Hash)\n        obj\n\n      # Embedding\n      when 'embedding'\n        body = {\n          'instances' => variables['texts'].map { |text|\n            {\n              'content'   => text,\n              'task_type' => variables['task_type'] || 'RETRIEVAL_DOCUMENT',\n              'title'     => variables['title']\n            }.compact\n          }\n        }\n        params = {}\n        params['autoTruncate']          = variables['auto_truncate'] unless variables['auto_truncate'].nil?\n        params['outputDimensionality']  = variables['output_dimensionality'] if variables['output_dimensionality']\n        body['parameters'] = params unless params.empty? \n        body\n      # Vector search\n      when 'find_neighbors'\n        queries = Array(variables['queries']).map do |q|\n          dp =\n            if q['feature_vector']\n              { 'feature_vector' => Array(q['feature_vector']).map(&:to_f) }\n            elsif q['vector'] # alias\n              { 'feature_vector' => Array(q['vector']).map(&:to_f) }\n            elsif q['datapoint_id']\n              { 'datapoint_id' => q['datapoint_id'] }\n            else\n              {}\n            end\n          {\n            'datapoint'        => dp,\n            'neighbor_count'   => (q['neighbor_count'] || variables['neighbor_count'] || 10).to_i,\n            'restricts'        => q['restricts'],\n            'numeric_restricts'=> q['numeric_restricts']\n          }.compact\n        end\n\n        {\n          'deployed_index_id'     => variables['deployed_index_id'],\n          'queries'               => queries,\n          'return_full_datapoint' => variables['return_full_datapoint']\n        }.compact\n\n      when 'upsert_datapoints'\n        datapoints =\n          if Array(variables['datapoints']).any?\n            Array(variables['datapoints']).map do |d|\n              {\n                'datapointId'      => d['datapoint_id'] || d['id'],\n                'featureVector'    => Array(d['feature_vector'] || d['vector']).map(&:to_f),\n                'sparseEmbedding'  => d['sparse_embedding'],\n                'restricts'        => d['restricts'],\n                'numericRestricts' => d['numeric_restricts'],\n                'crowdingTag'      => d['crowding_tag'],\n                'embeddingMetadata'=> d['embedding_metadata']\n              }.compact\n            end\n          elsif Array(variables['embeddings']).any?\n            call('coerce_embeddings_to_datapoints', variables)\n          else\n            []\n          end\n\n        { 'datapoints' => datapoints }\n      when 'read_index_datapoints'\n        ids = call('extract_ids_for_read', variables)\n        { 'deployed_index_id' => variables['deployed_index_id'], 'ids' => ids }\n      # Multimodal\n      when 'multimodal'\n        parts = []\n        parts << { 'text' => variables['text'] } if variables['text']\n        if variables['images']\n          variables['images'].each do |img|\n            parts << { 'inlineData' => { 'mimeType' => img['mime_type'] || 'image/jpeg', 'data' => img['data'] } }\n          end\n        end\n\n        payload = {\n          'contents' => [{ 'role' => 'user', 'parts' => parts }],\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        payload\n        \n      else\n        variables\n      end\n    end,\n    \n    # Response Enrichment\n    enrich_response: lambda do |response:, metadata: {}|\n      base  = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'result' => response }\n      trace = base.delete('_trace')\n      _http = base.delete('_http') # kept internal; not exposed\n\n      # Preserve success if caller provided it; otherwise assume true\n      success = base.key?('success') ? base['success'] : true\n\n      # Build a uniform trace object (always present)\n      trace_hash  = trace.is_a?(Hash) ? trace : {}\n      final_trace = {\n        'correlation_id' => trace_hash['correlation_id'] || SecureRandom.hex(8),\n        'duration_ms'    => (trace_hash['duration_ms'] || 0).to_i,\n        'attempt'        => (trace_hash['attempt'] || 1).to_i\n      }\n      final_trace['rate_limit'] = trace_hash['rate_limit'] if trace_hash['rate_limit']\n\n      base.merge(\n        'success'   => success,\n        'timestamp' => base['timestamp'] || Time.now.utc.iso8601,\n        'metadata'  => { 'operation' => metadata['operation'], 'model' => metadata['model'] }.compact,\n        'trace'     => final_trace\n      ).compact\n    end,\n\n    # Response Extraction\n    extract_response: lambda do |data:, path: nil, format: 'raw'|\n      case format\n      # Raw data\n      when 'raw' then data\n      # Json field\n      when 'json_field'\n        return data unless path\n        path.split('.').reduce(data) { |acc, seg| acc.is_a?(Array) && seg =~ /^\\d+$/ ? acc[seg.to_i] : (acc || {})[seg] }\n      # Vertex text\n      when 'vertex_text'\n        parts = data.dig('candidates', 0, 'content', 'parts') || []\n        text  = parts.select { |p| p['text'] }.map { |p| p['text'] }.join\n        text.empty? ? data.dig('predictions', 0, 'content').to_s : text\n      # Vertex-flavored json\n      when 'vertex_json'\n        raw = (data.dig('candidates', 0, 'content', 'parts') || []).map { |p| p['text'] }.compact.join\n        return {} if raw.nil? || raw.empty?\n        m = raw.match(/```(?:json)?\\s*(\\{.*?\\})\\s*```/m) || raw.match(/\\{.*\\}/m)\n        m ? (JSON.parse(m[1] || m[0]) rescue {}) : {}\n      # Embeddings\n      when 'embeddings'\n        # text-embedding APIs return embeddings under predictions[].embeddings.values\n        arr = (data['predictions'] || []).map { |p| p.dig('embeddings', 'values') || p['values'] }.compact\n        arr\n      else data\n      end\n    end,\n\n    # HTTP Request Execution\n    http_request: lambda do |connection, method:, url:, payload: nil, headers: {}, retry_config: {}, request_format: 'json'|\n      max_attempts = (retry_config['max_attempts'] || retry_config['max_retries'] || 3).to_i\n      base_backoff = (retry_config['backoff'] || 1.0).to_f\n      retry_on     = Array(retry_config['retry_on'] || [408, 429, 500, 502, 503, 504]).map(&:to_i)\n      do_not_retry = Array(retry_config['do_not_retry']).map(&:to_i)\n\n      attempt = 0\n      last_error = nil\n\n      while attempt < max_attempts\n        attempt += 1\n        begin\n          hdrs = (headers || {}).dup\n          corr = hdrs['X-Correlation-Id'] ||= \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n          last_error = nil\n\n          req = case method.to_s.upcase\n                when 'GET'    then get(url)\n                when 'POST'   then post(url, payload)\n                when 'PUT'    then put(url, payload)\n                when 'DELETE' then delete(url)\n                else error(\"Unsupported HTTP method: #{method}\")\n                end\n\n          # Respect application/x-www-form-urlencoded when requested\n          if request_format.to_s == 'form'\n            hdrs['Content-Type'] ||= 'application/x-www-form-urlencoded'\n            req = req.request_format_www_form_urlencoded\n          end\n\n          response =\n            req.headers(hdrs)\n              .after_error_response(/.*/) { |code, body, rheaders, message|\n                 dur_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n                 err = call('normalize_http_error',\n                            connection,\n                            code: code, body: body, headers: (rheaders || {}),\n                            message: message, url: url, corr_id: corr, attempt: attempt, duration_ms: dur_ms)\n                 last_error = err\n                 error(call('format_user_error', err))\n               }\n              .after_response { |code, body, rheaders|\n                # Always return a Hash payload with HTTP metadata, even when the API returns a raw string/bytes.\n                payload = body.is_a?(Hash) ? body : { 'raw' => body }\n                payload['_http'] = { 'status' => code, 'headers' => rheaders }\n                payload\n              }\n\n          duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n          out = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'raw' => response }\n\n          # Extract a useful Google/Vertex request id when present.\n          hdrs_out = (out.dig('_http', 'headers') || {})\n          rid = hdrs_out['x-request-id'] || hdrs_out['x-cloud-trace-context'] || hdrs_out['x-guploader-uploadid']\n\n          out['_trace'] = {\n            'correlation_id'    => corr,\n            'duration_ms'       => duration_ms,\n            'attempt'           => attempt,\n            'http_status'       => out.dig('_http','status'),\n            'remote_request_id' => rid\n          }.compact\n\n          return out\n\n        rescue => _e\n          code = last_error ? last_error['code'].to_i : 0\n          retryable = last_error ? last_error['retryable'] : retry_on.include?(code)\n          retryable &&= !do_not_retry.include?(code)\n\n          # Break if not retryable or out of attempts\n          break unless retryable && attempt < max_attempts\n\n          # Retry-After (seconds) takes precedence when present\n          delay =\n            if last_error && last_error['retry_after_s'].to_i > 0\n              last_error['retry_after_s'].to_i\n            else\n              # exp backoff with small jitter\n              (base_backoff * (2 ** (attempt - 1))).to_f + rand * 0.25\n            end\n\n          sleep(delay)\n        end\n      end\n\n      # Exhausted: bubble the last normalized message if present\n      msg = last_error ? call('format_user_error', last_error) : 'HTTP request failed'\n      error(msg)\n    end,\n\n    # Data Transformation\n    transform_data: lambda do |input:, from_format:, to_format:, connection: nil|\n      case \"#{from_format}_to_#{to_format}\"\n      when 'url_to_base64'\n        # Use centralized http_request for retries and telemetry\n        resp = call('http_request', connection, method: 'GET', url: input, headers: {})\n        raw  = resp['raw'] || resp.to_s\n        raw.to_s.encode_base64\n      when 'base64_to_bytes'\n        input.decode_base64\n      when 'language_code_to_name'\n        languages = { 'en' => 'English', 'es' => 'Spanish', 'fr' => 'French' }\n        languages[input] || input\n      when 'categories_to_text'\n        input.map { |c| \"#{c['name']}: #{c['description']}\" }.join(\"\\n\")\n      when 'distance_to_similarity'\n        1.0 - (input.to_f / 2.0)\n      else\n        input\n      end\n    end,\n    \n    # Input Validation\n    validate_input: lambda do |data:, schema: [], constraints: []|\n      errors = []\n      \n      # Schema validation\n      schema.each do |field|\n        field_name = field['name']\n        field_value = data[field_name]\n        \n        # Required check\n        if field['required'] && (field_value.nil? || field_value.to_s.empty?)\n          errors << \"#{field_name} is required\"\n        end\n        \n        # Length validation\n        if field['max_length'] && field_value.to_s.length > field['max_length']\n          errors << \"#{field_name} exceeds maximum length of #{field['max_length']}\"\n        end\n        \n        # Pattern validation\n        if field['pattern'] && field_value && !field_value.match?(Regexp.new(field['pattern']))\n          errors << \"#{field_name} format is invalid\"\n        end\n      end\n      \n      # Constraint validation\n      constraints.each do |constraint|\n        ctype = (constraint['type'] || constraint[:type]).to_s\n\n        case ctype\n        when 'min_value'\n          value = data[(constraint['field'] || constraint[:field]).to_s].to_f\n          if value < constraint['value'].to_f\n            errors << \"#{constraint['field'] || constraint[:field]} must be at least #{constraint['value']}\"\n          end\n\n        when 'max_items'\n          field = (constraint['field'] || constraint[:field]).to_s\n          items = data[field] || []\n          if Array(items).size > constraint['value'].to_i\n            errors << \"#{field} cannot exceed #{constraint['value']} items\"\n          end\n\n        # XOR/ONE-OF across fields (root or per-item scope)\n        when 'xor', 'one_of'\n         "}
{"id":"n_735f3339f67e3723","kind":"method","name":"build_payload","fqname":"connector.Vertex AI/methods.methods/method.build_payload","loc":{"line":817,"column":19,"length":7330,"begin":37600,"end":44930},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ld_payload: lambda do |template:, variables:, format:|\n      case format\n      \n      # Direct\n      when 'direct'\n        variables\n      \n      # Template\n      when 'template'\n        result = template.dup\n        variables.each { |k, v| result = result.gsub(\"{#{k}}\", v.to_s) }\n        result\n      \n      # Vertex prompt\n      when 'vertex_prompt'\n        payload = {\n          'contents' => [{\n            'role'  => 'user',\n            'parts' => [{ 'text' => call('apply_template', template, variables) }]\n          }],\n          'generationConfig' => call('build_generation_config', variables)\n        }.compact\n\n        sys = variables['system']\n        payload['systemInstruction'] = { 'parts' => [{ 'text' => sys }] } if sys && !sys.to_s.strip.empty?\n\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        # JSON mode (optional)\n        if variables['response_mime_type'] || variables['response_schema']\n          gc = (payload['generationConfig'] ||= {})\n          gc['responseMimeType'] = variables['response_mime_type'] if variables['response_mime_type']\n          gc['responseSchema']   = variables['response_schema']     if variables['response_schema']\n        end\n\n        payload['labels'] = variables['labels'] if variables['labels']\n        payload\n\n      when 'vertex_contents'\n        contents = Array(variables['contents']).map do |c|\n          role = c['role'] || c[:role] || 'user'\n          parts = Array(c['parts']).map do |p|\n            if p['text'] || p[:text]\n              { 'text' => p['text'] || p[:text] }\n            elsif p['inline_data'] || p[:inline_data] || p['inlineData']\n              src = p['inline_data'] || p[:inline_data] || p['inlineData']\n              { 'inlineData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'data'     => src['data'] || src[:data]\n                }.compact\n              }\n            elsif p['file_data'] || p[:file_data] || p['fileData']\n              src = p['file_data'] || p[:file_data] || p['fileData']\n              { 'fileData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'fileUri'  => src['file_uri']  || src[:file_uri]  || src['fileUri']\n                }.compact\n              }\n            else\n              {} # ignored\n            end\n          end.compact\n\n          { 'role' => role, 'parts' => parts }\n        end\n\n        payload = {\n          'contents'         => contents,\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        if variables['system']\n          payload['systemInstruction'] = { 'parts' => [{ 'text' => variables['system'] }] }\n        end\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        # JSON mode (optional)\n        gc = (payload['generationConfig'] ||= {})\n        gc['responseMimeType'] = variables['response_mime_type'] if variables['response_mime_type']\n        gc['responseSchema']   = variables['response_schema']     if variables['response_schema']\n\n        payload['labels'] = variables['labels'] if variables['labels']\n        payload\n      when 'vertex_passthrough'\n        src = variables['payload'] || variables['payload_json'] || variables['fully_formed'] || variables['request_json']\n        obj =\n          if src.is_a?(String)\n            begin\n              JSON.parse(src)\n            rescue\n              corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n              error(\"Invalid payload_json (must be valid JSON object). [corr_id=#{corr}]\")\n            end\n          else\n            src\n          end\n        error('payload_json must be a JSON object') unless obj.is_a?(Hash)\n        obj\n\n      # Embedding\n      when 'embedding'\n        body = {\n          'instances' => variables['texts'].map { |text|\n            {\n              'content'   => text,\n              'task_type' => variables['task_type'] || 'RETRIEVAL_DOCUMENT',\n              'title'     => variables['title']\n            }.compact\n          }\n        }\n        params = {}\n        params['autoTruncate']          = variables['auto_truncate'] unless variables['auto_truncate'].nil?\n        params['outputDimensionality']  = variables['output_dimensionality'] if variables['output_dimensionality']\n        body['parameters'] = params unless params.empty? \n        body\n      # Vector search\n      when 'find_neighbors'\n        queries = Array(variables['queries']).map do |q|\n          dp =\n            if q['feature_vector']\n              { 'feature_vector' => Array(q['feature_vector']).map(&:to_f) }\n            elsif q['vector'] # alias\n              { 'feature_vector' => Array(q['vector']).map(&:to_f) }\n            elsif q['datapoint_id']\n              { 'datapoint_id' => q['datapoint_id'] }\n            else\n              {}\n            end\n          {\n            'datapoint'        => dp,\n            'neighbor_count'   => (q['neighbor_count'] || variables['neighbor_count'] || 10).to_i,\n            'restricts'        => q['restricts'],\n            'numeric_restricts'=> q['numeric_restricts']\n          }.compact\n        end\n\n        {\n          'deployed_index_id'     => variables['deployed_index_id'],\n          'queries'               => queries,\n          'return_full_datapoint' => variables['return_full_datapoint']\n        }.compact\n\n      when 'upsert_datapoints'\n        datapoints =\n          if Array(variables['datapoints']).any?\n            Array(variables['datapoints']).map do |d|\n              {\n                'datapointId'      => d['datapoint_id'] || d['id'],\n                'featureVector'    => Array(d['feature_vector'] || d['vector']).map(&:to_f),\n                'sparseEmbedding'  => d['sparse_embedding'],\n                'restricts'        => d['restricts'],\n                'numericRestricts' => d['numeric_restricts'],\n                'crowdingTag'      => d['crowding_tag'],\n                'embeddingMetadata'=> d['embedding_metadata']\n              }.compact\n            end\n          elsif Array(variables['embeddings']).any?\n            call('coerce_embeddings_to_datapoints', variables)\n          else\n            []\n          end\n\n        { 'datapoints' => datapoints }\n      when 'read_index_datapoints'\n        ids = call('extract_ids_for_read', variables)\n        { 'deployed_index_id' => variables['deployed_index_id'], 'ids' => ids }\n      # Multimodal\n      when 'multimodal'\n        parts = []\n        parts << { 'text' => variables['text'] } if variables['text']\n        if variables['images']\n          variables['images'].each do |img|\n            parts << { 'inlineData' => { 'mimeType' => img['mime_type'] || 'image/jpeg', 'data' => img['data'] } }\n          end\n        end\n\n        payload = {\n          'contents' => [{ 'role' => 'user', 'parts' => parts }],\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        payload\n        \n      else\n        variables\n     "}
{"id":"n_c6ed6bd8ad0fbcef","kind":"method","name":"enrich_response","fqname":"connector.Vertex AI/methods.methods/method.enrich_response","loc":{"line":1014,"column":21,"length":1076,"begin":44984,"end":46060},"file":null,"keys":[],"http":{"verbs":["DELETE"],"endpoints":["DELETE _trace","DELETE _http"]},"text":"h_response: lambda do |response:, metadata: {}|\n      base  = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'result' => response }\n      trace = base.delete('_trace')\n      _http = base.delete('_http') # kept internal; not exposed\n\n      # Preserve success if caller provided it; otherwise assume true\n      success = base.key?('success') ? base['success'] : true\n\n      # Build a uniform trace object (always present)\n      trace_hash  = trace.is_a?(Hash) ? trace : {}\n      final_trace = {\n        'correlation_id' => trace_hash['correlation_id'] || SecureRandom.hex(8),\n        'duration_ms'    => (trace_hash['duration_ms'] || 0).to_i,\n        'attempt'        => (trace_hash['attempt'] || 1).to_i\n      }\n      final_trace['rate_limit'] = trace_hash['rate_limit'] if trace_hash['rate_limit']\n\n      base.merge(\n        'success'   => success,\n        'timestamp' => base['timestamp'] || Time.now.utc.iso8601,\n        'metadata'  => { 'operation' => metadata['operation'], 'model' => metadata['model'] }.compact,\n        'trace'     => final_trace\n      ).com"}
{"id":"n_e52c88457cbb1082","kind":"method","name":"extract_response","fqname":"connector.Vertex AI/methods.methods/method.extract_response","loc":{"line":1040,"column":22,"length":1179,"begin":46111,"end":47290},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"t_response: lambda do |data:, path: nil, format: 'raw'|\n      case format\n      # Raw data\n      when 'raw' then data\n      # Json field\n      when 'json_field'\n        return data unless path\n        path.split('.').reduce(data) { |acc, seg| acc.is_a?(Array) && seg =~ /^\\d+$/ ? acc[seg.to_i] : (acc || {})[seg] }\n      # Vertex text\n      when 'vertex_text'\n        parts = data.dig('candidates', 0, 'content', 'parts') || []\n        text  = parts.select { |p| p['text'] }.map { |p| p['text'] }.join\n        text.empty? ? data.dig('predictions', 0, 'content').to_s : text\n      # Vertex-flavored json\n      when 'vertex_json'\n        raw = (data.dig('candidates', 0, 'content', 'parts') || []).map { |p| p['text'] }.compact.join\n        return {} if raw.nil? || raw.empty?\n        m = raw.match(/```(?:json)?\\s*(\\{.*?\\})\\s*```/m) || raw.match(/\\{.*\\}/m)\n        m ? (JSON.parse(m[1] || m[0]) rescue {}) : {}\n      # Embeddings\n      when 'embeddings'\n        # text-embedding APIs return embeddings under predictions[].embeddings.values\n        arr = (data['predictions'] || []).map { |p| p.dig('embeddings', 'values') || p['values'] }.compact\n        arr\n      else data\n     "}
{"id":"n_4c55ba39e0f8a2fa","kind":"method","name":"http_request","fqname":"connector.Vertex AI/methods.methods/method.http_request","loc":{"line":1069,"column":18,"length":3959,"begin":47340,"end":51299},"file":null,"keys":[],"http":{"verbs":["GET","POST","PUT","DELETE"],"endpoints":["GET (dynamic)","POST (dynamic)","PUT (dynamic)","DELETE (dynamic)"]},"text":"tp_request: lambda do |connection, method:, url:, payload: nil, headers: {}, retry_config: {}, request_format: 'json'|\n      max_attempts = (retry_config['max_attempts'] || retry_config['max_retries'] || 3).to_i\n      base_backoff = (retry_config['backoff'] || 1.0).to_f\n      retry_on     = Array(retry_config['retry_on'] || [408, 429, 500, 502, 503, 504]).map(&:to_i)\n      do_not_retry = Array(retry_config['do_not_retry']).map(&:to_i)\n\n      attempt = 0\n      last_error = nil\n\n      while attempt < max_attempts\n        attempt += 1\n        begin\n          hdrs = (headers || {}).dup\n          corr = hdrs['X-Correlation-Id'] ||= \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n          last_error = nil\n\n          req = case method.to_s.upcase\n                when 'GET'    then get(url)\n                when 'POST'   then post(url, payload)\n                when 'PUT'    then put(url, payload)\n                when 'DELETE' then delete(url)\n                else error(\"Unsupported HTTP method: #{method}\")\n                end\n\n          # Respect application/x-www-form-urlencoded when requested\n          if request_format.to_s == 'form'\n            hdrs['Content-Type'] ||= 'application/x-www-form-urlencoded'\n            req = req.request_format_www_form_urlencoded\n          end\n\n          response =\n            req.headers(hdrs)\n              .after_error_response(/.*/) { |code, body, rheaders, message|\n                 dur_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n                 err = call('normalize_http_error',\n                            connection,\n                            code: code, body: body, headers: (rheaders || {}),\n                            message: message, url: url, corr_id: corr, attempt: attempt, duration_ms: dur_ms)\n                 last_error = err\n                 error(call('format_user_error', err))\n               }\n              .after_response { |code, body, rheaders|\n                # Always return a Hash payload with HTTP metadata, even when the API returns a raw string/bytes.\n                payload = body.is_a?(Hash) ? body : { 'raw' => body }\n                payload['_http'] = { 'status' => code, 'headers' => rheaders }\n                payload\n              }\n\n          duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n          out = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'raw' => response }\n\n          # Extract a useful Google/Vertex request id when present.\n          hdrs_out = (out.dig('_http', 'headers') || {})\n          rid = hdrs_out['x-request-id'] || hdrs_out['x-cloud-trace-context'] || hdrs_out['x-guploader-uploadid']\n\n          out['_trace'] = {\n            'correlation_id'    => corr,\n            'duration_ms'       => duration_ms,\n            'attempt'           => attempt,\n            'http_status'       => out.dig('_http','status'),\n            'remote_request_id' => rid\n          }.compact\n\n          return out\n\n        rescue => _e\n          code = last_error ? last_error['code'].to_i : 0\n          retryable = last_error ? last_error['retryable'] : retry_on.include?(code)\n          retryable &&= !do_not_retry.include?(code)\n\n          # Break if not retryable or out of attempts\n          break unless retryable && attempt < max_attempts\n\n          # Retry-After (seconds) takes precedence when present\n          delay =\n            if last_error && last_error['retry_after_s'].to_i > 0\n              last_error['retry_after_s'].to_i\n            else\n              # exp backoff with small jitter\n              (base_backoff * (2 ** (attempt - 1))).to_f + rand * 0.25\n            end\n\n          sleep(delay)\n        end\n      end\n\n      # Exhausted: bubble the last normalized message if present\n      msg = last_error ? call('format_user_error', last_error) : 'HTTP request failed'\n      error("}
{"id":"n_6fa4e7c38dbc3bf7","kind":"method","name":"transform_data","fqname":"connector.Vertex AI/methods.methods/method.transform_data","loc":{"line":1162,"column":20,"length":778,"begin":51348,"end":52126},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"sform_data: lambda do |input:, from_format:, to_format:, connection: nil|\n      case \"#{from_format}_to_#{to_format}\"\n      when 'url_to_base64'\n        # Use centralized http_request for retries and telemetry\n        resp = call('http_request', connection, method: 'GET', url: input, headers: {})\n        raw  = resp['raw'] || resp.to_s\n        raw.to_s.encode_base64\n      when 'base64_to_bytes'\n        input.decode_base64\n      when 'language_code_to_name'\n        languages = { 'en' => 'English', 'es' => 'Spanish', 'fr' => 'French' }\n        languages[input] || input\n      when 'categories_to_text'\n        input.map { |c| \"#{c['name']}: #{c['description']}\" }.join(\"\\n\")\n      when 'distance_to_similarity'\n        1.0 - (input.to_f / 2.0)\n      else\n        input\n     "}
{"id":"n_40d65e2414e179bf","kind":"method","name":"validate_input","fqname":"connector.Vertex AI/methods.methods/method.validate_input","loc":{"line":1184,"column":20,"length":4248,"begin":52176,"end":56424},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"date_input: lambda do |data:, schema: [], constraints: []|\n      errors = []\n      \n      # Schema validation\n      schema.each do |field|\n        field_name = field['name']\n        field_value = data[field_name]\n        \n        # Required check\n        if field['required'] && (field_value.nil? || field_value.to_s.empty?)\n          errors << \"#{field_name} is required\"\n        end\n        \n        # Length validation\n        if field['max_length'] && field_value.to_s.length > field['max_length']\n          errors << \"#{field_name} exceeds maximum length of #{field['max_length']}\"\n        end\n        \n        # Pattern validation\n        if field['pattern'] && field_value && !field_value.match?(Regexp.new(field['pattern']))\n          errors << \"#{field_name} format is invalid\"\n        end\n      end\n      \n      # Constraint validation\n      constraints.each do |constraint|\n        ctype = (constraint['type'] || constraint[:type]).to_s\n\n        case ctype\n        when 'min_value'\n          value = data[(constraint['field'] || constraint[:field]).to_s].to_f\n          if value < constraint['value'].to_f\n            errors << \"#{constraint['field'] || constraint[:field]} must be at least #{constraint['value']}\"\n          end\n\n        when 'max_items'\n          field = (constraint['field'] || constraint[:field]).to_s\n          items = data[field] || []\n          if Array(items).size > constraint['value'].to_i\n            errors << \"#{field} cannot exceed #{constraint['value']} items\"\n          end\n\n        # XOR/ONE-OF across fields (root or per-item scope)\n        when 'xor', 'one_of'\n          scope   = (constraint['scope'] || constraint[:scope]).to_s # e.g., 'queries[]' or ''\n          fields  = Array(constraint['fields'] || constraint[:fields]).map(&:to_s)\n          aliases = (constraint['aliases'] || constraint[:aliases] || {}) # { 'feature_vector' => ['vector'] }\n          exactly_one = (ctype == 'xor') || (constraint['exactly_one'] == true)\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            count = 0\n            fields.each do |f|\n              keys = [f] + Array(aliases[f] || aliases[f.to_sym]).map(&:to_s)\n              present = keys.any? { |k| call('value_present', ctx[k]) }\n              count += 1 if present\n            end\n\n            if exactly_one\n              if count != 1\n                display = fields.map { |f|\n                  al = Array(aliases[f] || aliases[f.to_sym])\n                  al.any? ? \"#{f} (alias: #{al.join(', ')})\" : f\n                }.join(', ')\n                errors << \"#{label}: exactly one of #{display} must be provided\"\n              end\n            else\n              if count < 1\n                errors << \"#{label}: at least one of #{fields.join(', ')} must be provided\"\n              end\n            end\n          end\n\n        # Conditional required with root-level fallback and optional default\n        # Example: each queries[].neighbor_count is optional if top-level neighbor_count is present,\n        # or if a default is defined; else required.\n        when 'fallback_required', 'conditional_required'\n          scope    = (constraint['scope'] || constraint[:scope]).to_s       # e.g., 'queries[]'\n          field    = (constraint['field'] || constraint[:field]).to_s       # e.g., 'neighbor_count'\n          fallback = (constraint['fallback_to_root'] || constraint[:fallback_to_root]).to_s # e.g., 'neighbor_count'\n          default_ok = constraint.key?('default_if_absent') || constraint.key?(:default_if_absent)\n\n          root_has_fallback = fallback.empty? ? false : call('value_present', data[fallback])\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            item_has = call('value_present', ctx[field])\n            unless item_has || root_has_fallback || default_ok\n              if fallback.empty?\n                errors << \"#{label}.#{field} is required\"\n              else\n                errors << \"#{label}.#{field} is required when top-level #{fallback} is not provided\"\n              end\n            end\n          end\n\n        else\n          # unknown constraint type: ignore silently (forward-compatible)\n        end\n      end\n      \n      error(errors.join('; ')) if errors.any?\n      "}
{"id":"n_a7bb99c191d71eac","kind":"method","name":"with_resilience","fqname":"connector.Vertex AI/methods.methods/method.with_resilience","loc":{"line":1288,"column":21,"length":2187,"begin":56473,"end":58660},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"resilience: lambda do |operation:, config: {}, task: {}, connection: nil, &blk|\n      # Rate limiting (per-job) — always initialize and use a unique name\n      rate_limit_info = nil\n      if config['rate_limit']\n        rate_limit_info = call('check_rate_limit', operation, config['rate_limit'])\n      end\n\n      circuit_key   = \"circuit_#{operation}\"\n      circuit_state = call('memo_get', circuit_key) || { 'failures' => 0 }\n      error(\"Circuit breaker open for #{operation}. Too many recent failures.\") if circuit_state['failures'] >= 5\n\n      begin\n        result =\n          if blk\n            # Instrument the block path so trace is still present\n            corr    = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n            started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n            raw     = blk.call\n            dur_ms  = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n            out     = raw.is_a?(Hash) ? JSON.parse(JSON.dump(raw)) : { 'result' => raw }\n            out['_trace'] ||= {}\n            out['_trace'].merge!({ 'correlation_id' => corr, 'duration_ms' => dur_ms, 'attempt' => 1 })\n            out\n          else\n            error('with_resilience requires a task hash with url/method') unless task.is_a?(Hash) && task['url']\n\n            call('http_request',\n              connection,\n              method:       (task['method'] || 'GET'),\n              url:          task['url'],\n              payload:      task['payload'],\n              headers:      (task['headers'] || {}),\n              retry_config: (task['retry_config'] || {})\n            )\n          end\n\n        # Attach rate-limit counters to trace if present (guarded)\n        if rate_limit_info && result.is_a?(Hash)\n          result['_trace'] ||= {}\n          result['_trace']['rate_limit'] = rate_limit_info\n        end\n\n        # Reset circuit on success\n        call('memo_put', circuit_key, { 'failures' => 0 }, 300)\n        result\n\n      rescue => e\n        circuit_state['failures'] += 1\n        call('memo_put', circuit_key, circuit_state, 300)\n        # Keep normalized messages intact; do not blanket-retry non-retryables here\n        raise e\n   "}
{"id":"n_c45bd0d836940af8","kind":"method","name":"execute_pipeline","fqname":"connector.Vertex AI/methods.methods/method.execute_pipeline","loc":{"line":1346,"column":22,"length":2915,"begin":58826,"end":61741},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ute_pipeline: lambda do |connection, operation, input, config|\n      # Don't mutate the input\n      local = call('deep_copy', input)\n\n      # 1. Validate\n      if config['validate']\n        call('validate_input',\n          data:         local,\n          schema:       config['validate']['schema'] || [],\n          constraints:  config['validate']['constraints'] || []\n        )\n      end\n      \n      # 2. Transform input\n      if config['transform_input']\n        config['transform_input'].each do |field, transform|\n          if local[field]\n            local[field] = call('transform_data',\n              input:        local[field],\n              from_format:  transform['from'],\n              to_format:    transform['to'],\n              connection:   connection\n            )\n          end\n        end\n      end\n\n      # -- Ensure selected model from ops config is visible to URL builder\n      local['model'] ||= config['model']\n\n      # 3. Build payload\n      payload = if config['payload']\n        call('build_payload',\n          template:   config['payload']['template'] || '',\n          variables:  local.merge('system' => config['payload']['system']),\n          format:     config['payload']['format'] || 'direct'\n        )\n      else\n        local\n      end\n      \n      # 4. Build URL\n      endpoint  = config['endpoint'] || {}\n      url       = call('build_endpoint_url', connection, endpoint, local)\n      \n      # 5. Execute with resilience\n      response = call('with_resilience',\n        operation:  operation,\n        config:     (config['resilience'] || {}),\n        task: {\n          'method'       => endpoint['method'] || 'POST',\n          'url'          => url,\n          'payload'      => payload,\n          'headers'      => call('build_headers', connection),\n          'retry_config' => (config.dig('resilience', 'retry') || {})\n        },\n        connection: connection\n      )\n      \n      trace_from_response = (response.is_a?(Hash) ? response['_trace'] : nil)\n\n      # 6. Extract response\n      extracted = if config['extract']\n        call('extract_response',\n          data:   response,\n          path:   config['extract']['path'],\n          format: config['extract']['format'] || 'raw'\n        )\n      else\n        response\n      end\n\n      # Preserve trace even if extracted is a primitive\n      if trace_from_response\n        if extracted.is_a?(Hash)\n          extracted['_trace'] ||= {}\n          extracted['_trace'].merge!(trace_from_response)\n        else\n          extracted = { 'result' => extracted, '_trace' => trace_from_response }\n        end\n      end\n      \n      # 7. Post-process\n      if config['post_process']\n        extracted = call(config['post_process'], extracted, local)\n      end\n      \n      # 8. Enrich\n      call('enrich_response',\n        response: extracted,\n        metadata: { 'operation' => operation, 'model' => config['model'] || local['model'] }\n "}
{"id":"n_08758123084516be","kind":"method","name":"behavior_registry","fqname":"connector.Vertex AI/methods.methods/method.behavior_registry","loc":{"line":1445,"column":23,"length":9985,"begin":61968,"end":71953},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ior_registry: lambda do\n      {\n        # Text Operations\n        'text.generate' => {\n          description: 'Generate text from a prompt',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['streaming', 'caching'],\n          config_template: {\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => '{prompt}',\n              'system' => nil\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        'text.translate' => {\n          description: 'Translate text between languages',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true, 'max_length' => 10000 },\n                { 'name' => 'target_language', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'source_language' => { 'from' => 'language_code', 'to' => 'name' },\n              'target_language' => { 'from' => 'language_code', 'to' => 'name' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Translate the following text from {source_language} to {target_language}. Return only the translation:\\n\\n{text}',\n              'system' => 'You are a professional translator. Maintain tone and context.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          },\n          defaults: {\n            'temperature' => 0.3,\n            'max_tokens' => 2048\n          }\n        },\n        'text.summarize' => {\n          description: 'Summarize text content',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'max_words', 'required' => false }\n              ]\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Summarize the following text in {max_words} words:\\n\\n{text}',\n              'system' => 'You are an expert at creating clear, concise summaries.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            },\n            'post_process' => 'add_word_count'\n          },\n          defaults: {\n            'temperature' => 0.5,\n            'max_words' => 200\n          }\n        },\n        'text.classify' => {\n          description: 'Classify text into categories',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'categories', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'categories' => { 'from' => 'categories', 'to' => 'text' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Classify this text into one of these categories:\\n{categories}\\n\\nText: {text}\\n\\nRespond with JSON: {\"category\": \"name\", \"confidence\": 0.0-1.0}',\n              'system' => 'You are a classification expert. Always return valid JSON.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_json'\n            }\n          },\n          defaults: {\n            'temperature' => 0.1\n          }\n        },\n        # Embedding Operations\n        'text.embed' => {\n          description: 'Generate text embeddings',\n          capability: 'embedding',\n          supported_models: ['text-embedding-005', 'text-embedding-004', 'textembedding-gecko', 'gemini-embedding-001'],\n          features: ['batching', 'caching'],\n          config_template: {\n            'validate' => {\n              'schema' => [ { 'name' => 'texts', 'required' => true } ],\n              'constraints' => [ { 'type' => 'max_items', 'field' => 'texts', 'value' => 100 } ]\n            },\n            'payload' => { 'format' => 'embedding' },\n            'endpoint' => { 'path' => ':predict', 'method' => 'POST' },\n            'extract' => { 'format' => 'embeddings' },\n            'post_process' => 'wrap_embeddings_vectors'\n          }\n        },\n        # Multimodal Operations\n        'multimodal.analyze' => {\n          description: 'Analyze images with text prompts',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-pro', 'gemini-1.5-flash'],\n          features: ['streaming'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'prompt', 'required' => true },\n                { 'name' => 'images', 'required' => true }\n              ]\n            },\n            'payload' => {\n              'format' => 'multimodal'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        # Vector Operations\n        'vector.upsert_datapoints' => {\n          description: 'Upsert datapoints into a Vector Search index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index', 'required' => true }\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['datapoints', 'embeddings'] }\n              ]\n            },\n            'payload'  => { 'format' => 'upsert_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_indexes',\n              'path'   => ':upsertDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' }, # empty body on success\n            'post_process' => 'add_upsert_ack'\n          }\n        },\n        'vector.find_neighbors' => {\n          description: 'Find nearest neighbors from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'queries',           'required' => true },\n                { 'name' => 'distance_metric' },        # optional\n                { 'name' => 'feature_norm_type' },      # optional\n                { 'name' => 'include_stats' }           # optional\n              ],\n              'constraints' => [\n                # Exactly one locator per query: vector OR datapoint_id\n                {\n                  'type'   => 'xor',\n                  'scope'  => 'queries[]',\n                  'fields' => ['feature_vector', 'datapoint_id'],\n                  'aliases'=> { 'feature_vector' => ['vector'] } # honor your alias\n                },\n                # If a query omits neighbor_count, allow top-level neighbor_count or the internal default (10)\n                {\n                  'type'               => 'fallback_required',\n                  'scope'              => 'queries[]',\n                  'field'              => 'neighbor_count',\n                  'fallback_to_root'   => 'neighbor_count',\n                  'default_if_absent'  => 10  # matches your payload fallback\n                }\n              ]\n            },\n            'payload'  => { 'format' => 'find_neighbors' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':findNeighbors',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_find_neighbors'\n          }\n        },\n        'vector.read_datapoints' => {\n          description: 'Read datapoints (vectors) by ID from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'ids' },        # manual ids\n                { 'name' => 'groups' },     # from find_neighbors (normalized)\n                { 'name' => 'neighbors' }   # flattened neighbors\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['ids', 'groups', 'neighbors'] },\n                { 'type' => 'max_items', 'field' => 'ids', 'value' => 1000 }\n              ]\n            },\n            'payload'  => { 'format' => 'read_index_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':readIndexDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_read_index_datapoints'\n          }\n        }     \n "}
{"id":"n_e618b10b6c0d1dfc","kind":"method","name":"configuration_registry","fqname":"connector.Vertex AI/methods.methods/method.configuration_registry","loc":{"line":1702,"column":28,"length":1079,"begin":72036,"end":73115},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ion_registry: lambda do |connection, user_config|\n      {\n        # Model selection\n        models: {\n          default: user_config['model'] || connection['default_model'] || 'gemini-1.5-flash',\n          strategy: connection['optimization_mode'] || 'balanced',\n          mode: user_config['model_mode'] || 'auto'\n        },\n        \n        # Generation settings\n        generation: {\n          temperature: user_config['temperature'],\n          max_tokens: user_config['max_tokens'],\n          top_p: user_config['top_p'],\n          top_k: user_config['top_k']\n        }.compact,\n        \n        # Features\n        features: {\n          caching: {\n            enabled: connection['enable_caching'] != false,\n            ttl: user_config['cache_ttl'] || 300\n          },\n          logging: {\n            enabled: connection['enable_logging'] == true\n          }\n        },\n        \n        # Execution\n        execution: {\n          retry: {\n            max_attempts: 3,\n            backoff: 1.0\n          },\n          rate_limit: {\n            rpm: 60\n          }\n        }\n "}
{"id":"n_d264c41acce19399","kind":"method","name":"execute_behavior","fqname":"connector.Vertex AI/methods.methods/method.execute_behavior","loc":{"line":1744,"column":22,"length":2835,"begin":73193,"end":76028},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ute_behavior: lambda do |connection, behavior, input, user_config = {}|\n      behavior_def = call('behavior_registry')[behavior] or error(\"Unknown behavior: #{behavior}\")\n      local_input = call('deep_copy', input) # Work on a local copy only\n\n      # Apply defaults without side effects\n      if behavior_def[:defaults]\n        behavior_def[:defaults].each { |k, v| local_input[k] = local_input.key?(k) ? local_input[k] : v }\n      end\n\n      # Bring model-selection keys into local_input so select_model can use them\n      %w[model model_mode lock_model_revision].each do |k|\n        if user_config.key?(k) && !user_config[k].nil?\n          local_input[k] = user_config[k]\n        end\n      end      \n\n      cfg = call('configuration_registry', connection, user_config)\n      operation_config = JSON.parse(JSON.dump(behavior_def[:config_template] || {})) # build op config from template\n\n      # Bring generation settings into the local input (don’t mutate cfg)\n      if cfg[:generation]\n        cfg[:generation].each { |k, v| local_input[k] = v unless v.nil? }\n      end\n\n      operation_config['model'] = call('select_model', behavior_def, cfg, local_input)\n      # methods.execute_behavior (after setting operation_config['model'])\n      if behavior_def[:supported_models].any? && !behavior_def[:supported_models].include?(operation_config['model'])\n        # force a sane fallback rather than issuing a bad :predict call\n        operation_config['model'] = call('select_model', behavior_def, cfg, local_input.merge('model_mode' => 'auto'))\n      end\n      operation_config['resilience'] = cfg[:execution]\n\n      # Caching key is derived from local_input\n      if cfg[:features][:caching][:enabled]\n        cache_key = \"vertex_#{behavior}_#{local_input.to_json.hash}\"\n        if (hit = call('memo_get', cache_key))\n          return hit\n        end\n      end\n\n      pmode = (local_input['prompt_mode'] || 'simple').to_s\n      case pmode\n      when 'contents'\n        operation_config['payload']   = { 'format' => 'vertex_contents' }\n        operation_config['validate']  = { 'schema' => [ { 'name' => 'contents', 'required' => true } ] }\n      when 'raw_json'\n        operation_config['payload']   = { 'format' => 'vertex_passthrough' }\n        operation_config['validate']  = { 'schema' => [ { 'name' => 'payload_json', 'required' => true } ] }\n      else\n        # keep the config_template default ('vertex_prompt')\n      end\n\n      # Gate index discovery\n      if behavior == 'vector.find_neighbors'\n        local_input = call('augment_vector_context', connection, local_input)\n      end\n\n      result = call('execute_pipeline', connection, behavior, local_input, operation_config)\n\n      if cfg[:features][:caching][:enabled]\n        call('memo_put', cache_key, result, cfg[:features][:caching][:ttl] || 300)\n      end\n\n    "}
{"id":"n_f833a200fc51f529","kind":"method","name":"add_upsert_ack","fqname":"connector.Vertex AI/methods.methods/method.add_upsert_ack","loc":{"line":1815,"column":20,"length":304,"begin":76213,"end":76517},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"add_upsert_ack: lambda do |response, input|\n      # response is empty on success; return a useful ack\n      {\n        'ack'         => 'upserted',\n        'count'       => Array(input['datapoints']).size,\n        'index'       => input['index'],\n        'empty_body'  => (response.nil? || response == {})"}
{"id":"n_bfd076c154d47a15","kind":"method","name":"add_word_count","fqname":"connector.Vertex AI/methods.methods/method.add_word_count","loc":{"line":1825,"column":20,"length":290,"begin":76540,"end":76830},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"add_word_count: lambda do |response, input|\n      if response.is_a?(String)\n        { \n          'result' => response,\n          'word_count' => response.split.size\n        }\n      else\n        {\n          'result' => response,\n          'word_count' => response.to_s.split.size\n        }\n "}
{"id":"n_8c827b6edaebcdbb","kind":"method","name":"apply_template","fqname":"connector.Vertex AI/methods.methods/method.apply_template","loc":{"line":1840,"column":20,"length":238,"begin":76884,"end":77122},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"apply_template: lambda do |template, variables|\n      return template unless template && variables\n      \n      result = template.dup\n      variables.each do |key, value|\n        result = result.gsub(\"{#{key}}\", value.to_s)\n      end\n    "}
{"id":"n_9d13fc2bf8078032","kind":"method","name":"approx_token_count","fqname":"connector.Vertex AI/methods.methods/method.approx_token_count","loc":{"line":1851,"column":24,"length":107,"begin":77184,"end":77291},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ox_token_count: lambda do |text|\n      # Fast, side-effect-free approximation\n      ((text.to_s.length) / 4"}
{"id":"n_925548b6f4bc6f8c","kind":"method","name":"augment_vector_context","fqname":"connector.Vertex AI/methods.methods/method.augment_vector_context","loc":{"line":1856,"column":28,"length":711,"begin":77322,"end":78033},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"vector_context: lambda do |connection, local_input|\n      out = call('deep_copy', local_input)\n      need_metric = !call('value_present', out['distance_metric'])\n      need_norm   = !call('value_present', out['feature_norm_type'])\n      return out unless need_metric || need_norm\n\n      # Only attempt admin discovery if the connection is allowed\n      return out unless connection['allow_admin_discovery'] == true\n\n      begin\n        disc = call('discover_index_config', connection, out)\n        out['distance_metric']   ||= disc['distance_metric']\n        out['feature_norm_type'] ||= disc['feature_norm_type']\n      rescue\n        # Soft‑fail; confidence will be nil but neighbors still returned\n      end"}
{"id":"n_17c58a6a0852b6b7","kind":"method","name":"build_endpoint_url","fqname":"connector.Vertex AI/methods.methods/method.build_endpoint_url","loc":{"line":1876,"column":24,"length":3050,"begin":78085,"end":81135},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ild_endpoint_url: lambda do |connection, endpoint_config, input|\n      v = connection['version']\n      api_version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n      region = connection['region']\n      base_regional = \"https://#{region}-aiplatform.googleapis.com/#{api_version}\"\n\n      family = endpoint_config['family']\n\n      case family\n      # PUBLISHER MODELS\n      when 'publisher_models'\n        api_version = (connection['version'].to_s.strip.empty? ? 'v1' : connection['version'])\n        publisher   = endpoint_config['publisher'] || 'google'\n        \"https://aiplatform.googleapis.com/#{api_version}/publishers/#{publisher}/models\"\n      # VECTOR INDEXES\n      when 'vector_indexes' # admin/data-plane ops on Index resources\n        index = call('qualify_resource', connection, 'index', input['index'] || endpoint_config['index'])\n        \"#{base_regional}/#{index}#{endpoint_config['path']}\" # e.g., ':upsertDatapoints'\n      # VECTOR INDEX ENDPOINTS\n      when 'vector_index_endpoints' # query via MatchService or admin reads\n        base =\n          if endpoint_config['admin'] == true\n            v = connection['version']; version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n            \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n          else\n            call('vector_search_base', connection, input) # uses vdb host when provided\n          end\n        ie = call('qualify_resource', connection, 'index_endpoint',\n                  input['index_endpoint'] || endpoint_config['index_endpoint'])\n        \"#{base}/#{ie}#{endpoint_config['path']}\" # e.g., ':findNeighbors' or ''\n\n\n      else\n        base_host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n        base_url  = \"https://#{base_host}/#{api_version}\"\n\n        model   = input['model'] || connection['default_model'] || 'gemini-1.5-flash'\n        model_id = model.to_s\n\n        # Honor lock model revision input flag\n        lock_rev = input['lock_model_revision'] == true || endpoint_config['require_version'] == true\n        if lock_rev && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n        # Only resolve to a numeric version when explicitly requested by endpoint config\n        if endpoint_config['require_version'] == true && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n\n        model_path = \"projects/#{connection['project']}/locations/#{region}/publishers/google/models/#{model_id}\"\n\n        # If the user supplies a custom path, replace the the critical elements with those from the connection\n        if endpoint_config['custom_path']\n          endpoint_config['custom_path']\n            .gsub('{project}',  connection['project'])\n            .gsub('{region}',   region)\n            .gsub('{endpoint}', connection['vector_search_endpoint'] || '')\n        else\n          \"#{base_url}/#{model_path}#{endpoint_config['path'] || ':generateContent'}\"\n        end"}
{"id":"n_3fdea89359c9c2d6","kind":"method","name":"build_generation_config","fqname":"connector.Vertex AI/methods.methods/method.build_generation_config","loc":{"line":1939,"column":29,"length":329,"begin":81171,"end":81500},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"eneration_config: lambda do |vars|\n      {\n        'temperature'     => vars['temperature'] || 0.7,\n        'maxOutputTokens' => vars['max_tokens']  || 2048,\n        'topP'            => vars['top_p']       || 0.95,\n        'topK'            => vars['top_k']       || 40,\n        'stopSequences'   => vars['stop_sequences']\n     "}
{"id":"n_c069b0e47f02acde","kind":"method","name":"build_headers","fqname":"connector.Vertex AI/methods.methods/method.build_headers","loc":{"line":1950,"column":19,"length":147,"begin":81550,"end":81697},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"   build_headers: lambda do |connection|\n      {\n        'Content-Type' => 'application/json',\n        'X-Goog-User-Project' => connection['project"}
{"id":"n_4cd0d50b5132a56e","kind":"method","name":"check_rate_limit","fqname":"connector.Vertex AI/methods.methods/method.check_rate_limit","loc":{"line":1958,"column":22,"length":661,"begin":81742,"end":82403},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"check_rate_limit: lambda do |operation, limits|\n      rpm  = (limits['rpm'] || limits[:rpm]).to_i\n      window_id     = Time.now.to_i / 60\n      window_start  = window_id * 60\n      key           = \"rate_#{operation}_#{window_id}\"\n\n      count = call('memo_get', key) || 0\n      error(\"Rate limit exceeded for #{operation}. Please wait before retrying.\") if count >= rpm\n\n      new_count = count + 1\n      reset_in  = (window_start + 60) - Time.now.to_i\n      reset_in  = 60 if reset_in <= 0\n\n      call('memo_put', key, new_count, reset_in)\n\n      { 'rpm' => rpm, 'count' => new_count, 'reset_in_s' => reset_in, 'window_started_at' => Time.at(window_start).utc"}
{"id":"n_0fa3ed9ab2f48f04","kind":"method","name":"chunk_by_tokens","fqname":"connector.Vertex AI/methods.methods/method.chunk_by_tokens","loc":{"line":1976,"column":21,"length":1696,"begin":82427,"end":84123},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" chunk_by_tokens: lambda do |items:, token_ceiling:, max_items:, max_body_bytes: nil|\n      token_cap = token_ceiling.to_i\n      token_cap = 8000 if token_cap <= 0 # conservative fallback if not provided\n      max_items = (max_items || 100).to_i\n      max_items = 1 if max_items <= 0\n      max_body  = max_body_bytes ? max_body_bytes.to_i : nil\n\n      batches   = []\n      oversized = []\n\n      current       = []\n      current_tokens= 0\n      current_bytes = 0\n\n      # crude but steady overheads so we don’t undercount request size\n      per_item_overhead = 64\n      base_overhead     = 512\n\n      items.each do |item|\n        txt = item['text'].to_s\n        t   = call('approx_token_count', txt)\n        b   = txt.bytesize + per_item_overhead\n\n        # single-item guards\n        if t > token_cap\n          oversized << { 'item' => item, 'reason' => \"estimated tokens #{t} exceed ceiling #{token_cap}\" }\n          next\n        end\n        if max_body && (b + base_overhead) > max_body\n          oversized << { 'item' => item, 'reason' => \"approx body bytes #{b + base_overhead} exceed limit #{max_body}\" }\n          next\n        end\n\n        # would adding this item break any limit?\n        if !current.empty? &&\n          (current_tokens + t > token_cap ||\n            current.length + 1 > max_items ||\n            (max_body && current_bytes + b + base_overhead > max_body))\n          batches << current\n          current        = []\n          current_tokens = 0\n          current_bytes  = 0\n        end\n\n        current << item\n        current_tokens += t\n        current_bytes  += b\n      end\n\n      batches << current unless current.empty?\n\n      { 'batches' => batches, 'oversized' =>"}
{"id":"n_15eed216496d696d","kind":"method","name":"coerce_embeddings_to_datapoints","fqname":"connector.Vertex AI/methods.methods/method.coerce_embeddings_to_datapoints","loc":{"line":2031,"column":37,"length":1241,"begin":84227,"end":85468},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ings_to_datapoints: lambda do |vars|\n      embeddings = Array(vars['embeddings'])\n      error('No embeddings provided') if embeddings.empty?\n\n      ids     = Array(vars['datapoint_ids'])\n      prefix  = (vars['datapoint_id_prefix'] || 'dp_').to_s\n      start   = (vars['start_index'] || 1).to_i\n      pad_to  = (vars['pad_to'] || 6).to_i\n\n      if ids.empty?\n        ids = embeddings.each_index.map { |i| \"#{prefix}#{(start + i).to_s.rjust(pad_to, '0')}\" }\n      elsif ids.length != embeddings.length\n        error(\"datapoint_ids length (#{ids.length}) must match embeddings length (#{embeddings.length})\")\n      end\n\n      common_restricts        = vars['common_restricts']\n      common_numeric          = vars['common_numeric_restricts']\n      common_crowding_tag     = vars['common_crowding_tag']\n      common_embedding_meta   = vars['embedding_metadata']\n\n      embeddings.each_with_index.map do |vec, i|\n        {\n          'datapointId'       => ids[i],\n          'featureVector'     => Array(vec).map(&:to_f),\n          'restricts'         => common_restricts,\n          'numericRestricts'  => common_numeric,\n          'crowdingTag'       => common_crowding_tag,\n          'embeddingMetadata' => common_embedding_meta\n        }.compa"}
{"id":"n_bc999617c9c294a3","kind":"method","name":"coerce_kwargs","fqname":"connector.Vertex AI/methods.methods/method.coerce_kwargs","loc":{"line":2063,"column":19,"length":886,"begin":85490,"end":86376},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"\n    coerce_kwargs: lambda do |*args, **kwargs|\n      # Non-destructive copies\n      positional = args.dup\n      kw = kwargs.dup\n\n      # If caller passed a trailing Hash, treat it as kwargs (merged with explicit kwargs)\n      if positional.last.is_a?(Hash)\n        trailing = positional.pop\n        # deep copy to avoid side-effects\n        trailing_copy = JSON.parse(JSON.dump(trailing)) rescue trailing.dup\n        trailing_sym  = trailing_copy.each_with_object({}) do |(k, v), acc|\n          key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n          acc[key] = v\n        end\n        # Explicit kwargs take precedence\n        kw = trailing_sym.merge(kw) { |_key, left, right| right }\n      end\n\n      # Ensure symbolized keys for kwargs\n      kw = kw.each_with_object({}) do |(k, v), acc|\n        key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n        acc[key] = v\n      end\n\n      [pos"}
{"id":"n_b669d9b85285a1a7","kind":"method","name":"confidence_from_distance","fqname":"connector.Vertex AI/methods.methods/method.confidence_from_distance","loc":{"line":2090,"column":30,"length":615,"begin":86413,"end":87028},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ence_from_distance: lambda do |distance, metric, feature_norm_type|\n      return nil unless distance\n      m = metric.to_s\n      case m\n      when 'COSINE_DISTANCE'\n        # distance = 1 - cos_sim  => confidence = (1 + cos_sim)/2 = 1 - distance/2\n        c = 1.0 - (distance.to_f / 2.0)\n        [[c, 0.0].max, 1.0].min\n      when 'DOT_PRODUCT_DISTANCE'\n        # distance = -dot; if vectors were UNIT_L2_NORM, dot ∈ [-1,1] ~ cos_sim\n        if feature_norm_type.to_s == 'UNIT_L2_NORM'\n          dot = -distance.to_f\n          c = 0.5 * (1.0 + dot)\n          [[c, 0.0].max, 1.0].min\n        end\n      else\n       "}
{"id":"n_92f7b1422419cdd5","kind":"method","name":"deep_copy","fqname":"connector.Vertex AI/methods.methods/method.deep_copy","loc":{"line":2111,"column":15,"length":43,"begin":87076,"end":87119},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"object\n    deep_copy: lambda { |obj| JSON.p"}
{"id":"n_f42dd856158874f0","kind":"method","name":"discover_index_config","fqname":"connector.Vertex AI/methods.methods/method.discover_index_config","loc":{"line":2113,"column":27,"length":1500,"begin":87149,"end":88649},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"iscover_index_config: lambda do |connection, input|\n      ep = call('qualify_resource', connection, 'index_endpoint', input['index_endpoint'])\n      dep_id = input['deployed_index_id'].to_s\n      return {} if ep.to_s.empty? || dep_id.empty?\n\n      cache_key = \"idxcfg:#{ep}:#{dep_id}\"\n      if (hit = call('memo_get', cache_key)); return hit; end\n\n      # 1) Read IndexEndpoint (admin host)\n      url_ep = call('build_endpoint_url', connection, {\n        'family' => 'vector_index_endpoints', 'index_endpoint' => ep, 'method' => 'GET', 'admin' => true\n      }, input)\n      ep_body = call('http_request', connection, method: 'GET', url: url_ep, headers: call('build_headers', connection))\n      deployed = Array(ep_body['deployedIndexes']).find { |d| d['id'] == dep_id }\n      return {} unless deployed && deployed['index']\n\n      # 2) Read Index (admin host)\n      url_idx = call('build_endpoint_url', connection, {\n        'family' => 'vector_indexes', 'index' => deployed['index'], 'method' => 'GET'\n      }, input)\n      idx_body = call('http_request', connection, method: 'GET', url: url_idx, headers: call('build_headers', connection))\n\n      cfg = idx_body.dig('metadata', 'config') || {}\n      out = {\n        'index'              => deployed['index'],\n        'distance_metric'    => (cfg['distanceMeasureType'] || cfg['distance_measure_type']),\n        'feature_norm_type'  => (cfg['featureNormType']     || cfg['feature_norm_type'])\n      }.compact\n\n      call('memo_put', cache_key, out, "}
{"id":"n_9651beb959b3e88c","kind":"method","name":"each_in_scope","fqname":"connector.Vertex AI/methods.methods/method.each_in_scope","loc":{"line":2147,"column":19,"length":273,"begin":88743,"end":89016},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"l)\n    each_in_scope: lambda do |data, scope|\n      s = scope.to_s\n      if s.end_with?('[]')\n        key = s[0..-3] # strip []\n        arr = Array(data[key]) # safe\n        arr.each_with_index.map { |item, idx| [item || {}, \"#{key}[#{idx}]\"] }\n      else\n        [[data, '"}
{"id":"n_3ece5dced310c4de","kind":"method","name":"error_hint","fqname":"connector.Vertex AI/methods.methods/method.error_hint","loc":{"line":2159,"column":16,"length":565,"begin":89054,"end":89619},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ELPER\n    error_hint: lambda do |connection, code, status|\n      c = code.to_i\n      case c\n      when 401\n        # keep small + actionable\n        'Unauthorized. Re‑authenticate; then check project/region, API enablement, and roles.'\n      when 403\n        'Forbidden. Check project/region, API enablement, and roles.'\n      when 404\n        'Not found. Check project/region (feature/model availability) and the resource id.'\n      when 429\n        'Rate limit/quota. Reduce request rate or increase quota. Will honor Retry‑After when present.'\n      else\n   "}
{"id":"n_cd9d44410ca551c6","kind":"method","name":"extract_ids_for_read","fqname":"connector.Vertex AI/methods.methods/method.extract_ids_for_read","loc":{"line":2177,"column":26,"length":984,"begin":89717,"end":90701},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"    extract_ids_for_read: lambda do |vars|\n      mode = (vars['id_source'] || 'auto').to_s\n      pick = lambda do |source|\n        case source\n        when 'manual'\n          Array(vars['ids']).compact\n        when 'neighbors'\n          Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact\n        when 'groups'\n          Array(vars['groups'])\n            .flat_map { |g| Array(g['neighbors']) }\n            .map { |n| n['datapoint_id'] }.compact\n        else # auto: prefer manual → neighbors → groups\n          ids = Array(vars['ids']).compact\n          ids = Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids = Array(vars['groups']).flat_map { |g| Array(g['neighbors']) }.map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids\n        end\n      end\n\n      ids = pick.call(mode).map(&:to_s)\n      ids = ids.uniq if vars['unique'] != false\n      error('No datapoint IDs provided or derivable from neighbors/groups') i"}
{"id":"n_f89ff50d1c180311","kind":"method","name":"extract_user_config","fqname":"connector.Vertex AI/methods.methods/method.extract_user_config","loc":{"line":2204,"column":25,"length":1397,"begin":90769,"end":92166},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"fely\n    extract_user_config: lambda do |input, cfg_enabled = false, config_ctx = {}|\n      cfg = {}\n      config_ctx ||= {}\n\n      # Prefer config_fields values; fall back to input (back-compat)\n      mode = (config_ctx['model_mode'] || input['model_mode'] || '').to_s\n      explicit_model = config_ctx['model'] || input['model'] || input['model_override']\n\n      case mode\n      when 'explicit'\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      when 'connection', 'auto', ''\n        # no-op; use selection logic defaults\n      else\n        # unknown mode: treat as legacy explicit if model present\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      end\n\n      cfg['model_mode']          = mode unless mode.empty?\n      # After (prefer config_fields, fall back to input for completeness)\n      if config_ctx.key?('lock_model_revision')\n        cfg['lock_model_revision'] = config_ctx['lock_model_revision']\n      elsif input.key?('lock_model_revision')\n        cfg['lock_model_revision'] = input['lock_model_revision']\n      end\n\n      # Advanced tuning (unchanged)\n      if cfg_enabled\n        cfg['temperature'] = input['temperature'] if input.key?('temperature')\n        cfg['max_tokens']  = input['max_tokens']  if input.key?('max_tokens')\n        cfg['cache_ttl']   = input['cache_ttl']   if input.key?('cache_ttl')\n      "}
{"id":"n_24530d8c73551fc9","kind":"method","name":"execute_batch_behavior","fqname":"connector.Vertex AI/methods.methods/method.execute_batch_behavior","loc":{"line":2241,"column":28,"length":2671,"begin":92221,"end":94892},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"n\n    execute_batch_behavior: lambda do |connection, behavior, items, batch_size, strategy, options = {}|\n      results = []\n      errors = []\n      total_processed = 0\n\n      local_items = call('deep_copy', Array(items))\n      \n      # 1) Build batches according to strategy\n      batches =\n        if strategy.to_s == 'tokens'\n          chunk = call('chunk_by_tokens',\n            items: local_items,\n            token_ceiling: (options['token_ceiling'] || options[:token_ceiling]),\n            max_items: (options['max_items_per_batch'] || options[:max_items_per_batch] || 100),\n            max_body_bytes: (options['max_body_bytes'] || options[:max_body_bytes])\n          )\n          # surface oversize items as per-batch errors (unchanged error shape: batch + error)\n          Array(chunk['oversized']).each do |o|\n            errors << { 'batch' => [o['item']], 'error' => \"Skipped item: #{o['reason']}\" }\n          end\n          chunk['batches'] || []\n        else\n          size  = (batch_size || 10).to_i\n          limit = (options['max_items_per_batch'] || options[:max_items_per_batch] || size).to_i\n          size  = [[size, limit].min, 1].max\n          local_items.each_slice(size).to_a\n        end\n\n      # 2) Execute batches\n      batches.each do |batch|\n        begin\n          if behavior.include?('embed')\n            texts = batch.map { |item| item['text'] }\n\n            payload = { 'texts' => texts }\n            unique_tasks = batch.map { |i| i['task_type'] }.compact.uniq\n            payload['task_type'] = unique_tasks.first if unique_tasks.length == 1\n\n            batch_result = call('execute_behavior', connection, behavior, payload)\n\n            # For embeddings, API is truly batchable: one result per batch (keep prior shape)\n            results.concat([batch_result])\n            total_processed += batch.length\n\n          else\n            # Non-embeddings: execute per-item so partial failures are surfaced\n            batch.each do |item|\n              begin\n                item_result = call('execute_behavior', connection, behavior, item)\n                results << item_result\n                total_processed += 1\n              rescue => e\n                errors << { 'batch' => [item], 'error' => e.message }\n              end\n            end\n          end\n\n        rescue => e\n          # catastrophic batch failure (network, quota, etc.)\n          errors << { 'batch' => batch, 'error' => e.message }\n        end\n      end\n      \n      {\n        'success'         => errors.empty?,\n        'results'         => results,\n        'errors'          => errors,\n        'total_processed' => total_processed,\n        'total_errors'    =>"}
{"id":"n_6288e3821588d03a","kind":"method","name":"format_user_error","fqname":"connector.Vertex AI/methods.methods/method.format_user_error","loc":{"line":2314,"column":23,"length":400,"begin":94939,"end":95339},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"HELPER\n    format_user_error: lambda do |err|\n      base = \"Vertex AI error #{err['code']}\"\n      base += \" #{err['status']}\" if err['status']\n      head = \"#{base}: #{err['summary']}\"\n      tags = [\"corr_id=#{err['correlation_id']}\"]\n      tags << \"remote_id=#{err['remote_request_id']}\" if err['remote_request_id']\n      msg = \"#{head} [#{tags.join(' ')}]\"\n      msg += \" — Hint: #{err['hint']}\" "}
{"id":"n_70d6d579013c1a6b","kind":"method","name":"get_behavior_input_fields","fqname":"connector.Vertex AI/methods.methods/method.get_behavior_input_fields","loc":{"line":2326,"column":31,"length":19673,"begin":95417,"end":115090},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"\n    get_behavior_input_fields: lambda do |behavior, show_advanced, ui_cfg = {}|\n      show_advanced = !!show_advanced\n      ui_cfg ||= {}\n      explicit      = (ui_cfg['model_mode'] == 'explicit')\n      legacy_mode   = !ui_cfg.key?('model_mode')\n      include_model = false\n\n      behavior_def = call('behavior_registry')[behavior]\n      return [] unless behavior_def\n      \n      # Map behavior to input fields\n      case behavior\n      when 'text.generate'\n        fields = [\n          { name: 'prompt', label: 'Prompt', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.translate'\n        fields = [\n          { name: 'text', label: 'Text to Translate', control_type: 'text-area', optional: false },\n          { name: 'target_language', label: 'Target Language', control_type: 'select', pick_list: 'languages', optional: false },\n          { name: 'source_language', label: 'Source Language', control_type: 'select', pick_list: 'languages', optional: true, hint: 'Leave blank for auto-detection' }\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.summarize'\n        fields = [\n          { name: 'text', label: 'Text to Summarize', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.classify'\n        fields = [\n          { name: 'text', label: 'Text to Classify', control_type: 'text-area', optional: false },\n          { name: 'categories', label: 'Categories', type: 'array', of: 'object', properties: [\n            { name: 'name', label: 'Category Name' },\n            { name: 'description', label: 'Description' }\n          ]}\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.embed'\n        fields = [\n          { name: 'texts', label: 'Texts to Embed', type: 'array', of: 'string', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'vector.upsert_datapoints'\n        fields = [\n          # Target\n          { name: 'index', label: 'Index', group: 'Target', hint: 'Index resource or ID (e.g., projects/.../indexes/IDX or just IDX)', optional: false },\n          # Source: from embeddings (recommended path from Generate embeddings)\n          { name: 'embeddings', label: 'Embeddings', group: 'Source (from embeddings)', type: 'array', of: 'array', optional: true,\n            hint: 'Map from Generate embeddings → vectors or embeddings' },\n          { name: 'datapoint_ids', label: 'Datapoint IDs', group: 'Source (from embeddings)', type: 'array', of: 'string', \n            optional: true, hint: 'Optional; if omitted, IDs are auto-generated' },\n          { name: 'datapoint_id_prefix', label: 'Auto ID prefix', group: 'Source (from embeddings)', optional: true, default: 'dp_' },\n          { name: 'start_index', label: 'Starting index (1-based)', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 1 },\n          { name: 'pad_to', label: 'Pad IDs to N digits', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 6 },\n\n          # Datapoint defaults applied to all when using embeddings\n          { name: 'common_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n          ]},\n          { name: 'common_numeric_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n            { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n          ]},\n          { name: 'common_crowding_tag', group: 'Datapoint defaults', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n          { name: 'embedding_metadata', group: 'Datapoint defaults', type: 'object' },\n\n          # Advanced: provide full datapoints directly (legacy / power-user)\n          { name: 'datapoints', label: 'Datapoints (advanced)', group: 'Provide full datapoints',\n            type: 'array', of: 'object', optional: true, properties: [\n              { name: 'datapoint_id', label: 'Datapoint ID', optional: false },\n              { name: 'feature_vector', label: 'Feature vector', type: 'array', of: 'number', optional: false },\n              { name: 'restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n              ]},\n              { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n                { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n              ]},\n              { name: 'crowding_tag', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n              { name: 'embedding_metadata', type: 'object' }\n            ]}\n        ]\n      when 'vector.find_neighbors'\n        fields = [\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Overrides connection host just for this call (e.g. <hash>....vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', hint: 'Resource or ID (e.g. projects/.../indexEndpoints/IEP or IEP)', optional: false, group: 'Target' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n          { name: 'neighbor_count', label: 'Neighbors per query', type: 'integer', default: 10, group: 'Query' },\n          { name: 'return_full_datapoint', label: 'Return full datapoint', control_type: 'checkbox', group: 'Query' },\n\n          # NEW: scoring & aggregates\n          { name: 'distance_metric', label: 'Index distance metric', control_type: 'select',\n            pick_list: 'vector_distance_metrics', optional: true, group: 'Scoring & aggregates',\n            hint: 'Set if you want valid confidence scores. For DOT_PRODUCT, set Feature normalization to UNIT_L2_NORM.' },\n          { name: 'feature_norm_type', label: 'Feature normalization', control_type: 'select',\n            pick_list: 'vector_feature_norm_types', optional: true, group: 'Scoring & aggregates' },\n          { name: 'include_stats', label: 'Include aggregate stats', control_type: 'checkbox',\n            default: true, optional: true, group: 'Scoring & aggregates' },\n\n          { name: 'queries', label: 'Queries', type: 'array', of: 'object', group: 'Queries', properties: [\n            { name: 'datapoint_id', label: 'Query datapoint ID' },\n            { name: 'feature_vector', label: 'Query vector', type: 'array', of: 'number', hint: 'Use either vector or datapoint_id' },\n            { name: 'neighbor_count', label: 'Override neighbors for this query', type: 'integer' },\n            { name: 'restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n            ]},\n            { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' }, { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        mode = (ui_cfg['id_source'] || 'auto').to_s\n        fields = [\n          # Target\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Optional override (e.g. <hash>.vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, group: 'Target', hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n        ]\n        # Helper lambdas to append groups\n        add_manual = lambda {\n          fields << { name: 'ids', label: 'Datapoint IDs (manual)',\n                      type: 'array', of: 'string', optional: true, group: 'IDs' }\n        }\n        add_neighbors = lambda {\n          fields << { name: 'neighbors', label: 'k‑NN neighbors (flattened)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [{ name: 'datapoint_id' }] }\n          fields << { name: 'unique', label: 'Deduplicate IDs',\n                      control_type: 'checkbox', default: true, group: 'Map from Find neighbors' }\n        }\n        add_groups = lambda {\n          fields << { name: 'groups', label: 'k‑NN groups (from Find neighbors)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [\n                        "}
{"id":"n_c86870237860409b","kind":"method","name":"get_behavior_output_fields","fqname":"connector.Vertex AI/methods.methods/method.get_behavior_output_fields","loc":{"line":2646,"column":32,"length":3245,"begin":115162,"end":118407},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ields\n    get_behavior_output_fields: lambda do |behavior|\n      case behavior\n      when 'text.generate'\n        [{ name: 'result', label: 'Generated Text' }]\n      when 'text.translate'\n        [\n          { name: 'result', label: 'Translated Text' },\n          { name: 'detected_language', label: 'Detected Source Language' }\n        ]\n      when 'text.summarize'\n        [\n          { name: 'result', label: 'Summary' },\n          { name: 'word_count', type: 'integer' }\n        ]\n      when 'text.classify'\n        [\n          { name: 'category', label: 'Selected Category' },\n          { name: 'confidence', type: 'number' }\n        ]\n      when 'text.embed'\n        [\n          { name: 'embeddings', type: 'array', of: 'array' },\n          { name: 'vectors', type: 'array', of: 'object', properties: [ { name: 'feature_vector', type: 'array', of: 'number' } ]},\n          { name: 'count', type: 'integer' },\n          { name: 'dimension', type: 'integer' },\n          { name: 'avg_norm', type: 'number' },\n          { name: 'norms', type: 'array', of: 'number' } \n        ]\n      when 'vector.upsert_datapoints'\n        [\n          { name: 'ack' }, { name: 'count', type: 'integer' }, { name: 'index' }, { name: 'empty_body', type: 'boolean' }\n        ]\n      when 'vector.find_neighbors'\n        [\n          { name: 'summary', type: 'object', properties: [\n            { name: 'groups', type: 'integer' },\n            { name: 'neighbors', type: 'integer' },\n            { name: 'distance_mean', type: 'number' },\n            { name: 'score_mean', type: 'number' },\n            { name: 'score_max',  type: 'number' },\n            { name: 'confidence_mean', type: 'number' },\n            { name: 'confidence_max',  type: 'number' }\n          ]},\n          { name: 'groups', type: 'array', of: 'object', properties: [\n            { name: 'query_id' },\n            { name: 'stats', type: 'object', properties: [\n              { name: 'neighbor_count', type: 'integer' },\n              { name: 'distance_mean', type: 'number' },\n              { name: 'score_mean',     type: 'number' },\n              { name: 'score_max',      type: 'number' },\n              { name: 'confidence_mean', type: 'number' },\n              { name: 'confidence_max',  type: 'number' }\n            ]},\n            { name: 'neighbors', type: 'array', of: 'object', properties: [\n              { name: 'datapoint_id' },\n              { name: 'distance', type: 'number' },\n              { name: 'score',    type: 'number' },\n              { name: 'confidence', type: 'number' },\n              { name: 'datapoint', type: 'object' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        [\n          { name: 'datapoints', type: 'array', of: 'object', properties: [\n            { name: 'datapoint_id' },\n            { name: 'feature_vector', type: 'array', of: 'number' },\n            { name: 'restricts', type: 'array', of: 'object' },\n            { name: 'numeric_restricts', type: 'array', of: 'object' },\n            { name: 'crowding_tag', type: 'object' },\n            { name: 'embedding_metadata', type: 'object' }\n          ] }\n        ]\n      when 'multimodal.analyze'\n        [{ name: 'result', label: 'Analysis' }]\n      else\n        "}
{"id":"n_b23ce8da46845469","kind":"method","name":"list_publisher_models","fqname":"connector.Vertex AI/methods.methods/method.list_publisher_models","loc":{"line":2727,"column":27,"length":758,"begin":118482,"end":119240},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" (v1beta1)\n    list_publisher_models: lambda do |connection, publisher: 'google'|\n      ver = connection['version'].to_s.strip\n      ver = ver.empty? ? 'v1' : ver\n      cache_key = \"pub_models:#{publisher}:#{ver}\"   # <— include version in key\n\n      if (cached = call('memo_get', cache_key))\n        return cached\n      end\n\n      url = call('build_endpoint_url', connection, { 'family' => 'publisher_models', 'publisher' => publisher }, {})\n      resp = call('http_request', connection, method: 'GET', url: url,\n                  headers: call('build_headers', connection),\n                  retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429, 500, 502, 503, 504] })\n\n      models = (resp['publisherModels'] || [])\n      call('memo_put', cache"}
{"id":"n_c30c802daceb012b","kind":"method","name":"memo_store","fqname":"connector.Vertex AI/methods.methods/method.memo_store","loc":{"line":2746,"column":16,"length":25,"begin":119259,"end":119284},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"\n      models\n    end,\n\n "}
{"id":"n_46020d74cfb9c27e","kind":"method","name":"memo_get","fqname":"connector.Vertex AI/methods.methods/method.memo_get","loc":{"line":2748,"column":14,"length":178,"begin":119301,"end":119479},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"mbda { @__memo ||= {} },\n\n    memo_get: lambda do |key|\n      item = call('memo_store')[key]\n      return nil unless item\n      exp = item['exp']\n      return nil if exp && Time."}
{"id":"n_22b5054d2258d8eb","kind":"method","name":"memo_put","fqname":"connector.Vertex AI/methods.methods/method.memo_put","loc":{"line":2756,"column":14,"length":145,"begin":119496,"end":119641},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"    item['val']\n    end,\n\n    memo_put: lambda do |key, val, ttl=nil|\n      call('memo_store')[key] = { 'val' => val, 'exp' => (ttl ? Time.now.to"}
{"id":"n_c67de61d21baba1a","kind":"method","name":"normalize_find_neighbors","fqname":"connector.Vertex AI/methods.methods/method.normalize_find_neighbors","loc":{"line":2762,"column":30,"length":2416,"begin":119750,"end":122166},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"dly shape\n    normalize_find_neighbors: lambda do |resp, input|\n      groups_raw = Array(resp['nearestNeighbors'])\n      metric     = input['distance_metric']\n      norm_type  = input['feature_norm_type']\n      include_stats = input.key?('include_stats') ? !!input['include_stats'] : true\n\n      groups = groups_raw.map do |nn|\n        neighbors = Array(nn['neighbors']).map do |n|\n          dist = n['distance']\n          did  = n.dig('datapoint', 'datapointId')\n          {\n            'datapoint_id' => did,\n            'distance'     => dist,\n            # Legacy score: normalized from distance (cosine heuristic)\n            'score'        => call('transform_data', input: dist, from_format: 'distance', to_format: 'similarity'),\n            # New: mathematically valid confidence when possible\n            'confidence'   => call('confidence_from_distance', dist, metric, norm_type),\n            'datapoint'    => n['datapoint']\n          }.compact\n        end\n\n        stats =\n          if include_stats\n            {\n              'neighbor_count'   => neighbors.length,\n              'distance_mean'    => call('safe_mean', neighbors.map { |z| z['distance'] }),\n              'score_mean'       => call('safe_mean', neighbors.map { |z| z['score'] }),\n              'score_max'        => (neighbors.map { |z| z['score'] }.compact.max),\n              'confidence_mean'  => call('safe_mean', neighbors.map { |z| z['confidence'] }),\n              'confidence_max'   => (neighbors.map { |z| z['confidence'] }.compact.max)\n            }.compact\n          end\n\n        {\n          'query_id'  => nn['id'],\n          'stats'     => stats,\n          'neighbors' => neighbors\n        }.compact\n      end\n\n      # Top-level summary if desired\n      summary =\n        if include_stats\n          flat = groups.flat_map { |g| g['neighbors'] || [] }\n          {\n            'groups'          => groups.length,\n            'neighbors'       => flat.length,\n            'distance_mean'   => call('safe_mean', flat.map { |z| z['distance'] }),\n            'score_mean'      => call('safe_mean', flat.map { |z| z['score'] }),\n            'score_max'       => (flat.map { |z| z['score'] }.compact.max),\n            'confidence_mean' => call('safe_mean', flat.map { |z| z['confidence'] }),\n            'confidence_max'  => (flat.map { |z| z['confidence'] }.compact.max)\n          }.compact\n        end\n\n      { 'summary' => summa"}
{"id":"n_4d9184244f0322ac","kind":"method","name":"normalize_http_error","fqname":"connector.Vertex AI/methods.methods/method.normalize_http_error","loc":{"line":2820,"column":26,"length":1256,"begin":122195,"end":123451},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"act\n    end,\n\n    normalize_http_error: lambda do |connection, code:, body:, headers:, message:, url:, corr_id:, attempt:, duration_ms:|\n      parsed = {}\n      if body.is_a?(Hash)\n        parsed = body\n      else\n        begin\n          parsed = JSON.parse(body.to_s)\n        rescue\n          parsed = {}\n        end\n      end\n\n      gerr    = parsed['error'].is_a?(Hash) ? parsed['error'] : {}\n      status  = gerr['status']\n      summary = (gerr['message'] || message || body.to_s).to_s.strip[0, 300] # compact\n      hint    = call('error_hint', connection, code, status)\n\n      remote_id = nil\n      if headers\n        remote_id = headers['x-request-id'] ||\n                    headers['x-cloud-trace-context'] ||\n                    headers['x-guploader-uploadid']\n      end\n\n      {\n        'code'              => code.to_i,\n        'status'            => status,\n        'summary'           => summary,\n        'hint'              => hint,\n        'retryable'         => call('retryable_http_code', code),\n        'retry_after_s'     => call('parse_retry_after', headers),\n        'correlation_id'    => corr_id,\n        'remote_request_id' => remote_id,\n        'attempt'           => attempt,\n        'duration_ms'       => duration_ms,\n        'u"}
{"id":"n_09220d3b79a996aa","kind":"method","name":"normalize_read_index_datapoints","fqname":"connector.Vertex AI/methods.methods/method.normalize_read_index_datapoints","loc":{"line":2859,"column":37,"length":704,"begin":123491,"end":124195},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":",\n\n    normalize_read_index_datapoints: lambda do |resp, _input|\n      # Expected Vertex shape: { \"datapoints\": [ { \"datapointId\": \"...\", \"featureVector\": [...],\n      #   \"restricts\": [...], \"numericRestricts\": [...], \"crowdingTag\": {...}, \"embeddingMetadata\": {...} } ] }\n      dps = Array(resp['datapoints']).map do |d|\n        {\n          'datapoint_id'      => d['datapointId'] || d['id'],\n          'feature_vector'    => Array(d['featureVector']).map(&:to_f),\n          'restricts'         => d['restricts'],\n          'numeric_restricts' => d['numericRestricts'],\n          'crowding_tag'      => d['crowdingTag'],\n          'embedding_metadata'=> d['embeddingMetadata']\n        }.compact\n      e"}
{"id":"n_715f0b4adbf4d9e5","kind":"method","name":"normalize_safety_settings","fqname":"connector.Vertex AI/methods.methods/method.normalize_safety_settings","loc":{"line":2876,"column":31,"length":1080,"begin":124261,"end":125341},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"settings\n    normalize_safety_settings: lambda do |input|\n      # Accepts either the new array shape or the legacy hash; returns array\n      if input.is_a?(Array)\n        # non-destructive copy with only supported keys\n        return input.map do |r|\n          {\n            'category'  => r['category']  || r[:category],\n            'threshold' => r['threshold'] || r[:threshold],\n            'method'    => r['method']    || r[:method]\n          }.compact\n        end\n      end\n\n      # Legacy object: { harassment: 'BLOCK_...', hate_speech: 'BLOCK_...', ... }\n      if input.is_a?(Hash)\n        map = {\n          'harassment'          => 'HARM_CATEGORY_HARASSMENT',\n          'hate_speech'         => 'HARM_CATEGORY_HATE_SPEECH',\n          'sexually_explicit'   => 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n          'dangerous_content'   => 'HARM_CATEGORY_DANGEROUS_CONTENT'\n        }\n        return input.each_with_object([]) do |(k, v), arr|\n          next if v.nil? || v.to_s.strip.empty?\n          cat = map[k.to_s]\n          arr << { 'category' => cat, 'threshold' => v } if cat"}
{"id":"n_421dc603a22e9fb5","kind":"method","name":"parse_retry_after","fqname":"connector.Vertex AI/methods.methods/method.parse_retry_after","loc":{"line":2908,"column":23,"length":257,"begin":125386,"end":125643},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"  # RETRY HELPER\n    parse_retry_after: lambda do |headers|\n      return nil unless headers\n      ra = headers['Retry-After'] || headers['retry-after']\n      return nil if ra.nil? || ra.to_s.strip.empty?\n      # integer seconds only (safe & simple)\n      ra"}
{"id":"n_6ebc8efc175b9297","kind":"method","name":"qualify_resource","fqname":"connector.Vertex AI/methods.methods/method.qualify_resource","loc":{"line":2917,"column":22,"length":415,"begin":125748,"end":126163},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ting caller input\n    qualify_resource: lambda do |connection, type, value|\n      return value if value.to_s.start_with?('projects/')\n      project = connection['project']\n      region  = connection['region']\n      case type.to_s\n      when 'index'          then \"projects/#{project}/locations/#{region}/indexes/#{value}\"\n      when 'index_endpoint' then \"projects/#{project}/locations/#{region}/indexEndpoints/#{va"}
{"id":"n_b66911f16e5dc874","kind":"method","name":"resolve_model_version","fqname":"connector.Vertex AI/methods.methods/method.resolve_model_version","loc":{"line":2929,"column":27,"length":680,"begin":126248,"end":126928},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"on available\n    resolve_model_version: lambda do |connection, short|\n      # If already versioned, keep it\n      return short if short.to_s.match?(/-\\d{3,}$/)\n\n      cache_key = \"model_resolve:#{short}\"\n      if (cached = call('memo_get', cache_key))\n        return cached\n      end\n\n      ids = Array(call('list_publisher_models', connection))\n              .map { |m| (m['name'] || '').split('/').last }\n              .select { |id| id.start_with?(\"#{short}-\") }\n\n      latest = ids.max_by { |id| id[/-(\\d+)$/, 1].to_i }\n\n      # IMPORTANT: if nothing is found, fall back to the alias itself (Vertex supports aliases)\n      chosen = latest || short\n      call('memo_put', cache"}
{"id":"n_d7d9f9252ddfd445","kind":"method","name":"retryable_http_code","fqname":"connector.Vertex AI/methods.methods/method.retryable_http_code","loc":{"line":2951,"column":25,"length":78,"begin":126975,"end":127053},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"# RETRY HELPER\n    retryable_http_code: lambda { |code|\n      [408, 429, 500, "}
{"id":"n_ba8ca8a529ef2680","kind":"method","name":"safe_mean","fqname":"connector.Vertex AI/methods.methods/method.safe_mean","loc":{"line":2955,"column":15,"length":120,"begin":127071,"end":127191},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"lude?(code.to_i)\n    },\n\n    safe_mean: lambda do |arr|\n      xs = Array(arr).compact\n      return nil if xs.empty?\n    "}
{"id":"n_622cde4ad7374526","kind":"method","name":"select_model","fqname":"connector.Vertex AI/methods.methods/method.select_model","loc":{"line":2962,"column":18,"length":2087,"begin":127240,"end":129327},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"Model selection logic\n    select_model: lambda do |behavior_def, cfg, input|\n      # 0) Respect explicit model in put\n      if (input['model'] && !input['model'].to_s.strip.empty?) ||\n        (input['model_override'] && !input['model_override'].to_s.strip.empty?)\n        return input['model'] || input['model_override']\n      end\n\n      mode      = (input['model_mode'] || cfg.dig(:models, :mode) || 'auto').to_s\n      strategy  = (cfg.dig(:models, :strategy) || 'balanced').to_s\n      supported = Array(behavior_def[:supported_models]).compact\n      default   = cfg.dig(:models, :default)\n\n      # Prefer an item if supported, else first supported, else default\n      prefer = lambda do |*candidates|\n        # Choose the first candidate that is in 'supported'; else first supported; else default\n        c = candidates.flatten.compact.find { |m| supported.include?(m) }\n        c || supported.first || default\n      end\n\n      case mode\n      when 'connection'\n        # Only honor connection default if it's supported by this behavior\n        return default if supported.include?(default)\n        return supported.first || default\n      when 'explicit'\n        # If user chose 'explicit' but didn't supply a model, pick a safe supported default\n        return supported.first || default\n      else # 'auto'\n        if behavior_def[:capability].to_s == 'embedding'\n          case strategy\n          when 'cost'        then prefer.call('textembedding-gecko', 'text-embedding-005', 'text-embedding-004')\n          when 'performance' then prefer.call('gemini-embedding-001', 'text-embedding-005', 'textembedding-gecko', 'text-embedding-004')\n          else                    prefer.call('text-embedding-005', 'gemini-embedding-001', 'textembedding-gecko', 'text-embedding-004')\n          end\n        else\n          case strategy\n          when 'cost'        then prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n          when 'performance' then prefer.call('gemini-1.5-pro',   'gemini-1.5-flash')\n          else                    prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n   "}
{"id":"n_ce3e51ca569e8339","kind":"method","name":"telemetry_envelope_fields","fqname":"connector.Vertex AI/methods.methods/method.telemetry_envelope_fields","loc":{"line":3007,"column":31,"length":325,"begin":129409,"end":129734},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"red) ===\n    telemetry_envelope_fields: lambda do\n      [\n        { name: 'success', type: 'boolean' },\n        { name: 'timestamp', type: 'datetime' },\n        { name: 'metadata', type: 'object', properties: [\n          { name: 'operation' }, { name: 'model' }\n        ]},\n        { name: 'trace', type: 'object', properties"}
{"id":"n_78eae672a9979cd4","kind":"method","name":"trace_fields","fqname":"connector.Vertex AI/methods.methods/method.trace_fields","loc":{"line":3017,"column":18,"length":525,"begin":129754,"end":130279},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"') }\n      ]\n    end,\n    trace_fields: lambda do\n      [\n        { name: 'correlation_id' },\n        { name: 'duration_ms', type: 'integer' },\n        { name: 'attempt', type: 'integer' },\n        { name: 'http_status', type: 'integer' },\n        { name: 'remote_request_id' }, \n        { name: 'rate_limit', type: 'object', properties: [\n          { name: 'rpm', type: 'integer' },\n          { name: 'count', type: 'integer' },\n          { name: 'reset_in_s', type: 'integer' },\n          { name: 'window_started_at', type:"}
{"id":"n_d6e43f892de4621a","kind":"method","name":"to_query","fqname":"connector.Vertex AI/methods.methods/method.to_query","loc":{"line":3034,"column":14,"length":579,"begin":130338,"end":130917},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"emetry schema helpers===\n\n    to_query: lambda do |params|\n      encode = lambda do |s|\n        # RFC3986 unreserved: ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n        s.to_s.bytes.map { |b|\n          if (48..57).cover?(b) || (65..90).cover?(b) || (97..122).cover?(b) || [45,46,95,126].include?(b)\n            b.chr\n          else\n            \"%%%02X\" % b\n          end\n        }.join\n      end\n\n      params.flat_map do |k, v|\n        key = encode.call(k)\n        if v.is_a?(Array)\n          v.map { |e| \"#{key}=#{encode.call(e)}\" }\n        else\n          \"#{key}=#{encode.call(v)}\""}
{"id":"n_a4875f92b4fcd209","kind":"method","name":"value_present","fqname":"connector.Vertex AI/methods.methods/method.value_present","loc":{"line":3057,"column":19,"length":174,"begin":131003,"end":131177},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"} treated as absent)\n    value_present: lambda do |v|\n      return false if v.nil?\n      return false if v.is_a?(String) && v.strip.empty?\n      return false if v.respond_to?"}
{"id":"n_3c7495c72c49ca52","kind":"method","name":"vector_search_base","fqname":"connector.Vertex AI/methods.methods/method.vector_search_base","loc":{"line":3065,"column":24,"length":653,"begin":131289,"end":131942},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" when provided.\n    vector_search_base: lambda do |connection, input|\n      host = (input['endpoint_host'] || connection['vector_search_endpoint']).to_s.strip\n      v = connection['version']\n      version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n\n      if host.empty?\n        # Fallback to regional API host (works for admin ops; query should use public vdb host)\n        \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n      elsif host.include?('vdb.vertexai.goog')\n        \"https://#{host}/#{version}\"\n      else\n        # Allow passing a full https://... custom host\n        host = host.sub(%r{\\Ahttps?://}i, '')\n        \"https"}
{"id":"n_4277dd6ab051e7ce","kind":"method","name":"wrap_embeddings_vectors","fqname":"connector.Vertex AI/methods.methods/method.wrap_embeddings_vectors","loc":{"line":3083,"column":29,"length":745,"begin":132052,"end":132797},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"erve trace\n    wrap_embeddings_vectors: lambda do |response, input|\n      raw = if response.is_a?(Hash) && response.key?('result')\n        response['result']\n      else\n        response\n      end\n      arr = Array(raw).map { |v| Array(v).map(&:to_f) }\n\n      norms = arr.map { |v| Math.sqrt(v.reduce(0.0) { |s, x| s + (x.to_f * x.to_f) }) }\n      dim   = arr.first ? arr.first.length : nil\n\n      out = {\n        'embeddings' => arr,\n        'vectors'    => arr.map { |v| { 'feature_vector' => v } },\n        'count'      => arr.length,\n        'dimension'  => dim,\n        'norms'      => norms,\n        'avg_norm'   => call('safe_mean', norms)\n      }.compact\n\n      if response.is_a?(Hash) && response['_trace']\n        out['_trace'] = respon"}
{"id":"n_aaa50cc7b82c38d3","kind":"object_definitions","name":"object_definitions","fqname":"connector.Vertex AI/object_definitions.object_definitions","loc":{"line":3286,"column":22,"length":1004,"begin":138392,"end":139396},"file":null,"keys":[],"http":{},"text":"===================\n  object_definitions: {\n    generation_config: {\n      fields: lambda do |connection|\n        [\n          { name: 'temperature', type: 'number', hint: 'Controls randomness (0-1)' },\n          { name: 'max_tokens', type: 'integer', hint: 'Maximum response length' },\n          { name: 'top_p', type: 'number', hint: 'Nucleus sampling' },\n          { name: 'top_k', type: 'integer', hint: 'Top-k sampling' },\n          { name: 'stop_sequences', type: 'array', of: 'string', hint: 'Stop generation at these sequences' }\n        ]\n      end\n    },\n    \n    safety_settings: {\n      fields: lambda do |_connection|\n        [\n          { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n          { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n          { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true,\n            hint: 'Optional; defaults to model"}
{"id":"n_5428152129bdb2da","kind":"object_definition","name":"generation_config","fqname":"connector.Vertex AI/object_definitions.object_definitions/object_definition.generation_config","loc":{"line":3287,"column":23,"length":495,"begin":138417,"end":138912},"file":null,"keys":[],"http":{},"text":"ect_definitions: {\n    generation_config: {\n      fields: lambda do |connection|\n        [\n          { name: 'temperature', type: 'number', hint: 'Controls randomness (0-1)' },\n          { name: 'max_tokens', type: 'integer', hint: 'Maximum response length' },\n          { name: 'top_p', type: 'number', hint: 'Nucleus sampling' },\n          { name: 'top_k', type: 'integer', hint: 'Top-k sampling' },\n          { name: 'stop_sequences', type: 'array', of: 'string', hint: 'Stop generation at th"}
{"id":"n_928c4d34d3b843aa","kind":"object_definition","name":"safety_settings","fqname":"connector.Vertex AI/object_definitions.object_definitions/object_definition.safety_settings","loc":{"line":3299,"column":21,"length":452,"begin":138940,"end":139392},"file":null,"keys":[],"http":{},"text":"     end\n    },\n    \n    safety_settings: {\n      fields: lambda do |_connection|\n        [\n          { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n          { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n          { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true,\n            hint: 'Optional; defaults to m"}
{"id":"n_ad0dd4944e093039","kind":"actions","name":"actions","fqname":"connector.Vertex AI/actions.actions","loc":{"line":248,"column":11,"length":26772,"begin":10489,"end":37261},"file":null,"keys":[],"http":{},"text":"ns: {\n\n    # ------------------------------------------\n    # CORE\n    # ------------------------------------------\n    # Batch Operation Action\n    batch_operation: {\n      title: 'UNIVERSAL - Batch AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'batchable_behaviors', optional: false },\n        { name: 'batch_strategy', label: 'Batch Strategy', control_type: 'select', default: 'count', options: [['By Count', 'count'], ['By Token Limit', 'tokens']] },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens ≈ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n      end,\n      # SAMPLE\n      sample_output: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n      end\n    },\n    # Universal Action\n    vertex_operation: {\n      title: 'UNIVERSAL - Vertex AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'available_behaviors', optional: false, extends_schema: true,\n          hint: 'Select the AI operation to perform' },\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select',\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          default: 'auto', optional: false, sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.'\n        },\n        { name: 'model',\n          label: 'Model',\n          group: 'Model & tuning',\n          control_type: 'select',\n          sticky: true,\n          optional: true,\n          extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"',\n          pick_list: 'models_dynamic_for_behavior', \n          pick_list_params: { behavior: 'behavior' },  # bind by field by NAME\n          toggle_hint: 'Select from list',\n          toggle_field: {\n            name: 'model',\n            label: 'Model (custom id)',\n            type: 'string',\n            control_type: 'text',\n            optional: true,\n            toggle_hint: 'Provide custom value'\n          } },\n        { name: 'lock_model_revision',\n          label: 'Lock to latest numbered revision',\n          control_type: 'checkbox',\n          group: 'Model & tuning',\n          ngIf: 'input.model_mode == \"explicit\"',\n          hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        behavior = cfg['behavior']\n        behavior ? call('get_behavior_input_fields', behavior, cfg['advanced_config'], cfg) : []\n      end,\n      # OUTPUT\n      output_fields: lambda do |_object_definitions, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('telemetry_envelope_fields') + (call('get_behavior_output_fields', cfg['behavior']) || [])\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior     = config_fields['behavior']\n        user_config  = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input   = call('deep_copy', input) # do NOT mutate Workato’s input\n\n        # Leave advanced fields in safe_input; pipeline reads only what it needs\n        call('execute_behavior', connection, behavior, safe_input, user_config)\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, config_fields|\n        behavior = (config_fields.is_a?(Hash) ? config_fields : {})['behavior']\n        case behavior\n        when 'text.generate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.generate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hello world.\" }\n        when 'text.translate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.translate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hola mundo.\", \"detected_language\"=>\"en\" }\n        when 'text.summarize'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.summarize\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Concise summary.\", \"word_count\"=>2 }\n        when 'text.classify'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.classify\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"category\"=>\"Support\", \"confidence\"=>0.98 }\n        when 'text.embed'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.embed\", \"model\"=>\"text-embedding-004\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"embeddings\"=>[[0.01,0.02,0.03]] }\n        when 'multimodal.analyze'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"multimodal.analyze\", \"model\"=>\"gemini-1.5-pro\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"The image shows a tabby cat on a desk.\" }\n        else\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"unknown\", \"model\"=>\"gemini-1.5-flash\" } }\n        end\n      end\n    },\n\n    # ------------------------------------------\n    # THIN WRAPPERS\n    # ------------------------------------------\n    # Index disovery\n    discover_index_config: {\n      title: 'VECTOR SEARCH - Discover index configuration',\n      description: 'Reads IndexEndpoint and Index to determine distance metrics and feature normalization',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        [\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false }\n        ]\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'index', label: 'Index resource' },\n          { name: 'distance_metric' },\n          { name: 'feature_norm_type' }\n        ]\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input|\n        safe = call('deep_copy', input)\n        disc = call('discover_index_config', connection, safe)\n        call('enrich_response',\n          response: disc,\n          metadata: { 'operation' => 'vector.discover_config', 'model' => 'n/a' }\n        )\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n          \"metadata\"=>{ \"operation\"=>\"vector.discover_config\", \"model\"=>\"n/a\" },\n          \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>12, \"attempt\"=>1 },\n          \"index\"=>\"projects/.../locations/us-central1/indexes/123\",\n          \"distance_metric\"=>\"COSINE_DISTANCE\",\n          \"feature_norm_type\"=>\"UNIT_L2_NORM\"\n        }\n      end\n    },\n    # Text\n    classify_text: {\n      title: 'AI - Classify Text',\n      description: 'Classify text into one of the provided categories',\n\n      # CONFIG\n      config_fields: [\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        { name: 'model',\n          label: 'Model',\n          group: 'Model & tuning',\n          control_type: 'select',\n          sticky: true,\n          optional: true,\n          extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"',\n          pick_list: 'models_dynamic_for_behavior',\n          toggle_hint: 'Select from list',\n          toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }},\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning', ngIf: 'input.model_mode == \"explicit\"' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.classify', cfg['advanced_config'], cfg)\n      end,\n\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.classify')\n      end,\n\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input) # never mutate Workato’s input\n        call('execute_behavior', connection, 'text.classify', safe, user_cfg)\n      end,\n\n      # SAMPLE \n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"text.classify\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"category\"  => \"Support\",\n          \"confidence\"=> 0.98\n        }\n      end\n    },\n    generate_text: {\n      title: 'AI - Generate Text',\n      description: 'Gemini text generation',\n\n      # CONFIG\n      config_fields: [\n        { name: 'prompt_mode', label: 'Prompt mode', control_type: 'select', default: 'simple', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Simple (text prompt)',       'simple'],\n            ['Structured (contents array)', 'contents'],\n            ['Raw JSON payload',           'raw_json']\n          ],\n          hint: 'Structured modes let you pass a pre-built Vertex request (useful with RAG).' },\n\n        # --- Standardized model selector (unified with universal op) ---\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)',  'explicit'],\n            ['Use connection default',         'connection']\n          ],\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', sticky: true, optional: true, extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"', pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: 'text.generate' },\n          toggle_hint: 'Select from list', toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string',\n            control_type: 'text', optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning',\n          ngIf: 'input.model_mode == \"explicit\"', hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPU"}
{"id":"n_9aa5d2325aa633a9","kind":"action","name":"batch_operation","fqname":"connector.Vertex AI/actions.actions/action.batch_operation","loc":{"line":254,"column":21,"length":4210,"begin":10651,"end":14861},"file":null,"keys":["config_fields","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"on: {\n      title: 'UNIVERSAL - Batch AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'batchable_behaviors', optional: false },\n        { name: 'batch_strategy', label: 'Batch Strategy', control_type: 'select', default: 'count', options: [['By Count', 'count'], ['By Token Limit', 'tokens']] },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens ≈ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n      end,\n      # SAMPLE\n      sample_output: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n      end"}
{"id":"n_4efbd4d4a10f1807","kind":"action","name":"vertex_operation","fqname":"connector.Vertex AI/actions.actions/action.vertex_operation","loc":{"line":356,"column":22,"length":5327,"begin":14908,"end":20235},"file":null,"keys":["config_fields","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"tion: {\n      title: 'UNIVERSAL - Vertex AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'available_behaviors', optional: false, extends_schema: true,\n          hint: 'Select the AI operation to perform' },\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select',\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          default: 'auto', optional: false, sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.'\n        },\n        { name: 'model',\n          label: 'Model',\n          group: 'Model & tuning',\n          control_type: 'select',\n          sticky: true,\n          optional: true,\n          extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"',\n          pick_list: 'models_dynamic_for_behavior', \n          pick_list_params: { behavior: 'behavior' },  # bind by field by NAME\n          toggle_hint: 'Select from list',\n          toggle_field: {\n            name: 'model',\n            label: 'Model (custom id)',\n            type: 'string',\n            control_type: 'text',\n            optional: true,\n            toggle_hint: 'Provide custom value'\n          } },\n        { name: 'lock_model_revision',\n          label: 'Lock to latest numbered revision',\n          control_type: 'checkbox',\n          group: 'Model & tuning',\n          ngIf: 'input.model_mode == \"explicit\"',\n          hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        behavior = cfg['behavior']\n        behavior ? call('get_behavior_input_fields', behavior, cfg['advanced_config'], cfg) : []\n      end,\n      # OUTPUT\n      output_fields: lambda do |_object_definitions, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('telemetry_envelope_fields') + (call('get_behavior_output_fields', cfg['behavior']) || [])\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior     = config_fields['behavior']\n        user_config  = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input   = call('deep_copy', input) # do NOT mutate Workato’s input\n\n        # Leave advanced fields in safe_input; pipeline reads only what it needs\n        call('execute_behavior', connection, behavior, safe_input, user_config)\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, config_fields|\n        behavior = (config_fields.is_a?(Hash) ? config_fields : {})['behavior']\n        case behavior\n        when 'text.generate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.generate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hello world.\" }\n        when 'text.translate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.translate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hola mundo.\", \"detected_language\"=>\"en\" }\n        when 'text.summarize'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.summarize\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Concise summary.\", \"word_count\"=>2 }\n        when 'text.classify'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.classify\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"category\"=>\"Support\", \"confidence\"=>0.98 }\n        when 'text.embed'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.embed\", \"model\"=>\"text-embedding-004\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"embeddings\"=>[[0.01,0.02,0.03]] }\n        when 'multimodal.analyze'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"multimodal.analyze\", \"model\"=>\"gemini-1.5-pro\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"The image shows a tabby cat on a desk.\" }\n        else\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"unknown\", \"model\"=>\"gemini-1.5-flash\" } }\n        end\n      e"}
{"id":"n_ca0d710017151d8b","kind":"action","name":"discover_index_config","fqname":"connector.Vertex AI/actions.actions/action.discover_index_config","loc":{"line":462,"column":27,"length":1610,"begin":20404,"end":22014},"file":null,"keys":["description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"config: {\n      title: 'VECTOR SEARCH - Discover index configuration',\n      description: 'Reads IndexEndpoint and Index to determine distance metrics and feature normalization',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        [\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false }\n        ]\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'index', label: 'Index resource' },\n          { name: 'distance_metric' },\n          { name: 'feature_norm_type' }\n        ]\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input|\n        safe = call('deep_copy', input)\n        disc = call('discover_index_config', connection, safe)\n        call('enrich_response',\n          response: disc,\n          metadata: { 'operation' => 'vector.discover_config', 'model' => 'n/a' }\n        )\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n          \"metadata\"=>{ \"operation\"=>\"vector.discover_config\", \"model\"=>\"n/a\" },\n          \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>12, \"attempt\"=>1 },\n          \"index\"=>\"projects/.../locations/us-central1/indexes/123\",\n          \"distance_metric\"=>\"COSINE_DISTANCE\",\n          \"feature_norm_type\"=>\"UNIT_L2_NORM\"\n        }\n      e"}
{"id":"n_7e4687040a3946be","kind":"action","name":"classify_text","fqname":"connector.Vertex AI/actions.actions/action.classify_text","loc":{"line":502,"column":19,"length":2748,"begin":22046,"end":24794},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"y_text: {\n      title: 'AI - Classify Text',\n      description: 'Classify text into one of the provided categories',\n\n      # CONFIG\n      config_fields: [\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        { name: 'model',\n          label: 'Model',\n          group: 'Model & tuning',\n          control_type: 'select',\n          sticky: true,\n          optional: true,\n          extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"',\n          pick_list: 'models_dynamic_for_behavior',\n          toggle_hint: 'Select from list',\n          toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }},\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning', ngIf: 'input.model_mode == \"explicit\"' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.classify', cfg['advanced_config'], cfg)\n      end,\n\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.classify')\n      end,\n\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input) # never mutate Workato’s input\n        call('execute_behavior', connection, 'text.classify', safe, user_cfg)\n      end,\n\n      # SAMPLE \n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"text.classify\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"category\"  => \"Support\",\n          \"confidence\"=> 0.98\n        }\n     "}
{"id":"n_45e8d3b9be86d48a","kind":"action","name":"generate_text","fqname":"connector.Vertex AI/actions.actions/action.generate_text","loc":{"line":560,"column":19,"length":6545,"begin":24815,"end":31360},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"ate_text: {\n      title: 'AI - Generate Text',\n      description: 'Gemini text generation',\n\n      # CONFIG\n      config_fields: [\n        { name: 'prompt_mode', label: 'Prompt mode', control_type: 'select', default: 'simple', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Simple (text prompt)',       'simple'],\n            ['Structured (contents array)', 'contents'],\n            ['Raw JSON payload',           'raw_json']\n          ],\n          hint: 'Structured modes let you pass a pre-built Vertex request (useful with RAG).' },\n\n        # --- Standardized model selector (unified with universal op) ---\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)',  'explicit'],\n            ['Use connection default',         'connection']\n          ],\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', sticky: true, optional: true, extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"', pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: 'text.generate' },\n          toggle_hint: 'Select from list', toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string',\n            control_type: 'text', optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning',\n          ngIf: 'input.model_mode == \"explicit\"', hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg       = config_fields.is_a?(Hash) ? config_fields : {}\n        mode      = (cfg['prompt_mode'] || 'simple').to_s\n        show_adv  = !!cfg['advanced_config']\n\n        case mode\n        when 'contents'\n          fields = [\n            # Prompt structure\n            { name: 'contents', label: 'Contents', type: 'array', of: 'object', group: 'Prompt structure', optional: false,\n              properties: [\n                { name: 'role', label: 'Role', control_type: 'select',\n                  options: [['User','user'], ['Model','model']], optional: true },\n                { name: 'parts', label: 'Parts', type: 'array', of: 'object', properties: [\n                  { name: 'text',        label: 'Text' },\n                  { name: 'inline_data', label: 'Inline data', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'data',      label: 'Base64 data', control_type: 'text-area' }\n                  ]},\n                  { name: 'file_data',   label: 'File data (URI)', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'file_uri',  label: 'File URI' }\n                  ]}\n                ]}\n              ]\n            }\n          ]\n          if show_adv\n            fields += [\n              { name: 'system', label: 'System instruction', control_type: 'text-area', group: 'Advanced' },\n              { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n                properties: [\n                  { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                  { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                  { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n                ]\n              },\n              { name: 'response_mime_type', label: 'Response MIME type', group: 'Advanced',\n                hint: 'e.g., application/json for JSON mode' },\n              { name: 'response_schema', label: 'Response schema (object)', type: 'object', group: 'Advanced',\n                hint: 'When set, a compatible response_mime_type is required' },\n              { name: 'temperature', label: 'Temperature', type: 'number', group: 'Advanced', hint: '0.0 to 1.0' },\n              { name: 'max_tokens',  label: 'Max Tokens',  type: 'integer', group: 'Advanced' },\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        when 'raw_json'\n          fields = [\n            { name: 'payload_json', label: 'Full request JSON', control_type: 'text-area',\n              optional: false, group: 'Prompt (raw JSON)',\n              hint: 'Paste the entire models.generateContent request body including contents[].' }\n          ]\n          if show_adv\n            fields += [\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        else # 'simple'\n          call('get_behavior_input_fields', 'text.generate', show_adv, cfg)\n        end\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.generate')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg   = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input = call('deep_copy', input)\n        # Make prompt mode visible to the pipeline selector without mutating recipe input\n        safe_input['prompt_mode'] = config_fields['prompt_mode'] || 'simple'\n        call('execute_behavior', connection, 'text.generate', safe_input, user_cfg)\n      end,\n      # SAMPLE OUT\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\" => true, \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\" => { \"operation\" => \"text.generate\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\" => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"result\" => \"Hello world.\"\n        }\n     "}
{"id":"n_36bde34f7b478d55","kind":"action","name":"find_neighbors","fqname":"connector.Vertex AI/actions.actions/action.find_neighbors","loc":{"line":680,"column":20,"length":670,"begin":31402,"end":32072},"file":null,"keys":["description","execute","input_fields","output_fields","title"],"http":{"verbs":[],"endpoints":[]},"text":"eighbors: {\n      title: 'VECTOR SEARCH - Find nearest neighbors',\n      description: 'Query a deployed Vector Search index',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.find_neighbors', true)\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.find_neighbors')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        call('execute_behavior', connection, 'vector.find_neighbors', call('deep_copy', input))\n     "}
{"id":"n_f806a6dad72db3a0","kind":"action","name":"read_index_datapoints","fqname":"connector.Vertex AI/actions.actions/action.read_index_datapoints","loc":{"line":696,"column":27,"length":1883,"begin":32101,"end":33984},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"tapoints: {\n      title: 'VECTOR SEARCH - Read datapoints (vectors) by ID',\n      description: 'Fetch stored vectors and metadata for specific datapoint IDs from a deployed index',\n      # CONFIG\n      config_fields: [\n        { name: 'id_source', label: 'ID source', control_type: 'select', optional: false,\n          options: [\n            ['Auto (accept any)', 'auto'],\n            ['Manual IDs',        'manual'],\n            ['Neighbors array',   'neighbors'],\n            ['k‑NN groups',       'groups']\n          ],\n          default: 'auto', sticky: true, extends_schema: true,\n          hint: 'Controls which input fields are shown for datapoint IDs.'\n        }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, cfg|\n        call('get_behavior_input_fields', 'vector.read_datapoints', true, cfg)\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.read_datapoints')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, cfg|\n        safe = call('deep_copy', input)\n        # Pass the chosen mode to the behavior without mutating the original input\n        safe['id_source'] = cfg['id_source'] if cfg['id_source']\n        call('execute_behavior', connection, 'vector.read_datapoints', safe)\n      end,\n      # SAMPLE OUT\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"vector.read_datapoints\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 12, \"attempt\" => 1 },\n          \"datapoints\"=> [\n            { \"datapoint_id\" => \"dp_000001\", \"feature_vector\" => [0.01, 0.02, 0.03] }\n          ]\n        }\n   "}
{"id":"n_517e11efa6e55793","kind":"action","name":"upsert_index_datapoints","fqname":"connector.Vertex AI/actions.actions/action.upsert_index_datapoints","loc":{"line":740,"column":29,"length":693,"begin":34015,"end":34708},"file":null,"keys":["description","execute","input_fields","output_fields","title"],"http":{"verbs":[],"endpoints":[]},"text":"datapoints: {\n      title: 'VECTOR SEARCH - Upsert index datapoints',\n      description: 'Add or update datapoints in a Vector Search index',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.upsert_datapoints', true)\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.upsert_datapoints')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        call('execute_behavior', connection, 'vector.upsert_datapoints', call('deep_copy', input))\n   "}
{"id":"n_1a22dc4f9c934cc8","kind":"action","name":"generate_embeddings","fqname":"connector.Vertex AI/actions.actions/action.generate_embeddings","loc":{"line":757,"column":25,"length":2505,"begin":34752,"end":37257},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","title"],"http":{"verbs":[],"endpoints":[]},"text":"embeddings: {\n      title: 'VECTOR SEARCH - Generate embeddings',\n      description: 'Create dense embeddings for text',\n      # CONFIG\n      config_fields: [\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.'\n        },\n        { name: 'model',\n          label: 'Model',\n          group: 'Model & tuning',\n          control_type: 'select',\n          sticky: true,\n          optional: true,\n          extends_schema: true,\n          ngIf: 'input.model_mode == \"explicit\"',\n          pick_list: 'models_dynamic_for_behavior',\n          toggle_hint: 'Select from list',\n          toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning', ngIf: 'input.model_mode == \"explicit\"' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.embed', cfg['advanced_config'], cfg)\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'embeddings', type: 'array', of: 'array' },\n          { name: 'vectors', type: 'array', of: 'object', properties: [\n            { name: 'feature_vector', type: 'array', of: 'number' }\n          ]},\n          { name: 'count', type: 'integer' }\n        ]\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input)\n        call('execute_behavior', connection, 'text.embed', safe, user_cfg)\n   "}
{"id":"n_309b594bada0bd28","kind":"triggers","name":"triggers","fqname":"connector.Vertex AI/triggers.triggers","loc":{"line":3314,"column":12,"length":2,"begin":139534,"end":139536},"file":null,"keys":[],"http":{},"text":"=="}
{"id":"n_0658cf6cb3dd0ad4","kind":"pick_lists","name":"pick_lists","fqname":"connector.Vertex AI/pick_lists.pick_lists","loc":{"line":3114,"column":14,"length":5295,"begin":132932,"end":138227},"file":null,"keys":[],"http":{},"text":"=========================\n  pick_lists: {\n\n    all_models: lambda do |connection|\n      [\n        ['Gemini 1.5 Flash', 'gemini-1.5-flash'],\n        ['Gemini 1.5 Pro', 'gemini-1.5-pro'],\n        ['Gemini Embedding 001',  'gemini-embedding-001'],\n        ['Text Embedding 005',    'text-embedding-005'],\n        ['Text Embedding 004',    'text-embedding-004'],\n        ['Text Embedding Gecko', 'textembedding-gecko']\n      ]\n    end,\n    \n    available_behaviors: lambda do |connection|\n      behaviors = call('behavior_registry')\n      behaviors.map do |key, config|\n        [config[:description], key]\n      end.sort_by { |label, _| label }\n    end,\n    \n    batchable_behaviors: lambda do |connection|\n      behaviors = call('behavior_registry')\n      behaviors.select { |_, config| \n        config[:features]&.include?('batching') \n      }.map { |key, config|\n        [config[:description], key]\n      }\n    end,\n    \n    embedding_tasks: lambda do |connection|\n      [\n        ['Document Retrieval', 'RETRIEVAL_DOCUMENT'],\n        ['Query Retrieval', 'RETRIEVAL_QUERY'],\n        ['Semantic Similarity', 'SEMANTIC_SIMILARITY'],\n        ['Classification', 'CLASSIFICATION'],\n        ['Clustering', 'CLUSTERING']\n      ]\n    end,\n\n    gcp_regions: lambda do |connection|\n      [\n        ['US Central 1', 'us-central1'],\n        ['US East 1', 'us-east1'],\n        ['US East 4', 'us-east4'],\n        ['US West 1', 'us-west1'],\n        ['US West 4', 'us-west4']\n      ]\n    end,\n\n    languages: lambda do |connection|\n      [\n        ['Auto-detect', 'auto'],\n        ['English', 'en'],\n        ['Spanish', 'es'],\n        ['French', 'fr'],\n        ['German', 'de'],\n        ['Italian', 'it'],\n        ['Portuguese', 'pt'],\n        ['Japanese', 'ja'],\n        ['Korean', 'ko'],\n        ['Chinese (Simplified)', 'zh-CN'],\n        ['Chinese (Traditional)', 'zh-TW']\n      ]\n    end,\n\n    models_for_behavior: lambda do |connection, input = {}|\n      behavior = input['behavior']\n      defn = call('behavior_registry')[behavior]\n\n      if defn && defn[:supported_models]\n        defn[:supported_models].map do |model|\n          [model.split('-').map!(&:capitalize).join(' '), model]\n        end\n      else\n        []\n      end\n    end,\n\n    models_dynamic_for_behavior: lambda do |connection, behavior: nil, **_|\n      prefixes = if behavior.to_s == 'text.embed'\n        ['text-embedding-', 'textembedding-', 'gemini-embedding-']\n      else\n        ['gemini-'] # last\n      end\n      items = []\n      begin\n        items = call('list_publisher_models', connection)\n          .map { |m| id = (m['name'] || '').split('/').last; [m['displayName'] || id, id] }\n          .select { |_label, id| prefixes.any? { |p| id.start_with?(p) } }\n          .sort_by { |_label, id| - (id[/(\\d+)$/, 1].to_i) } # still works for hyphen & @ suffixes\n      rescue\n        items = [] # fall through to fallback\n      end\n\n      if items.empty?\n        # Minimal, safe fallback to keep the UI usable before a connection is fully ready.\n        items = if prefixes.first == 'gemini-'\n          [['Gemini 2.5 Flash', 'gemini-2.5-flash'], ['Gemini 2.5 Pro', 'gemini-2.5-pro']]\n        else\n          [\n            ['Gemini Embedding 001',   'gemini-embedding-001'],\n            ['Text Embedding 005',     'text-embedding-005'],\n            ['Text Embedding 004',     'text-embedding-004'],\n            ['Text Embedding Gecko',   'textembedding-gecko']\n          ]\n        end\n      end\n      items\n    end,\n\n    # DEPRECATED: remove after testing/development\n    models_generation_dynamic: lambda do |connection|\n      call('list_publisher_models', connection)\n        .map { |m| id = (m['name'] || '').split('/').last; [m['displayName'] || id, id] }\n        .select { |_label, id| id.start_with?('gemini-') }\n        .sort_by { |_label, id| - (id[/-(\\d+)$/, 1].to_i) }\n    end,\n\n    safety_categories: lambda do |_connection|\n      [\n        ['Harassment',           'HARM_CATEGORY_HARASSMENT'],\n        ['Hate speech',          'HARM_CATEGORY_HATE_SPEECH'],\n        ['Sexually explicit',    'HARM_CATEGORY_SEXUALLY_EXPLICIT'],\n        ['Dangerous content',    'HARM_CATEGORY_DANGEROUS_CONTENT']\n      ]\n    end,\n\n    safety_levels: lambda do |_connection|\n      [\n        ['Block none',   'BLOCK_NONE'],\n        ['Block low',    'BLOCK_LOW'],\n        ['Block medium', 'BLOCK_MEDIUM'],\n        ['Block high',   'BLOCK_HIGH']\n      ]\n    end,\n\n    safety_thresholds: lambda do |_connection|\n      [\n        ['Block none',              'BLOCK_NONE'],\n        ['Block only high',         'BLOCK_ONLY_HIGH'],\n        ['Block medium and above',  'BLOCK_MEDIUM_AND_ABOVE'],\n        ['Block low and above',     'BLOCK_LOW_AND_ABOVE']\n      ]\n    end,\n\n    safety_methods: lambda do |_connection|\n      [\n        ['By severity',    'SEVERITY'],\n        ['By probability', 'PROBABILITY']\n      ]\n    end,\n  \n    vector_distance_metrics: lambda do |_|\n      [\n        ['Cosine distance (1 - cos_sim)', 'COSINE_DISTANCE'],\n        ['Dot-product distance (−dot)',   'DOT_PRODUCT_DISTANCE'],\n        ['Squared L2 (Euclidean^2)',      'SQUARED_L2_DISTANCE'],\n        ['L1 (Manhattan)',                'L1_DISTANCE']\n      ]\n    end,\n\n    vector_feature_norm_types: lambda do |_|\n      [\n        ['Unit L2 norm', 'UNIT_L2_NORM'],\n        ['"}
{"id":"lam:766e3f5c0b5a9bad","kind":"lambda","name":"test","fqname":"connector#test","loc":{"line":224,"column":8,"length":678,"begin":9637,"end":10315},"file":"connector.rb","text":": lambda do |connection|\n    project = connection['project']\n    region  = connection['region']\n\n    # 1) Token + API enablement (global catalog)\n    call('list_publisher_models', connection) # raises normalized errors\n\n    # 2) Regional reachability / permissions\n    url = call('build_endpoint_url', connection,\n      { 'custom_path' => \"https://{region}-aiplatform.googleapis.com/#{connection['version']}/projects/{project}/locations/{region}/endpoints\" },\n      {}\n    )\n    call('http_request', connection, method: 'GET', url: url, headers: call('build_headers', connection))\n\n    true\n  rescue => e\n    # Keep the normalized, compact message as‑is\n    error(e.message)\n "}
{"id":"lam:6a455401c561495d","kind":"lambda","name":"method","fqname":"method:build_payload","loc":{"line":817,"column":19,"length":7330,"begin":37600,"end":44930},"file":"connector.rb","text":"ld_payload: lambda do |template:, variables:, format:|\n      case format\n      \n      # Direct\n      when 'direct'\n        variables\n      \n      # Template\n      when 'template'\n        result = template.dup\n        variables.each { |k, v| result = result.gsub(\"{#{k}}\", v.to_s) }\n        result\n      \n      # Vertex prompt\n      when 'vertex_prompt'\n        payload = {\n          'contents' => [{\n            'role'  => 'user',\n            'parts' => [{ 'text' => call('apply_template', template, variables) }]\n          }],\n          'generationConfig' => call('build_generation_config', variables)\n        }.compact\n\n        sys = variables['system']\n        payload['systemInstruction'] = { 'parts' => [{ 'text' => sys }] } if sys && !sys.to_s.strip.empty?\n\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        # JSON mode (optional)\n        if variables['response_mime_type'] || variables['response_schema']\n          gc = (payload['generationConfig'] ||= {})\n          gc['responseMimeType'] = variables['response_mime_type'] if variables['response_mime_type']\n          gc['responseSchema']   = variables['response_schema']     if variables['response_schema']\n        end\n\n        payload['labels'] = variables['labels'] if variables['labels']\n        payload\n\n      when 'vertex_contents'\n        contents = Array(variables['contents']).map do |c|\n          role = c['role'] || c[:role] || 'user'\n          parts = Array(c['parts']).map do |p|\n            if p['text'] || p[:text]\n              { 'text' => p['text'] || p[:text] }\n            elsif p['inline_data'] || p[:inline_data] || p['inlineData']\n              src = p['inline_data'] || p[:inline_data] || p['inlineData']\n              { 'inlineData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'data'     => src['data'] || src[:data]\n                }.compact\n              }\n            elsif p['file_data'] || p[:file_data] || p['fileData']\n              src = p['file_data'] || p[:file_data] || p['fileData']\n              { 'fileData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'fileUri'  => src['file_uri']  || src[:file_uri]  || src['fileUri']\n                }.compact\n              }\n            else\n              {} # ignored\n            end\n          end.compact\n\n          { 'role' => role, 'parts' => parts }\n        end\n\n        payload = {\n          'contents'         => contents,\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        if variables['system']\n          payload['systemInstruction'] = { 'parts' => [{ 'text' => variables['system'] }] }\n        end\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        # JSON mode (optional)\n        gc = (payload['generationConfig'] ||= {})\n        gc['responseMimeType'] = variables['response_mime_type'] if variables['response_mime_type']\n        gc['responseSchema']   = variables['response_schema']     if variables['response_schema']\n\n        payload['labels'] = variables['labels'] if variables['labels']\n        payload\n      when 'vertex_passthrough'\n        src = variables['payload'] || variables['payload_json'] || variables['fully_formed'] || variables['request_json']\n        obj =\n          if src.is_a?(String)\n            begin\n              JSON.parse(src)\n            rescue\n              corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n              error(\"Invalid payload_json (must be valid JSON object). [corr_id=#{corr}]\")\n            end\n          else\n            src\n          end\n        error('payload_json must be a JSON object') unless obj.is_a?(Hash)\n        obj\n\n      # Embedding\n      when 'embedding'\n        body = {\n          'instances' => variables['texts'].map { |text|\n            {\n              'content'   => text,\n              'task_type' => variables['task_type'] || 'RETRIEVAL_DOCUMENT',\n              'title'     => variables['title']\n            }.compact\n          }\n        }\n        params = {}\n        params['autoTruncate']          = variables['auto_truncate'] unless variables['auto_truncate'].nil?\n        params['outputDimensionality']  = variables['output_dimensionality'] if variables['output_dimensionality']\n        body['parameters'] = params unless params.empty? \n        body\n      # Vector search\n      when 'find_neighbors'\n        queries = Array(variables['queries']).map do |q|\n          dp =\n            if q['feature_vector']\n              { 'feature_vector' => Array(q['feature_vector']).map(&:to_f) }\n            elsif q['vector'] # alias\n              { 'feature_vector' => Array(q['vector']).map(&:to_f) }\n            elsif q['datapoint_id']\n              { 'datapoint_id' => q['datapoint_id'] }\n            else\n              {}\n            end\n          {\n            'datapoint'        => dp,\n            'neighbor_count'   => (q['neighbor_count'] || variables['neighbor_count'] || 10).to_i,\n            'restricts'        => q['restricts'],\n            'numeric_restricts'=> q['numeric_restricts']\n          }.compact\n        end\n\n        {\n          'deployed_index_id'     => variables['deployed_index_id'],\n          'queries'               => queries,\n          'return_full_datapoint' => variables['return_full_datapoint']\n        }.compact\n\n      when 'upsert_datapoints'\n        datapoints =\n          if Array(variables['datapoints']).any?\n            Array(variables['datapoints']).map do |d|\n              {\n                'datapointId'      => d['datapoint_id'] || d['id'],\n                'featureVector'    => Array(d['feature_vector'] || d['vector']).map(&:to_f),\n                'sparseEmbedding'  => d['sparse_embedding'],\n                'restricts'        => d['restricts'],\n                'numericRestricts' => d['numeric_restricts'],\n                'crowdingTag'      => d['crowding_tag'],\n                'embeddingMetadata'=> d['embedding_metadata']\n              }.compact\n            end\n          elsif Array(variables['embeddings']).any?\n            call('coerce_embeddings_to_datapoints', variables)\n          else\n            []\n          end\n\n        { 'datapoints' => datapoints }\n      when 'read_index_datapoints'\n        ids = call('extract_ids_for_read', variables)\n        { 'deployed_index_id' => variables['deployed_index_id'], 'ids' => ids }\n      # Multimodal\n      when 'multimodal'\n        parts = []\n        parts << { 'text' => variables['text'] } if variables['text']\n        if variables['images']\n          variables['images'].each do |img|\n            parts << { 'inlineData' => { 'mimeType' => img['mime_type'] || 'image/jpeg', 'data' => img['data'] } }\n          end\n        end\n\n        payload = {\n          'contents' => [{ 'role' => 'user', 'parts' => parts }],\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        if variables['safety_settings']\n          payload['safetySettings'] = call('normalize_safety_settings', variables['safety_settings'])\n        end\n\n        payload\n        \n      else\n        variables\n     "}
{"id":"lam:b2d520b8102af6a3","kind":"lambda","name":"method","fqname":"method:enrich_response","loc":{"line":1014,"column":21,"length":1076,"begin":44984,"end":46060},"file":"connector.rb","text":"h_response: lambda do |response:, metadata: {}|\n      base  = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'result' => response }\n      trace = base.delete('_trace')\n      _http = base.delete('_http') # kept internal; not exposed\n\n      # Preserve success if caller provided it; otherwise assume true\n      success = base.key?('success') ? base['success'] : true\n\n      # Build a uniform trace object (always present)\n      trace_hash  = trace.is_a?(Hash) ? trace : {}\n      final_trace = {\n        'correlation_id' => trace_hash['correlation_id'] || SecureRandom.hex(8),\n        'duration_ms'    => (trace_hash['duration_ms'] || 0).to_i,\n        'attempt'        => (trace_hash['attempt'] || 1).to_i\n      }\n      final_trace['rate_limit'] = trace_hash['rate_limit'] if trace_hash['rate_limit']\n\n      base.merge(\n        'success'   => success,\n        'timestamp' => base['timestamp'] || Time.now.utc.iso8601,\n        'metadata'  => { 'operation' => metadata['operation'], 'model' => metadata['model'] }.compact,\n        'trace'     => final_trace\n      ).com"}
{"id":"lam:b1fad3d98ecfde72","kind":"lambda","name":"method","fqname":"method:extract_response","loc":{"line":1040,"column":22,"length":1179,"begin":46111,"end":47290},"file":"connector.rb","text":"t_response: lambda do |data:, path: nil, format: 'raw'|\n      case format\n      # Raw data\n      when 'raw' then data\n      # Json field\n      when 'json_field'\n        return data unless path\n        path.split('.').reduce(data) { |acc, seg| acc.is_a?(Array) && seg =~ /^\\d+$/ ? acc[seg.to_i] : (acc || {})[seg] }\n      # Vertex text\n      when 'vertex_text'\n        parts = data.dig('candidates', 0, 'content', 'parts') || []\n        text  = parts.select { |p| p['text'] }.map { |p| p['text'] }.join\n        text.empty? ? data.dig('predictions', 0, 'content').to_s : text\n      # Vertex-flavored json\n      when 'vertex_json'\n        raw = (data.dig('candidates', 0, 'content', 'parts') || []).map { |p| p['text'] }.compact.join\n        return {} if raw.nil? || raw.empty?\n        m = raw.match(/```(?:json)?\\s*(\\{.*?\\})\\s*```/m) || raw.match(/\\{.*\\}/m)\n        m ? (JSON.parse(m[1] || m[0]) rescue {}) : {}\n      # Embeddings\n      when 'embeddings'\n        # text-embedding APIs return embeddings under predictions[].embeddings.values\n        arr = (data['predictions'] || []).map { |p| p.dig('embeddings', 'values') || p['values'] }.compact\n        arr\n      else data\n     "}
{"id":"lam:77252378ba03583c","kind":"lambda","name":"method","fqname":"method:http_request","loc":{"line":1069,"column":18,"length":3959,"begin":47340,"end":51299},"file":"connector.rb","text":"tp_request: lambda do |connection, method:, url:, payload: nil, headers: {}, retry_config: {}, request_format: 'json'|\n      max_attempts = (retry_config['max_attempts'] || retry_config['max_retries'] || 3).to_i\n      base_backoff = (retry_config['backoff'] || 1.0).to_f\n      retry_on     = Array(retry_config['retry_on'] || [408, 429, 500, 502, 503, 504]).map(&:to_i)\n      do_not_retry = Array(retry_config['do_not_retry']).map(&:to_i)\n\n      attempt = 0\n      last_error = nil\n\n      while attempt < max_attempts\n        attempt += 1\n        begin\n          hdrs = (headers || {}).dup\n          corr = hdrs['X-Correlation-Id'] ||= \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n          last_error = nil\n\n          req = case method.to_s.upcase\n                when 'GET'    then get(url)\n                when 'POST'   then post(url, payload)\n                when 'PUT'    then put(url, payload)\n                when 'DELETE' then delete(url)\n                else error(\"Unsupported HTTP method: #{method}\")\n                end\n\n          # Respect application/x-www-form-urlencoded when requested\n          if request_format.to_s == 'form'\n            hdrs['Content-Type'] ||= 'application/x-www-form-urlencoded'\n            req = req.request_format_www_form_urlencoded\n          end\n\n          response =\n            req.headers(hdrs)\n              .after_error_response(/.*/) { |code, body, rheaders, message|\n                 dur_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n                 err = call('normalize_http_error',\n                            connection,\n                            code: code, body: body, headers: (rheaders || {}),\n                            message: message, url: url, corr_id: corr, attempt: attempt, duration_ms: dur_ms)\n                 last_error = err\n                 error(call('format_user_error', err))\n               }\n              .after_response { |code, body, rheaders|\n                # Always return a Hash payload with HTTP metadata, even when the API returns a raw string/bytes.\n                payload = body.is_a?(Hash) ? body : { 'raw' => body }\n                payload['_http'] = { 'status' => code, 'headers' => rheaders }\n                payload\n              }\n\n          duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n          out = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'raw' => response }\n\n          # Extract a useful Google/Vertex request id when present.\n          hdrs_out = (out.dig('_http', 'headers') || {})\n          rid = hdrs_out['x-request-id'] || hdrs_out['x-cloud-trace-context'] || hdrs_out['x-guploader-uploadid']\n\n          out['_trace'] = {\n            'correlation_id'    => corr,\n            'duration_ms'       => duration_ms,\n            'attempt'           => attempt,\n            'http_status'       => out.dig('_http','status'),\n            'remote_request_id' => rid\n          }.compact\n\n          return out\n\n        rescue => _e\n          code = last_error ? last_error['code'].to_i : 0\n          retryable = last_error ? last_error['retryable'] : retry_on.include?(code)\n          retryable &&= !do_not_retry.include?(code)\n\n          # Break if not retryable or out of attempts\n          break unless retryable && attempt < max_attempts\n\n          # Retry-After (seconds) takes precedence when present\n          delay =\n            if last_error && last_error['retry_after_s'].to_i > 0\n              last_error['retry_after_s'].to_i\n            else\n              # exp backoff with small jitter\n              (base_backoff * (2 ** (attempt - 1))).to_f + rand * 0.25\n            end\n\n          sleep(delay)\n        end\n      end\n\n      # Exhausted: bubble the last normalized message if present\n      msg = last_error ? call('format_user_error', last_error) : 'HTTP request failed'\n      error("}
{"id":"lam:2a607aa57765874b","kind":"lambda","name":"method","fqname":"method:transform_data","loc":{"line":1162,"column":20,"length":778,"begin":51348,"end":52126},"file":"connector.rb","text":"sform_data: lambda do |input:, from_format:, to_format:, connection: nil|\n      case \"#{from_format}_to_#{to_format}\"\n      when 'url_to_base64'\n        # Use centralized http_request for retries and telemetry\n        resp = call('http_request', connection, method: 'GET', url: input, headers: {})\n        raw  = resp['raw'] || resp.to_s\n        raw.to_s.encode_base64\n      when 'base64_to_bytes'\n        input.decode_base64\n      when 'language_code_to_name'\n        languages = { 'en' => 'English', 'es' => 'Spanish', 'fr' => 'French' }\n        languages[input] || input\n      when 'categories_to_text'\n        input.map { |c| \"#{c['name']}: #{c['description']}\" }.join(\"\\n\")\n      when 'distance_to_similarity'\n        1.0 - (input.to_f / 2.0)\n      else\n        input\n     "}
{"id":"lam:a0c3599b70b56e35","kind":"lambda","name":"method","fqname":"method:validate_input","loc":{"line":1184,"column":20,"length":4248,"begin":52176,"end":56424},"file":"connector.rb","text":"date_input: lambda do |data:, schema: [], constraints: []|\n      errors = []\n      \n      # Schema validation\n      schema.each do |field|\n        field_name = field['name']\n        field_value = data[field_name]\n        \n        # Required check\n        if field['required'] && (field_value.nil? || field_value.to_s.empty?)\n          errors << \"#{field_name} is required\"\n        end\n        \n        # Length validation\n        if field['max_length'] && field_value.to_s.length > field['max_length']\n          errors << \"#{field_name} exceeds maximum length of #{field['max_length']}\"\n        end\n        \n        # Pattern validation\n        if field['pattern'] && field_value && !field_value.match?(Regexp.new(field['pattern']))\n          errors << \"#{field_name} format is invalid\"\n        end\n      end\n      \n      # Constraint validation\n      constraints.each do |constraint|\n        ctype = (constraint['type'] || constraint[:type]).to_s\n\n        case ctype\n        when 'min_value'\n          value = data[(constraint['field'] || constraint[:field]).to_s].to_f\n          if value < constraint['value'].to_f\n            errors << \"#{constraint['field'] || constraint[:field]} must be at least #{constraint['value']}\"\n          end\n\n        when 'max_items'\n          field = (constraint['field'] || constraint[:field]).to_s\n          items = data[field] || []\n          if Array(items).size > constraint['value'].to_i\n            errors << \"#{field} cannot exceed #{constraint['value']} items\"\n          end\n\n        # XOR/ONE-OF across fields (root or per-item scope)\n        when 'xor', 'one_of'\n          scope   = (constraint['scope'] || constraint[:scope]).to_s # e.g., 'queries[]' or ''\n          fields  = Array(constraint['fields'] || constraint[:fields]).map(&:to_s)\n          aliases = (constraint['aliases'] || constraint[:aliases] || {}) # { 'feature_vector' => ['vector'] }\n          exactly_one = (ctype == 'xor') || (constraint['exactly_one'] == true)\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            count = 0\n            fields.each do |f|\n              keys = [f] + Array(aliases[f] || aliases[f.to_sym]).map(&:to_s)\n              present = keys.any? { |k| call('value_present', ctx[k]) }\n              count += 1 if present\n            end\n\n            if exactly_one\n              if count != 1\n                display = fields.map { |f|\n                  al = Array(aliases[f] || aliases[f.to_sym])\n                  al.any? ? \"#{f} (alias: #{al.join(', ')})\" : f\n                }.join(', ')\n                errors << \"#{label}: exactly one of #{display} must be provided\"\n              end\n            else\n              if count < 1\n                errors << \"#{label}: at least one of #{fields.join(', ')} must be provided\"\n              end\n            end\n          end\n\n        # Conditional required with root-level fallback and optional default\n        # Example: each queries[].neighbor_count is optional if top-level neighbor_count is present,\n        # or if a default is defined; else required.\n        when 'fallback_required', 'conditional_required'\n          scope    = (constraint['scope'] || constraint[:scope]).to_s       # e.g., 'queries[]'\n          field    = (constraint['field'] || constraint[:field]).to_s       # e.g., 'neighbor_count'\n          fallback = (constraint['fallback_to_root'] || constraint[:fallback_to_root]).to_s # e.g., 'neighbor_count'\n          default_ok = constraint.key?('default_if_absent') || constraint.key?(:default_if_absent)\n\n          root_has_fallback = fallback.empty? ? false : call('value_present', data[fallback])\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            item_has = call('value_present', ctx[field])\n            unless item_has || root_has_fallback || default_ok\n              if fallback.empty?\n                errors << \"#{label}.#{field} is required\"\n              else\n                errors << \"#{label}.#{field} is required when top-level #{fallback} is not provided\"\n              end\n            end\n          end\n\n        else\n          # unknown constraint type: ignore silently (forward-compatible)\n        end\n      end\n      \n      error(errors.join('; ')) if errors.any?\n      "}
{"id":"lam:b13ec893e3c8aca0","kind":"lambda","name":"method","fqname":"method:with_resilience","loc":{"line":1288,"column":21,"length":2187,"begin":56473,"end":58660},"file":"connector.rb","text":"resilience: lambda do |operation:, config: {}, task: {}, connection: nil, &blk|\n      # Rate limiting (per-job) — always initialize and use a unique name\n      rate_limit_info = nil\n      if config['rate_limit']\n        rate_limit_info = call('check_rate_limit', operation, config['rate_limit'])\n      end\n\n      circuit_key   = \"circuit_#{operation}\"\n      circuit_state = call('memo_get', circuit_key) || { 'failures' => 0 }\n      error(\"Circuit breaker open for #{operation}. Too many recent failures.\") if circuit_state['failures'] >= 5\n\n      begin\n        result =\n          if blk\n            # Instrument the block path so trace is still present\n            corr    = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n            started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n            raw     = blk.call\n            dur_ms  = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n            out     = raw.is_a?(Hash) ? JSON.parse(JSON.dump(raw)) : { 'result' => raw }\n            out['_trace'] ||= {}\n            out['_trace'].merge!({ 'correlation_id' => corr, 'duration_ms' => dur_ms, 'attempt' => 1 })\n            out\n          else\n            error('with_resilience requires a task hash with url/method') unless task.is_a?(Hash) && task['url']\n\n            call('http_request',\n              connection,\n              method:       (task['method'] || 'GET'),\n              url:          task['url'],\n              payload:      task['payload'],\n              headers:      (task['headers'] || {}),\n              retry_config: (task['retry_config'] || {})\n            )\n          end\n\n        # Attach rate-limit counters to trace if present (guarded)\n        if rate_limit_info && result.is_a?(Hash)\n          result['_trace'] ||= {}\n          result['_trace']['rate_limit'] = rate_limit_info\n        end\n\n        # Reset circuit on success\n        call('memo_put', circuit_key, { 'failures' => 0 }, 300)\n        result\n\n      rescue => e\n        circuit_state['failures'] += 1\n        call('memo_put', circuit_key, circuit_state, 300)\n        # Keep normalized messages intact; do not blanket-retry non-retryables here\n        raise e\n   "}
{"id":"lam:639d6c0f9d1cb8ea","kind":"lambda","name":"method","fqname":"method:execute_pipeline","loc":{"line":1346,"column":22,"length":2915,"begin":58826,"end":61741},"file":"connector.rb","text":"ute_pipeline: lambda do |connection, operation, input, config|\n      # Don't mutate the input\n      local = call('deep_copy', input)\n\n      # 1. Validate\n      if config['validate']\n        call('validate_input',\n          data:         local,\n          schema:       config['validate']['schema'] || [],\n          constraints:  config['validate']['constraints'] || []\n        )\n      end\n      \n      # 2. Transform input\n      if config['transform_input']\n        config['transform_input'].each do |field, transform|\n          if local[field]\n            local[field] = call('transform_data',\n              input:        local[field],\n              from_format:  transform['from'],\n              to_format:    transform['to'],\n              connection:   connection\n            )\n          end\n        end\n      end\n\n      # -- Ensure selected model from ops config is visible to URL builder\n      local['model'] ||= config['model']\n\n      # 3. Build payload\n      payload = if config['payload']\n        call('build_payload',\n          template:   config['payload']['template'] || '',\n          variables:  local.merge('system' => config['payload']['system']),\n          format:     config['payload']['format'] || 'direct'\n        )\n      else\n        local\n      end\n      \n      # 4. Build URL\n      endpoint  = config['endpoint'] || {}\n      url       = call('build_endpoint_url', connection, endpoint, local)\n      \n      # 5. Execute with resilience\n      response = call('with_resilience',\n        operation:  operation,\n        config:     (config['resilience'] || {}),\n        task: {\n          'method'       => endpoint['method'] || 'POST',\n          'url'          => url,\n          'payload'      => payload,\n          'headers'      => call('build_headers', connection),\n          'retry_config' => (config.dig('resilience', 'retry') || {})\n        },\n        connection: connection\n      )\n      \n      trace_from_response = (response.is_a?(Hash) ? response['_trace'] : nil)\n\n      # 6. Extract response\n      extracted = if config['extract']\n        call('extract_response',\n          data:   response,\n          path:   config['extract']['path'],\n          format: config['extract']['format'] || 'raw'\n        )\n      else\n        response\n      end\n\n      # Preserve trace even if extracted is a primitive\n      if trace_from_response\n        if extracted.is_a?(Hash)\n          extracted['_trace'] ||= {}\n          extracted['_trace'].merge!(trace_from_response)\n        else\n          extracted = { 'result' => extracted, '_trace' => trace_from_response }\n        end\n      end\n      \n      # 7. Post-process\n      if config['post_process']\n        extracted = call(config['post_process'], extracted, local)\n      end\n      \n      # 8. Enrich\n      call('enrich_response',\n        response: extracted,\n        metadata: { 'operation' => operation, 'model' => config['model'] || local['model'] }\n "}
{"id":"lam:d680ff9d511a154c","kind":"lambda","name":"method","fqname":"method:behavior_registry","loc":{"line":1445,"column":23,"length":9985,"begin":61968,"end":71953},"file":"connector.rb","text":"ior_registry: lambda do\n      {\n        # Text Operations\n        'text.generate' => {\n          description: 'Generate text from a prompt',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['streaming', 'caching'],\n          config_template: {\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => '{prompt}',\n              'system' => nil\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        'text.translate' => {\n          description: 'Translate text between languages',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true, 'max_length' => 10000 },\n                { 'name' => 'target_language', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'source_language' => { 'from' => 'language_code', 'to' => 'name' },\n              'target_language' => { 'from' => 'language_code', 'to' => 'name' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Translate the following text from {source_language} to {target_language}. Return only the translation:\\n\\n{text}',\n              'system' => 'You are a professional translator. Maintain tone and context.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          },\n          defaults: {\n            'temperature' => 0.3,\n            'max_tokens' => 2048\n          }\n        },\n        'text.summarize' => {\n          description: 'Summarize text content',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'max_words', 'required' => false }\n              ]\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Summarize the following text in {max_words} words:\\n\\n{text}',\n              'system' => 'You are an expert at creating clear, concise summaries.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            },\n            'post_process' => 'add_word_count'\n          },\n          defaults: {\n            'temperature' => 0.5,\n            'max_words' => 200\n          }\n        },\n        'text.classify' => {\n          description: 'Classify text into categories',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'categories', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'categories' => { 'from' => 'categories', 'to' => 'text' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Classify this text into one of these categories:\\n{categories}\\n\\nText: {text}\\n\\nRespond with JSON: {\"category\": \"name\", \"confidence\": 0.0-1.0}',\n              'system' => 'You are a classification expert. Always return valid JSON.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_json'\n            }\n          },\n          defaults: {\n            'temperature' => 0.1\n          }\n        },\n        # Embedding Operations\n        'text.embed' => {\n          description: 'Generate text embeddings',\n          capability: 'embedding',\n          supported_models: ['text-embedding-005', 'text-embedding-004', 'textembedding-gecko', 'gemini-embedding-001'],\n          features: ['batching', 'caching'],\n          config_template: {\n            'validate' => {\n              'schema' => [ { 'name' => 'texts', 'required' => true } ],\n              'constraints' => [ { 'type' => 'max_items', 'field' => 'texts', 'value' => 100 } ]\n            },\n            'payload' => { 'format' => 'embedding' },\n            'endpoint' => { 'path' => ':predict', 'method' => 'POST' },\n            'extract' => { 'format' => 'embeddings' },\n            'post_process' => 'wrap_embeddings_vectors'\n          }\n        },\n        # Multimodal Operations\n        'multimodal.analyze' => {\n          description: 'Analyze images with text prompts',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-pro', 'gemini-1.5-flash'],\n          features: ['streaming'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'prompt', 'required' => true },\n                { 'name' => 'images', 'required' => true }\n              ]\n            },\n            'payload' => {\n              'format' => 'multimodal'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        # Vector Operations\n        'vector.upsert_datapoints' => {\n          description: 'Upsert datapoints into a Vector Search index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index', 'required' => true }\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['datapoints', 'embeddings'] }\n              ]\n            },\n            'payload'  => { 'format' => 'upsert_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_indexes',\n              'path'   => ':upsertDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' }, # empty body on success\n            'post_process' => 'add_upsert_ack'\n          }\n        },\n        'vector.find_neighbors' => {\n          description: 'Find nearest neighbors from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'queries',           'required' => true },\n                { 'name' => 'distance_metric' },        # optional\n                { 'name' => 'feature_norm_type' },      # optional\n                { 'name' => 'include_stats' }           # optional\n              ],\n              'constraints' => [\n                # Exactly one locator per query: vector OR datapoint_id\n                {\n                  'type'   => 'xor',\n                  'scope'  => 'queries[]',\n                  'fields' => ['feature_vector', 'datapoint_id'],\n                  'aliases'=> { 'feature_vector' => ['vector'] } # honor your alias\n                },\n                # If a query omits neighbor_count, allow top-level neighbor_count or the internal default (10)\n                {\n                  'type'               => 'fallback_required',\n                  'scope'              => 'queries[]',\n                  'field'              => 'neighbor_count',\n                  'fallback_to_root'   => 'neighbor_count',\n                  'default_if_absent'  => 10  # matches your payload fallback\n                }\n              ]\n            },\n            'payload'  => { 'format' => 'find_neighbors' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':findNeighbors',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_find_neighbors'\n          }\n        },\n        'vector.read_datapoints' => {\n          description: 'Read datapoints (vectors) by ID from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'ids' },        # manual ids\n                { 'name' => 'groups' },     # from find_neighbors (normalized)\n                { 'name' => 'neighbors' }   # flattened neighbors\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['ids', 'groups', 'neighbors'] },\n                { 'type' => 'max_items', 'field' => 'ids', 'value' => 1000 }\n              ]\n            },\n            'payload'  => { 'format' => 'read_index_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':readIndexDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_read_index_datapoints'\n          }\n        }     \n "}
{"id":"lam:71f4544d121b37b0","kind":"lambda","name":"method","fqname":"method:configuration_registry","loc":{"line":1702,"column":28,"length":1079,"begin":72036,"end":73115},"file":"connector.rb","text":"ion_registry: lambda do |connection, user_config|\n      {\n        # Model selection\n        models: {\n          default: user_config['model'] || connection['default_model'] || 'gemini-1.5-flash',\n          strategy: connection['optimization_mode'] || 'balanced',\n          mode: user_config['model_mode'] || 'auto'\n        },\n        \n        # Generation settings\n        generation: {\n          temperature: user_config['temperature'],\n          max_tokens: user_config['max_tokens'],\n          top_p: user_config['top_p'],\n          top_k: user_config['top_k']\n        }.compact,\n        \n        # Features\n        features: {\n          caching: {\n            enabled: connection['enable_caching'] != false,\n            ttl: user_config['cache_ttl'] || 300\n          },\n          logging: {\n            enabled: connection['enable_logging'] == true\n          }\n        },\n        \n        # Execution\n        execution: {\n          retry: {\n            max_attempts: 3,\n            backoff: 1.0\n          },\n          rate_limit: {\n            rpm: 60\n          }\n        }\n "}
{"id":"lam:20991aee350538e8","kind":"lambda","name":"method","fqname":"method:execute_behavior","loc":{"line":1744,"column":22,"length":2835,"begin":73193,"end":76028},"file":"connector.rb","text":"ute_behavior: lambda do |connection, behavior, input, user_config = {}|\n      behavior_def = call('behavior_registry')[behavior] or error(\"Unknown behavior: #{behavior}\")\n      local_input = call('deep_copy', input) # Work on a local copy only\n\n      # Apply defaults without side effects\n      if behavior_def[:defaults]\n        behavior_def[:defaults].each { |k, v| local_input[k] = local_input.key?(k) ? local_input[k] : v }\n      end\n\n      # Bring model-selection keys into local_input so select_model can use them\n      %w[model model_mode lock_model_revision].each do |k|\n        if user_config.key?(k) && !user_config[k].nil?\n          local_input[k] = user_config[k]\n        end\n      end      \n\n      cfg = call('configuration_registry', connection, user_config)\n      operation_config = JSON.parse(JSON.dump(behavior_def[:config_template] || {})) # build op config from template\n\n      # Bring generation settings into the local input (don’t mutate cfg)\n      if cfg[:generation]\n        cfg[:generation].each { |k, v| local_input[k] = v unless v.nil? }\n      end\n\n      operation_config['model'] = call('select_model', behavior_def, cfg, local_input)\n      # methods.execute_behavior (after setting operation_config['model'])\n      if behavior_def[:supported_models].any? && !behavior_def[:supported_models].include?(operation_config['model'])\n        # force a sane fallback rather than issuing a bad :predict call\n        operation_config['model'] = call('select_model', behavior_def, cfg, local_input.merge('model_mode' => 'auto'))\n      end\n      operation_config['resilience'] = cfg[:execution]\n\n      # Caching key is derived from local_input\n      if cfg[:features][:caching][:enabled]\n        cache_key = \"vertex_#{behavior}_#{local_input.to_json.hash}\"\n        if (hit = call('memo_get', cache_key))\n          return hit\n        end\n      end\n\n      pmode = (local_input['prompt_mode'] || 'simple').to_s\n      case pmode\n      when 'contents'\n        operation_config['payload']   = { 'format' => 'vertex_contents' }\n        operation_config['validate']  = { 'schema' => [ { 'name' => 'contents', 'required' => true } ] }\n      when 'raw_json'\n        operation_config['payload']   = { 'format' => 'vertex_passthrough' }\n        operation_config['validate']  = { 'schema' => [ { 'name' => 'payload_json', 'required' => true } ] }\n      else\n        # keep the config_template default ('vertex_prompt')\n      end\n\n      # Gate index discovery\n      if behavior == 'vector.find_neighbors'\n        local_input = call('augment_vector_context', connection, local_input)\n      end\n\n      result = call('execute_pipeline', connection, behavior, local_input, operation_config)\n\n      if cfg[:features][:caching][:enabled]\n        call('memo_put', cache_key, result, cfg[:features][:caching][:ttl] || 300)\n      end\n\n    "}
{"id":"lam:94534e9717179760","kind":"lambda","name":"method","fqname":"method:add_upsert_ack","loc":{"line":1815,"column":20,"length":304,"begin":76213,"end":76517},"file":"connector.rb","text":"add_upsert_ack: lambda do |response, input|\n      # response is empty on success; return a useful ack\n      {\n        'ack'         => 'upserted',\n        'count'       => Array(input['datapoints']).size,\n        'index'       => input['index'],\n        'empty_body'  => (response.nil? || response == {})"}
{"id":"lam:791cc81cb2f87ed4","kind":"lambda","name":"method","fqname":"method:add_word_count","loc":{"line":1825,"column":20,"length":290,"begin":76540,"end":76830},"file":"connector.rb","text":"add_word_count: lambda do |response, input|\n      if response.is_a?(String)\n        { \n          'result' => response,\n          'word_count' => response.split.size\n        }\n      else\n        {\n          'result' => response,\n          'word_count' => response.to_s.split.size\n        }\n "}
{"id":"lam:eff626676d5fb1a8","kind":"lambda","name":"method","fqname":"method:apply_template","loc":{"line":1840,"column":20,"length":238,"begin":76884,"end":77122},"file":"connector.rb","text":"apply_template: lambda do |template, variables|\n      return template unless template && variables\n      \n      result = template.dup\n      variables.each do |key, value|\n        result = result.gsub(\"{#{key}}\", value.to_s)\n      end\n    "}
{"id":"lam:08246d5b08a1783e","kind":"lambda","name":"method","fqname":"method:approx_token_count","loc":{"line":1851,"column":24,"length":107,"begin":77184,"end":77291},"file":"connector.rb","text":"ox_token_count: lambda do |text|\n      # Fast, side-effect-free approximation\n      ((text.to_s.length) / 4"}
{"id":"lam:ac3d9f0c368b1b58","kind":"lambda","name":"method","fqname":"method:augment_vector_context","loc":{"line":1856,"column":28,"length":711,"begin":77322,"end":78033},"file":"connector.rb","text":"vector_context: lambda do |connection, local_input|\n      out = call('deep_copy', local_input)\n      need_metric = !call('value_present', out['distance_metric'])\n      need_norm   = !call('value_present', out['feature_norm_type'])\n      return out unless need_metric || need_norm\n\n      # Only attempt admin discovery if the connection is allowed\n      return out unless connection['allow_admin_discovery'] == true\n\n      begin\n        disc = call('discover_index_config', connection, out)\n        out['distance_metric']   ||= disc['distance_metric']\n        out['feature_norm_type'] ||= disc['feature_norm_type']\n      rescue\n        # Soft‑fail; confidence will be nil but neighbors still returned\n      end"}
{"id":"lam:fdf2ab9ec4618ada","kind":"lambda","name":"method","fqname":"method:build_endpoint_url","loc":{"line":1876,"column":24,"length":3050,"begin":78085,"end":81135},"file":"connector.rb","text":"ild_endpoint_url: lambda do |connection, endpoint_config, input|\n      v = connection['version']\n      api_version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n      region = connection['region']\n      base_regional = \"https://#{region}-aiplatform.googleapis.com/#{api_version}\"\n\n      family = endpoint_config['family']\n\n      case family\n      # PUBLISHER MODELS\n      when 'publisher_models'\n        api_version = (connection['version'].to_s.strip.empty? ? 'v1' : connection['version'])\n        publisher   = endpoint_config['publisher'] || 'google'\n        \"https://aiplatform.googleapis.com/#{api_version}/publishers/#{publisher}/models\"\n      # VECTOR INDEXES\n      when 'vector_indexes' # admin/data-plane ops on Index resources\n        index = call('qualify_resource', connection, 'index', input['index'] || endpoint_config['index'])\n        \"#{base_regional}/#{index}#{endpoint_config['path']}\" # e.g., ':upsertDatapoints'\n      # VECTOR INDEX ENDPOINTS\n      when 'vector_index_endpoints' # query via MatchService or admin reads\n        base =\n          if endpoint_config['admin'] == true\n            v = connection['version']; version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n            \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n          else\n            call('vector_search_base', connection, input) # uses vdb host when provided\n          end\n        ie = call('qualify_resource', connection, 'index_endpoint',\n                  input['index_endpoint'] || endpoint_config['index_endpoint'])\n        \"#{base}/#{ie}#{endpoint_config['path']}\" # e.g., ':findNeighbors' or ''\n\n\n      else\n        base_host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n        base_url  = \"https://#{base_host}/#{api_version}\"\n\n        model   = input['model'] || connection['default_model'] || 'gemini-1.5-flash'\n        model_id = model.to_s\n\n        # Honor lock model revision input flag\n        lock_rev = input['lock_model_revision'] == true || endpoint_config['require_version'] == true\n        if lock_rev && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n        # Only resolve to a numeric version when explicitly requested by endpoint config\n        if endpoint_config['require_version'] == true && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n\n        model_path = \"projects/#{connection['project']}/locations/#{region}/publishers/google/models/#{model_id}\"\n\n        # If the user supplies a custom path, replace the the critical elements with those from the connection\n        if endpoint_config['custom_path']\n          endpoint_config['custom_path']\n            .gsub('{project}',  connection['project'])\n            .gsub('{region}',   region)\n            .gsub('{endpoint}', connection['vector_search_endpoint'] || '')\n        else\n          \"#{base_url}/#{model_path}#{endpoint_config['path'] || ':generateContent'}\"\n        end"}
{"id":"lam:53e63761fa775f0b","kind":"lambda","name":"method","fqname":"method:build_generation_config","loc":{"line":1939,"column":29,"length":329,"begin":81171,"end":81500},"file":"connector.rb","text":"eneration_config: lambda do |vars|\n      {\n        'temperature'     => vars['temperature'] || 0.7,\n        'maxOutputTokens' => vars['max_tokens']  || 2048,\n        'topP'            => vars['top_p']       || 0.95,\n        'topK'            => vars['top_k']       || 40,\n        'stopSequences'   => vars['stop_sequences']\n     "}
{"id":"lam:9343b3f559672aaa","kind":"lambda","name":"method","fqname":"method:build_headers","loc":{"line":1950,"column":19,"length":147,"begin":81550,"end":81697},"file":"connector.rb","text":"   build_headers: lambda do |connection|\n      {\n        'Content-Type' => 'application/json',\n        'X-Goog-User-Project' => connection['project"}
{"id":"lam:ebfac5bb723be1e9","kind":"lambda","name":"method","fqname":"method:check_rate_limit","loc":{"line":1958,"column":22,"length":661,"begin":81742,"end":82403},"file":"connector.rb","text":"check_rate_limit: lambda do |operation, limits|\n      rpm  = (limits['rpm'] || limits[:rpm]).to_i\n      window_id     = Time.now.to_i / 60\n      window_start  = window_id * 60\n      key           = \"rate_#{operation}_#{window_id}\"\n\n      count = call('memo_get', key) || 0\n      error(\"Rate limit exceeded for #{operation}. Please wait before retrying.\") if count >= rpm\n\n      new_count = count + 1\n      reset_in  = (window_start + 60) - Time.now.to_i\n      reset_in  = 60 if reset_in <= 0\n\n      call('memo_put', key, new_count, reset_in)\n\n      { 'rpm' => rpm, 'count' => new_count, 'reset_in_s' => reset_in, 'window_started_at' => Time.at(window_start).utc"}
{"id":"lam:96474af78775f4d1","kind":"lambda","name":"method","fqname":"method:chunk_by_tokens","loc":{"line":1976,"column":21,"length":1696,"begin":82427,"end":84123},"file":"connector.rb","text":" chunk_by_tokens: lambda do |items:, token_ceiling:, max_items:, max_body_bytes: nil|\n      token_cap = token_ceiling.to_i\n      token_cap = 8000 if token_cap <= 0 # conservative fallback if not provided\n      max_items = (max_items || 100).to_i\n      max_items = 1 if max_items <= 0\n      max_body  = max_body_bytes ? max_body_bytes.to_i : nil\n\n      batches   = []\n      oversized = []\n\n      current       = []\n      current_tokens= 0\n      current_bytes = 0\n\n      # crude but steady overheads so we don’t undercount request size\n      per_item_overhead = 64\n      base_overhead     = 512\n\n      items.each do |item|\n        txt = item['text'].to_s\n        t   = call('approx_token_count', txt)\n        b   = txt.bytesize + per_item_overhead\n\n        # single-item guards\n        if t > token_cap\n          oversized << { 'item' => item, 'reason' => \"estimated tokens #{t} exceed ceiling #{token_cap}\" }\n          next\n        end\n        if max_body && (b + base_overhead) > max_body\n          oversized << { 'item' => item, 'reason' => \"approx body bytes #{b + base_overhead} exceed limit #{max_body}\" }\n          next\n        end\n\n        # would adding this item break any limit?\n        if !current.empty? &&\n          (current_tokens + t > token_cap ||\n            current.length + 1 > max_items ||\n            (max_body && current_bytes + b + base_overhead > max_body))\n          batches << current\n          current        = []\n          current_tokens = 0\n          current_bytes  = 0\n        end\n\n        current << item\n        current_tokens += t\n        current_bytes  += b\n      end\n\n      batches << current unless current.empty?\n\n      { 'batches' => batches, 'oversized' =>"}
{"id":"lam:b461ab38a24c469f","kind":"lambda","name":"method","fqname":"method:coerce_embeddings_to_datapoints","loc":{"line":2031,"column":37,"length":1241,"begin":84227,"end":85468},"file":"connector.rb","text":"ings_to_datapoints: lambda do |vars|\n      embeddings = Array(vars['embeddings'])\n      error('No embeddings provided') if embeddings.empty?\n\n      ids     = Array(vars['datapoint_ids'])\n      prefix  = (vars['datapoint_id_prefix'] || 'dp_').to_s\n      start   = (vars['start_index'] || 1).to_i\n      pad_to  = (vars['pad_to'] || 6).to_i\n\n      if ids.empty?\n        ids = embeddings.each_index.map { |i| \"#{prefix}#{(start + i).to_s.rjust(pad_to, '0')}\" }\n      elsif ids.length != embeddings.length\n        error(\"datapoint_ids length (#{ids.length}) must match embeddings length (#{embeddings.length})\")\n      end\n\n      common_restricts        = vars['common_restricts']\n      common_numeric          = vars['common_numeric_restricts']\n      common_crowding_tag     = vars['common_crowding_tag']\n      common_embedding_meta   = vars['embedding_metadata']\n\n      embeddings.each_with_index.map do |vec, i|\n        {\n          'datapointId'       => ids[i],\n          'featureVector'     => Array(vec).map(&:to_f),\n          'restricts'         => common_restricts,\n          'numericRestricts'  => common_numeric,\n          'crowdingTag'       => common_crowding_tag,\n          'embeddingMetadata' => common_embedding_meta\n        }.compa"}
{"id":"lam:00684efe6aea2045","kind":"lambda","name":"method","fqname":"method:coerce_kwargs","loc":{"line":2063,"column":19,"length":886,"begin":85490,"end":86376},"file":"connector.rb","text":"\n    coerce_kwargs: lambda do |*args, **kwargs|\n      # Non-destructive copies\n      positional = args.dup\n      kw = kwargs.dup\n\n      # If caller passed a trailing Hash, treat it as kwargs (merged with explicit kwargs)\n      if positional.last.is_a?(Hash)\n        trailing = positional.pop\n        # deep copy to avoid side-effects\n        trailing_copy = JSON.parse(JSON.dump(trailing)) rescue trailing.dup\n        trailing_sym  = trailing_copy.each_with_object({}) do |(k, v), acc|\n          key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n          acc[key] = v\n        end\n        # Explicit kwargs take precedence\n        kw = trailing_sym.merge(kw) { |_key, left, right| right }\n      end\n\n      # Ensure symbolized keys for kwargs\n      kw = kw.each_with_object({}) do |(k, v), acc|\n        key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n        acc[key] = v\n      end\n\n      [pos"}
{"id":"lam:4b44155c8554803d","kind":"lambda","name":"method","fqname":"method:confidence_from_distance","loc":{"line":2090,"column":30,"length":615,"begin":86413,"end":87028},"file":"connector.rb","text":"ence_from_distance: lambda do |distance, metric, feature_norm_type|\n      return nil unless distance\n      m = metric.to_s\n      case m\n      when 'COSINE_DISTANCE'\n        # distance = 1 - cos_sim  => confidence = (1 + cos_sim)/2 = 1 - distance/2\n        c = 1.0 - (distance.to_f / 2.0)\n        [[c, 0.0].max, 1.0].min\n      when 'DOT_PRODUCT_DISTANCE'\n        # distance = -dot; if vectors were UNIT_L2_NORM, dot ∈ [-1,1] ~ cos_sim\n        if feature_norm_type.to_s == 'UNIT_L2_NORM'\n          dot = -distance.to_f\n          c = 0.5 * (1.0 + dot)\n          [[c, 0.0].max, 1.0].min\n        end\n      else\n       "}
{"id":"lam:451e89882947a483","kind":"lambda","name":"method","fqname":"method:deep_copy","loc":{"line":2111,"column":15,"length":43,"begin":87076,"end":87119},"file":"connector.rb","text":"object\n    deep_copy: lambda { |obj| JSON.p"}
{"id":"lam:17b87205bf6bdfe0","kind":"lambda","name":"method","fqname":"method:discover_index_config","loc":{"line":2113,"column":27,"length":1500,"begin":87149,"end":88649},"file":"connector.rb","text":"iscover_index_config: lambda do |connection, input|\n      ep = call('qualify_resource', connection, 'index_endpoint', input['index_endpoint'])\n      dep_id = input['deployed_index_id'].to_s\n      return {} if ep.to_s.empty? || dep_id.empty?\n\n      cache_key = \"idxcfg:#{ep}:#{dep_id}\"\n      if (hit = call('memo_get', cache_key)); return hit; end\n\n      # 1) Read IndexEndpoint (admin host)\n      url_ep = call('build_endpoint_url', connection, {\n        'family' => 'vector_index_endpoints', 'index_endpoint' => ep, 'method' => 'GET', 'admin' => true\n      }, input)\n      ep_body = call('http_request', connection, method: 'GET', url: url_ep, headers: call('build_headers', connection))\n      deployed = Array(ep_body['deployedIndexes']).find { |d| d['id'] == dep_id }\n      return {} unless deployed && deployed['index']\n\n      # 2) Read Index (admin host)\n      url_idx = call('build_endpoint_url', connection, {\n        'family' => 'vector_indexes', 'index' => deployed['index'], 'method' => 'GET'\n      }, input)\n      idx_body = call('http_request', connection, method: 'GET', url: url_idx, headers: call('build_headers', connection))\n\n      cfg = idx_body.dig('metadata', 'config') || {}\n      out = {\n        'index'              => deployed['index'],\n        'distance_metric'    => (cfg['distanceMeasureType'] || cfg['distance_measure_type']),\n        'feature_norm_type'  => (cfg['featureNormType']     || cfg['feature_norm_type'])\n      }.compact\n\n      call('memo_put', cache_key, out, "}
{"id":"lam:cb19b66d925b420e","kind":"lambda","name":"method","fqname":"method:each_in_scope","loc":{"line":2147,"column":19,"length":273,"begin":88743,"end":89016},"file":"connector.rb","text":"l)\n    each_in_scope: lambda do |data, scope|\n      s = scope.to_s\n      if s.end_with?('[]')\n        key = s[0..-3] # strip []\n        arr = Array(data[key]) # safe\n        arr.each_with_index.map { |item, idx| [item || {}, \"#{key}[#{idx}]\"] }\n      else\n        [[data, '"}
{"id":"lam:a42434ccef7a4853","kind":"lambda","name":"method","fqname":"method:error_hint","loc":{"line":2159,"column":16,"length":565,"begin":89054,"end":89619},"file":"connector.rb","text":"ELPER\n    error_hint: lambda do |connection, code, status|\n      c = code.to_i\n      case c\n      when 401\n        # keep small + actionable\n        'Unauthorized. Re‑authenticate; then check project/region, API enablement, and roles.'\n      when 403\n        'Forbidden. Check project/region, API enablement, and roles.'\n      when 404\n        'Not found. Check project/region (feature/model availability) and the resource id.'\n      when 429\n        'Rate limit/quota. Reduce request rate or increase quota. Will honor Retry‑After when present.'\n      else\n   "}
{"id":"lam:6ada95537045ed6f","kind":"lambda","name":"method","fqname":"method:extract_ids_for_read","loc":{"line":2177,"column":26,"length":984,"begin":89717,"end":90701},"file":"connector.rb","text":"    extract_ids_for_read: lambda do |vars|\n      mode = (vars['id_source'] || 'auto').to_s\n      pick = lambda do |source|\n        case source\n        when 'manual'\n          Array(vars['ids']).compact\n        when 'neighbors'\n          Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact\n        when 'groups'\n          Array(vars['groups'])\n            .flat_map { |g| Array(g['neighbors']) }\n            .map { |n| n['datapoint_id'] }.compact\n        else # auto: prefer manual → neighbors → groups\n          ids = Array(vars['ids']).compact\n          ids = Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids = Array(vars['groups']).flat_map { |g| Array(g['neighbors']) }.map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids\n        end\n      end\n\n      ids = pick.call(mode).map(&:to_s)\n      ids = ids.uniq if vars['unique'] != false\n      error('No datapoint IDs provided or derivable from neighbors/groups') i"}
{"id":"lam:d506df87bad04896","kind":"lambda","name":"method","fqname":"method:extract_user_config","loc":{"line":2204,"column":25,"length":1397,"begin":90769,"end":92166},"file":"connector.rb","text":"fely\n    extract_user_config: lambda do |input, cfg_enabled = false, config_ctx = {}|\n      cfg = {}\n      config_ctx ||= {}\n\n      # Prefer config_fields values; fall back to input (back-compat)\n      mode = (config_ctx['model_mode'] || input['model_mode'] || '').to_s\n      explicit_model = config_ctx['model'] || input['model'] || input['model_override']\n\n      case mode\n      when 'explicit'\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      when 'connection', 'auto', ''\n        # no-op; use selection logic defaults\n      else\n        # unknown mode: treat as legacy explicit if model present\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      end\n\n      cfg['model_mode']          = mode unless mode.empty?\n      # After (prefer config_fields, fall back to input for completeness)\n      if config_ctx.key?('lock_model_revision')\n        cfg['lock_model_revision'] = config_ctx['lock_model_revision']\n      elsif input.key?('lock_model_revision')\n        cfg['lock_model_revision'] = input['lock_model_revision']\n      end\n\n      # Advanced tuning (unchanged)\n      if cfg_enabled\n        cfg['temperature'] = input['temperature'] if input.key?('temperature')\n        cfg['max_tokens']  = input['max_tokens']  if input.key?('max_tokens')\n        cfg['cache_ttl']   = input['cache_ttl']   if input.key?('cache_ttl')\n      "}
{"id":"lam:5715f937567fa94e","kind":"lambda","name":"method","fqname":"method:execute_batch_behavior","loc":{"line":2241,"column":28,"length":2671,"begin":92221,"end":94892},"file":"connector.rb","text":"n\n    execute_batch_behavior: lambda do |connection, behavior, items, batch_size, strategy, options = {}|\n      results = []\n      errors = []\n      total_processed = 0\n\n      local_items = call('deep_copy', Array(items))\n      \n      # 1) Build batches according to strategy\n      batches =\n        if strategy.to_s == 'tokens'\n          chunk = call('chunk_by_tokens',\n            items: local_items,\n            token_ceiling: (options['token_ceiling'] || options[:token_ceiling]),\n            max_items: (options['max_items_per_batch'] || options[:max_items_per_batch] || 100),\n            max_body_bytes: (options['max_body_bytes'] || options[:max_body_bytes])\n          )\n          # surface oversize items as per-batch errors (unchanged error shape: batch + error)\n          Array(chunk['oversized']).each do |o|\n            errors << { 'batch' => [o['item']], 'error' => \"Skipped item: #{o['reason']}\" }\n          end\n          chunk['batches'] || []\n        else\n          size  = (batch_size || 10).to_i\n          limit = (options['max_items_per_batch'] || options[:max_items_per_batch] || size).to_i\n          size  = [[size, limit].min, 1].max\n          local_items.each_slice(size).to_a\n        end\n\n      # 2) Execute batches\n      batches.each do |batch|\n        begin\n          if behavior.include?('embed')\n            texts = batch.map { |item| item['text'] }\n\n            payload = { 'texts' => texts }\n            unique_tasks = batch.map { |i| i['task_type'] }.compact.uniq\n            payload['task_type'] = unique_tasks.first if unique_tasks.length == 1\n\n            batch_result = call('execute_behavior', connection, behavior, payload)\n\n            # For embeddings, API is truly batchable: one result per batch (keep prior shape)\n            results.concat([batch_result])\n            total_processed += batch.length\n\n          else\n            # Non-embeddings: execute per-item so partial failures are surfaced\n            batch.each do |item|\n              begin\n                item_result = call('execute_behavior', connection, behavior, item)\n                results << item_result\n                total_processed += 1\n              rescue => e\n                errors << { 'batch' => [item], 'error' => e.message }\n              end\n            end\n          end\n\n        rescue => e\n          # catastrophic batch failure (network, quota, etc.)\n          errors << { 'batch' => batch, 'error' => e.message }\n        end\n      end\n      \n      {\n        'success'         => errors.empty?,\n        'results'         => results,\n        'errors'          => errors,\n        'total_processed' => total_processed,\n        'total_errors'    =>"}
{"id":"lam:00bb76312c7f9c24","kind":"lambda","name":"method","fqname":"method:format_user_error","loc":{"line":2314,"column":23,"length":400,"begin":94939,"end":95339},"file":"connector.rb","text":"HELPER\n    format_user_error: lambda do |err|\n      base = \"Vertex AI error #{err['code']}\"\n      base += \" #{err['status']}\" if err['status']\n      head = \"#{base}: #{err['summary']}\"\n      tags = [\"corr_id=#{err['correlation_id']}\"]\n      tags << \"remote_id=#{err['remote_request_id']}\" if err['remote_request_id']\n      msg = \"#{head} [#{tags.join(' ')}]\"\n      msg += \" — Hint: #{err['hint']}\" "}
{"id":"lam:f607db0cafb20ce8","kind":"lambda","name":"method","fqname":"method:get_behavior_input_fields","loc":{"line":2326,"column":31,"length":19673,"begin":95417,"end":115090},"file":"connector.rb","text":"\n    get_behavior_input_fields: lambda do |behavior, show_advanced, ui_cfg = {}|\n      show_advanced = !!show_advanced\n      ui_cfg ||= {}\n      explicit      = (ui_cfg['model_mode'] == 'explicit')\n      legacy_mode   = !ui_cfg.key?('model_mode')\n      include_model = false\n\n      behavior_def = call('behavior_registry')[behavior]\n      return [] unless behavior_def\n      \n      # Map behavior to input fields\n      case behavior\n      when 'text.generate'\n        fields = [\n          { name: 'prompt', label: 'Prompt', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.translate'\n        fields = [\n          { name: 'text', label: 'Text to Translate', control_type: 'text-area', optional: false },\n          { name: 'target_language', label: 'Target Language', control_type: 'select', pick_list: 'languages', optional: false },\n          { name: 'source_language', label: 'Source Language', control_type: 'select', pick_list: 'languages', optional: true, hint: 'Leave blank for auto-detection' }\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.summarize'\n        fields = [\n          { name: 'text', label: 'Text to Summarize', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.classify'\n        fields = [\n          { name: 'text', label: 'Text to Classify', control_type: 'text-area', optional: false },\n          { name: 'categories', label: 'Categories', type: 'array', of: 'object', properties: [\n            { name: 'name', label: 'Category Name' },\n            { name: 'description', label: 'Description' }\n          ]}\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.embed'\n        fields = [\n          { name: 'texts', label: 'Texts to Embed', type: 'array', of: 'string', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'vector.upsert_datapoints'\n        fields = [\n          # Target\n          { name: 'index', label: 'Index', group: 'Target', hint: 'Index resource or ID (e.g., projects/.../indexes/IDX or just IDX)', optional: false },\n          # Source: from embeddings (recommended path from Generate embeddings)\n          { name: 'embeddings', label: 'Embeddings', group: 'Source (from embeddings)', type: 'array', of: 'array', optional: true,\n            hint: 'Map from Generate embeddings → vectors or embeddings' },\n          { name: 'datapoint_ids', label: 'Datapoint IDs', group: 'Source (from embeddings)', type: 'array', of: 'string', \n            optional: true, hint: 'Optional; if omitted, IDs are auto-generated' },\n          { name: 'datapoint_id_prefix', label: 'Auto ID prefix', group: 'Source (from embeddings)', optional: true, default: 'dp_' },\n          { name: 'start_index', label: 'Starting index (1-based)', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 1 },\n          { name: 'pad_to', label: 'Pad IDs to N digits', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 6 },\n\n          # Datapoint defaults applied to all when using embeddings\n          { name: 'common_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n          ]},\n          { name: 'common_numeric_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n            { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n          ]},\n          { name: 'common_crowding_tag', group: 'Datapoint defaults', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n          { name: 'embedding_metadata', group: 'Datapoint defaults', type: 'object' },\n\n          # Advanced: provide full datapoints directly (legacy / power-user)\n          { name: 'datapoints', label: 'Datapoints (advanced)', group: 'Provide full datapoints',\n            type: 'array', of: 'object', optional: true, properties: [\n              { name: 'datapoint_id', label: 'Datapoint ID', optional: false },\n              { name: 'feature_vector', label: 'Feature vector', type: 'array', of: 'number', optional: false },\n              { name: 'restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n              ]},\n              { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n                { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n              ]},\n              { name: 'crowding_tag', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n              { name: 'embedding_metadata', type: 'object' }\n            ]}\n        ]\n      when 'vector.find_neighbors'\n        fields = [\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Overrides connection host just for this call (e.g. <hash>....vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', hint: 'Resource or ID (e.g. projects/.../indexEndpoints/IEP or IEP)', optional: false, group: 'Target' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n          { name: 'neighbor_count', label: 'Neighbors per query', type: 'integer', default: 10, group: 'Query' },\n          { name: 'return_full_datapoint', label: 'Return full datapoint', control_type: 'checkbox', group: 'Query' },\n\n          # NEW: scoring & aggregates\n          { name: 'distance_metric', label: 'Index distance metric', control_type: 'select',\n            pick_list: 'vector_distance_metrics', optional: true, group: 'Scoring & aggregates',\n            hint: 'Set if you want valid confidence scores. For DOT_PRODUCT, set Feature normalization to UNIT_L2_NORM.' },\n          { name: 'feature_norm_type', label: 'Feature normalization', control_type: 'select',\n            pick_list: 'vector_feature_norm_types', optional: true, group: 'Scoring & aggregates' },\n          { name: 'include_stats', label: 'Include aggregate stats', control_type: 'checkbox',\n            default: true, optional: true, group: 'Scoring & aggregates' },\n\n          { name: 'queries', label: 'Queries', type: 'array', of: 'object', group: 'Queries', properties: [\n            { name: 'datapoint_id', label: 'Query datapoint ID' },\n            { name: 'feature_vector', label: 'Query vector', type: 'array', of: 'number', hint: 'Use either vector or datapoint_id' },\n            { name: 'neighbor_count', label: 'Override neighbors for this query', type: 'integer' },\n            { name: 'restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n            ]},\n            { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' }, { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        mode = (ui_cfg['id_source'] || 'auto').to_s\n        fields = [\n          # Target\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Optional override (e.g. <hash>.vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, group: 'Target', hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n        ]\n        # Helper lambdas to append groups\n        add_manual = lambda {\n          fields << { name: 'ids', label: 'Datapoint IDs (manual)',\n                      type: 'array', of: 'string', optional: true, group: 'IDs' }\n        }\n        add_neighbors = lambda {\n          fields << { name: 'neighbors', label: 'k‑NN neighbors (flattened)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [{ name: 'datapoint_id' }] }\n          fields << { name: 'unique', label: 'Deduplicate IDs',\n                      control_type: 'checkbox', default: true, group: 'Map from Find neighbors' }\n        }\n        add_groups = lambda {\n          fields << { name: 'groups', label: 'k‑NN groups (from Find neighbors)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [\n                        "}
{"id":"lam:9b139a36726019bf","kind":"lambda","name":"method","fqname":"method:get_behavior_output_fields","loc":{"line":2646,"column":32,"length":3245,"begin":115162,"end":118407},"file":"connector.rb","text":"ields\n    get_behavior_output_fields: lambda do |behavior|\n      case behavior\n      when 'text.generate'\n        [{ name: 'result', label: 'Generated Text' }]\n      when 'text.translate'\n        [\n          { name: 'result', label: 'Translated Text' },\n          { name: 'detected_language', label: 'Detected Source Language' }\n        ]\n      when 'text.summarize'\n        [\n          { name: 'result', label: 'Summary' },\n          { name: 'word_count', type: 'integer' }\n        ]\n      when 'text.classify'\n        [\n          { name: 'category', label: 'Selected Category' },\n          { name: 'confidence', type: 'number' }\n        ]\n      when 'text.embed'\n        [\n          { name: 'embeddings', type: 'array', of: 'array' },\n          { name: 'vectors', type: 'array', of: 'object', properties: [ { name: 'feature_vector', type: 'array', of: 'number' } ]},\n          { name: 'count', type: 'integer' },\n          { name: 'dimension', type: 'integer' },\n          { name: 'avg_norm', type: 'number' },\n          { name: 'norms', type: 'array', of: 'number' } \n        ]\n      when 'vector.upsert_datapoints'\n        [\n          { name: 'ack' }, { name: 'count', type: 'integer' }, { name: 'index' }, { name: 'empty_body', type: 'boolean' }\n        ]\n      when 'vector.find_neighbors'\n        [\n          { name: 'summary', type: 'object', properties: [\n            { name: 'groups', type: 'integer' },\n            { name: 'neighbors', type: 'integer' },\n            { name: 'distance_mean', type: 'number' },\n            { name: 'score_mean', type: 'number' },\n            { name: 'score_max',  type: 'number' },\n            { name: 'confidence_mean', type: 'number' },\n            { name: 'confidence_max',  type: 'number' }\n          ]},\n          { name: 'groups', type: 'array', of: 'object', properties: [\n            { name: 'query_id' },\n            { name: 'stats', type: 'object', properties: [\n              { name: 'neighbor_count', type: 'integer' },\n              { name: 'distance_mean', type: 'number' },\n              { name: 'score_mean',     type: 'number' },\n              { name: 'score_max',      type: 'number' },\n              { name: 'confidence_mean', type: 'number' },\n              { name: 'confidence_max',  type: 'number' }\n            ]},\n            { name: 'neighbors', type: 'array', of: 'object', properties: [\n              { name: 'datapoint_id' },\n              { name: 'distance', type: 'number' },\n              { name: 'score',    type: 'number' },\n              { name: 'confidence', type: 'number' },\n              { name: 'datapoint', type: 'object' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        [\n          { name: 'datapoints', type: 'array', of: 'object', properties: [\n            { name: 'datapoint_id' },\n            { name: 'feature_vector', type: 'array', of: 'number' },\n            { name: 'restricts', type: 'array', of: 'object' },\n            { name: 'numeric_restricts', type: 'array', of: 'object' },\n            { name: 'crowding_tag', type: 'object' },\n            { name: 'embedding_metadata', type: 'object' }\n          ] }\n        ]\n      when 'multimodal.analyze'\n        [{ name: 'result', label: 'Analysis' }]\n      else\n        "}
{"id":"lam:a1566d424ac58ea5","kind":"lambda","name":"method","fqname":"method:list_publisher_models","loc":{"line":2727,"column":27,"length":758,"begin":118482,"end":119240},"file":"connector.rb","text":" (v1beta1)\n    list_publisher_models: lambda do |connection, publisher: 'google'|\n      ver = connection['version'].to_s.strip\n      ver = ver.empty? ? 'v1' : ver\n      cache_key = \"pub_models:#{publisher}:#{ver}\"   # <— include version in key\n\n      if (cached = call('memo_get', cache_key))\n        return cached\n      end\n\n      url = call('build_endpoint_url', connection, { 'family' => 'publisher_models', 'publisher' => publisher }, {})\n      resp = call('http_request', connection, method: 'GET', url: url,\n                  headers: call('build_headers', connection),\n                  retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429, 500, 502, 503, 504] })\n\n      models = (resp['publisherModels'] || [])\n      call('memo_put', cache"}
{"id":"lam:8dbd666879789dc5","kind":"lambda","name":"method","fqname":"method:memo_store","loc":{"line":2746,"column":16,"length":25,"begin":119259,"end":119284},"file":"connector.rb","text":"\n      models\n    end,\n\n "}
{"id":"lam:9028ce130c46b44f","kind":"lambda","name":"method","fqname":"method:memo_get","loc":{"line":2748,"column":14,"length":178,"begin":119301,"end":119479},"file":"connector.rb","text":"mbda { @__memo ||= {} },\n\n    memo_get: lambda do |key|\n      item = call('memo_store')[key]\n      return nil unless item\n      exp = item['exp']\n      return nil if exp && Time."}
{"id":"lam:2efe6b8330c9374f","kind":"lambda","name":"method","fqname":"method:memo_put","loc":{"line":2756,"column":14,"length":145,"begin":119496,"end":119641},"file":"connector.rb","text":"    item['val']\n    end,\n\n    memo_put: lambda do |key, val, ttl=nil|\n      call('memo_store')[key] = { 'val' => val, 'exp' => (ttl ? Time.now.to"}
{"id":"lam:c576223264344f59","kind":"lambda","name":"method","fqname":"method:normalize_find_neighbors","loc":{"line":2762,"column":30,"length":2416,"begin":119750,"end":122166},"file":"connector.rb","text":"dly shape\n    normalize_find_neighbors: lambda do |resp, input|\n      groups_raw = Array(resp['nearestNeighbors'])\n      metric     = input['distance_metric']\n      norm_type  = input['feature_norm_type']\n      include_stats = input.key?('include_stats') ? !!input['include_stats'] : true\n\n      groups = groups_raw.map do |nn|\n        neighbors = Array(nn['neighbors']).map do |n|\n          dist = n['distance']\n          did  = n.dig('datapoint', 'datapointId')\n          {\n            'datapoint_id' => did,\n            'distance'     => dist,\n            # Legacy score: normalized from distance (cosine heuristic)\n            'score'        => call('transform_data', input: dist, from_format: 'distance', to_format: 'similarity'),\n            # New: mathematically valid confidence when possible\n            'confidence'   => call('confidence_from_distance', dist, metric, norm_type),\n            'datapoint'    => n['datapoint']\n          }.compact\n        end\n\n        stats =\n          if include_stats\n            {\n              'neighbor_count'   => neighbors.length,\n              'distance_mean'    => call('safe_mean', neighbors.map { |z| z['distance'] }),\n              'score_mean'       => call('safe_mean', neighbors.map { |z| z['score'] }),\n              'score_max'        => (neighbors.map { |z| z['score'] }.compact.max),\n              'confidence_mean'  => call('safe_mean', neighbors.map { |z| z['confidence'] }),\n              'confidence_max'   => (neighbors.map { |z| z['confidence'] }.compact.max)\n            }.compact\n          end\n\n        {\n          'query_id'  => nn['id'],\n          'stats'     => stats,\n          'neighbors' => neighbors\n        }.compact\n      end\n\n      # Top-level summary if desired\n      summary =\n        if include_stats\n          flat = groups.flat_map { |g| g['neighbors'] || [] }\n          {\n            'groups'          => groups.length,\n            'neighbors'       => flat.length,\n            'distance_mean'   => call('safe_mean', flat.map { |z| z['distance'] }),\n            'score_mean'      => call('safe_mean', flat.map { |z| z['score'] }),\n            'score_max'       => (flat.map { |z| z['score'] }.compact.max),\n            'confidence_mean' => call('safe_mean', flat.map { |z| z['confidence'] }),\n            'confidence_max'  => (flat.map { |z| z['confidence'] }.compact.max)\n          }.compact\n        end\n\n      { 'summary' => summa"}
{"id":"lam:3b62cca77849ec90","kind":"lambda","name":"method","fqname":"method:normalize_http_error","loc":{"line":2820,"column":26,"length":1256,"begin":122195,"end":123451},"file":"connector.rb","text":"act\n    end,\n\n    normalize_http_error: lambda do |connection, code:, body:, headers:, message:, url:, corr_id:, attempt:, duration_ms:|\n      parsed = {}\n      if body.is_a?(Hash)\n        parsed = body\n      else\n        begin\n          parsed = JSON.parse(body.to_s)\n        rescue\n          parsed = {}\n        end\n      end\n\n      gerr    = parsed['error'].is_a?(Hash) ? parsed['error'] : {}\n      status  = gerr['status']\n      summary = (gerr['message'] || message || body.to_s).to_s.strip[0, 300] # compact\n      hint    = call('error_hint', connection, code, status)\n\n      remote_id = nil\n      if headers\n        remote_id = headers['x-request-id'] ||\n                    headers['x-cloud-trace-context'] ||\n                    headers['x-guploader-uploadid']\n      end\n\n      {\n        'code'              => code.to_i,\n        'status'            => status,\n        'summary'           => summary,\n        'hint'              => hint,\n        'retryable'         => call('retryable_http_code', code),\n        'retry_after_s'     => call('parse_retry_after', headers),\n        'correlation_id'    => corr_id,\n        'remote_request_id' => remote_id,\n        'attempt'           => attempt,\n        'duration_ms'       => duration_ms,\n        'u"}
{"id":"lam:1025920ca0821d68","kind":"lambda","name":"method","fqname":"method:normalize_read_index_datapoints","loc":{"line":2859,"column":37,"length":704,"begin":123491,"end":124195},"file":"connector.rb","text":",\n\n    normalize_read_index_datapoints: lambda do |resp, _input|\n      # Expected Vertex shape: { \"datapoints\": [ { \"datapointId\": \"...\", \"featureVector\": [...],\n      #   \"restricts\": [...], \"numericRestricts\": [...], \"crowdingTag\": {...}, \"embeddingMetadata\": {...} } ] }\n      dps = Array(resp['datapoints']).map do |d|\n        {\n          'datapoint_id'      => d['datapointId'] || d['id'],\n          'feature_vector'    => Array(d['featureVector']).map(&:to_f),\n          'restricts'         => d['restricts'],\n          'numeric_restricts' => d['numericRestricts'],\n          'crowding_tag'      => d['crowdingTag'],\n          'embedding_metadata'=> d['embeddingMetadata']\n        }.compact\n      e"}
{"id":"lam:c1bfc1a36f0f0238","kind":"lambda","name":"method","fqname":"method:normalize_safety_settings","loc":{"line":2876,"column":31,"length":1080,"begin":124261,"end":125341},"file":"connector.rb","text":"settings\n    normalize_safety_settings: lambda do |input|\n      # Accepts either the new array shape or the legacy hash; returns array\n      if input.is_a?(Array)\n        # non-destructive copy with only supported keys\n        return input.map do |r|\n          {\n            'category'  => r['category']  || r[:category],\n            'threshold' => r['threshold'] || r[:threshold],\n            'method'    => r['method']    || r[:method]\n          }.compact\n        end\n      end\n\n      # Legacy object: { harassment: 'BLOCK_...', hate_speech: 'BLOCK_...', ... }\n      if input.is_a?(Hash)\n        map = {\n          'harassment'          => 'HARM_CATEGORY_HARASSMENT',\n          'hate_speech'         => 'HARM_CATEGORY_HATE_SPEECH',\n          'sexually_explicit'   => 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n          'dangerous_content'   => 'HARM_CATEGORY_DANGEROUS_CONTENT'\n        }\n        return input.each_with_object([]) do |(k, v), arr|\n          next if v.nil? || v.to_s.strip.empty?\n          cat = map[k.to_s]\n          arr << { 'category' => cat, 'threshold' => v } if cat"}
{"id":"lam:c9655815b09851ba","kind":"lambda","name":"method","fqname":"method:parse_retry_after","loc":{"line":2908,"column":23,"length":257,"begin":125386,"end":125643},"file":"connector.rb","text":"  # RETRY HELPER\n    parse_retry_after: lambda do |headers|\n      return nil unless headers\n      ra = headers['Retry-After'] || headers['retry-after']\n      return nil if ra.nil? || ra.to_s.strip.empty?\n      # integer seconds only (safe & simple)\n      ra"}
{"id":"lam:1f546340feb915ff","kind":"lambda","name":"method","fqname":"method:qualify_resource","loc":{"line":2917,"column":22,"length":415,"begin":125748,"end":126163},"file":"connector.rb","text":"ting caller input\n    qualify_resource: lambda do |connection, type, value|\n      return value if value.to_s.start_with?('projects/')\n      project = connection['project']\n      region  = connection['region']\n      case type.to_s\n      when 'index'          then \"projects/#{project}/locations/#{region}/indexes/#{value}\"\n      when 'index_endpoint' then \"projects/#{project}/locations/#{region}/indexEndpoints/#{va"}
{"id":"lam:bf2246b7bbd79113","kind":"lambda","name":"method","fqname":"method:resolve_model_version","loc":{"line":2929,"column":27,"length":680,"begin":126248,"end":126928},"file":"connector.rb","text":"on available\n    resolve_model_version: lambda do |connection, short|\n      # If already versioned, keep it\n      return short if short.to_s.match?(/-\\d{3,}$/)\n\n      cache_key = \"model_resolve:#{short}\"\n      if (cached = call('memo_get', cache_key))\n        return cached\n      end\n\n      ids = Array(call('list_publisher_models', connection))\n              .map { |m| (m['name'] || '').split('/').last }\n              .select { |id| id.start_with?(\"#{short}-\") }\n\n      latest = ids.max_by { |id| id[/-(\\d+)$/, 1].to_i }\n\n      # IMPORTANT: if nothing is found, fall back to the alias itself (Vertex supports aliases)\n      chosen = latest || short\n      call('memo_put', cache"}
{"id":"lam:abcecdea387db933","kind":"lambda","name":"method","fqname":"method:retryable_http_code","loc":{"line":2951,"column":25,"length":78,"begin":126975,"end":127053},"file":"connector.rb","text":"# RETRY HELPER\n    retryable_http_code: lambda { |code|\n      [408, 429, 500, "}
{"id":"lam:d005b7cf0597757d","kind":"lambda","name":"method","fqname":"method:safe_mean","loc":{"line":2955,"column":15,"length":120,"begin":127071,"end":127191},"file":"connector.rb","text":"lude?(code.to_i)\n    },\n\n    safe_mean: lambda do |arr|\n      xs = Array(arr).compact\n      return nil if xs.empty?\n    "}
{"id":"lam:3ad576048d2ee938","kind":"lambda","name":"method","fqname":"method:select_model","loc":{"line":2962,"column":18,"length":2087,"begin":127240,"end":129327},"file":"connector.rb","text":"Model selection logic\n    select_model: lambda do |behavior_def, cfg, input|\n      # 0) Respect explicit model in put\n      if (input['model'] && !input['model'].to_s.strip.empty?) ||\n        (input['model_override'] && !input['model_override'].to_s.strip.empty?)\n        return input['model'] || input['model_override']\n      end\n\n      mode      = (input['model_mode'] || cfg.dig(:models, :mode) || 'auto').to_s\n      strategy  = (cfg.dig(:models, :strategy) || 'balanced').to_s\n      supported = Array(behavior_def[:supported_models]).compact\n      default   = cfg.dig(:models, :default)\n\n      # Prefer an item if supported, else first supported, else default\n      prefer = lambda do |*candidates|\n        # Choose the first candidate that is in 'supported'; else first supported; else default\n        c = candidates.flatten.compact.find { |m| supported.include?(m) }\n        c || supported.first || default\n      end\n\n      case mode\n      when 'connection'\n        # Only honor connection default if it's supported by this behavior\n        return default if supported.include?(default)\n        return supported.first || default\n      when 'explicit'\n        # If user chose 'explicit' but didn't supply a model, pick a safe supported default\n        return supported.first || default\n      else # 'auto'\n        if behavior_def[:capability].to_s == 'embedding'\n          case strategy\n          when 'cost'        then prefer.call('textembedding-gecko', 'text-embedding-005', 'text-embedding-004')\n          when 'performance' then prefer.call('gemini-embedding-001', 'text-embedding-005', 'textembedding-gecko', 'text-embedding-004')\n          else                    prefer.call('text-embedding-005', 'gemini-embedding-001', 'textembedding-gecko', 'text-embedding-004')\n          end\n        else\n          case strategy\n          when 'cost'        then prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n          when 'performance' then prefer.call('gemini-1.5-pro',   'gemini-1.5-flash')\n          else                    prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n   "}
{"id":"lam:cc546e3e219b1f70","kind":"lambda","name":"method","fqname":"method:telemetry_envelope_fields","loc":{"line":3007,"column":31,"length":325,"begin":129409,"end":129734},"file":"connector.rb","text":"red) ===\n    telemetry_envelope_fields: lambda do\n      [\n        { name: 'success', type: 'boolean' },\n        { name: 'timestamp', type: 'datetime' },\n        { name: 'metadata', type: 'object', properties: [\n          { name: 'operation' }, { name: 'model' }\n        ]},\n        { name: 'trace', type: 'object', properties"}
{"id":"lam:0d4e21030d86dc9d","kind":"lambda","name":"method","fqname":"method:trace_fields","loc":{"line":3017,"column":18,"length":525,"begin":129754,"end":130279},"file":"connector.rb","text":"') }\n      ]\n    end,\n    trace_fields: lambda do\n      [\n        { name: 'correlation_id' },\n        { name: 'duration_ms', type: 'integer' },\n        { name: 'attempt', type: 'integer' },\n        { name: 'http_status', type: 'integer' },\n        { name: 'remote_request_id' }, \n        { name: 'rate_limit', type: 'object', properties: [\n          { name: 'rpm', type: 'integer' },\n          { name: 'count', type: 'integer' },\n          { name: 'reset_in_s', type: 'integer' },\n          { name: 'window_started_at', type:"}
{"id":"lam:7f8e590173ca809a","kind":"lambda","name":"method","fqname":"method:to_query","loc":{"line":3034,"column":14,"length":579,"begin":130338,"end":130917},"file":"connector.rb","text":"emetry schema helpers===\n\n    to_query: lambda do |params|\n      encode = lambda do |s|\n        # RFC3986 unreserved: ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n        s.to_s.bytes.map { |b|\n          if (48..57).cover?(b) || (65..90).cover?(b) || (97..122).cover?(b) || [45,46,95,126].include?(b)\n            b.chr\n          else\n            \"%%%02X\" % b\n          end\n        }.join\n      end\n\n      params.flat_map do |k, v|\n        key = encode.call(k)\n        if v.is_a?(Array)\n          v.map { |e| \"#{key}=#{encode.call(e)}\" }\n        else\n          \"#{key}=#{encode.call(v)}\""}
{"id":"lam:48ca21c6f4f69d15","kind":"lambda","name":"method","fqname":"method:value_present","loc":{"line":3057,"column":19,"length":174,"begin":131003,"end":131177},"file":"connector.rb","text":"} treated as absent)\n    value_present: lambda do |v|\n      return false if v.nil?\n      return false if v.is_a?(String) && v.strip.empty?\n      return false if v.respond_to?"}
{"id":"lam:b502fec778cfc488","kind":"lambda","name":"method","fqname":"method:vector_search_base","loc":{"line":3065,"column":24,"length":653,"begin":131289,"end":131942},"file":"connector.rb","text":" when provided.\n    vector_search_base: lambda do |connection, input|\n      host = (input['endpoint_host'] || connection['vector_search_endpoint']).to_s.strip\n      v = connection['version']\n      version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n\n      if host.empty?\n        # Fallback to regional API host (works for admin ops; query should use public vdb host)\n        \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n      elsif host.include?('vdb.vertexai.goog')\n        \"https://#{host}/#{version}\"\n      else\n        # Allow passing a full https://... custom host\n        host = host.sub(%r{\\Ahttps?://}i, '')\n        \"https"}
{"id":"lam:3624e3e86e7ee03a","kind":"lambda","name":"method","fqname":"method:wrap_embeddings_vectors","loc":{"line":3083,"column":29,"length":745,"begin":132052,"end":132797},"file":"connector.rb","text":"erve trace\n    wrap_embeddings_vectors: lambda do |response, input|\n      raw = if response.is_a?(Hash) && response.key?('result')\n        response['result']\n      else\n        response\n      end\n      arr = Array(raw).map { |v| Array(v).map(&:to_f) }\n\n      norms = arr.map { |v| Math.sqrt(v.reduce(0.0) { |s, x| s + (x.to_f * x.to_f) }) }\n      dim   = arr.first ? arr.first.length : nil\n\n      out = {\n        'embeddings' => arr,\n        'vectors'    => arr.map { |v| { 'feature_vector' => v } },\n        'count'      => arr.length,\n        'dimension'  => dim,\n        'norms'      => norms,\n        'avg_norm'   => call('safe_mean', norms)\n      }.compact\n\n      if response.is_a?(Hash) && response['_trace']\n        out['_trace'] = respon"}
{"id":"lam:ff27b6be19c8932c","kind":"lambda","name":"input_fields","fqname":"action:batch_operation#input_fields","loc":{"line":263,"column":20,"length":1266,"begin":11232,"end":12498},"file":"connector.rb","text":"ds: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens ≈ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n   "}
{"id":"lam:4bad4316e0299946","kind":"lambda","name":"execute","fqname":"action:batch_operation#execute","loc":{"line":292,"column":15,"length":1098,"begin":12531,"end":13629},"file":"connector.rb","text":"cute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n   "}
{"id":"lam:491b42dbeabbfac4","kind":"lambda","name":"output_fields","fqname":"action:batch_operation#output_fields","loc":{"line":319,"column":21,"length":439,"begin":13667,"end":14106},"file":"connector.rb","text":"elds: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n   "}
{"id":"lam:d7bb31efae37a50a","kind":"lambda","name":"sample_output","fqname":"action:batch_operation#sample_output","loc":{"line":331,"column":21,"length":711,"begin":14144,"end":14855},"file":"connector.rb","text":"tput: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n   "}
{"id":"lam:efba8721aada71b3","kind":"lambda","name":"input_fields","fqname":"action:vertex_operation#input_fields","loc":{"line":398,"column":20,"length":252,"begin":16904,"end":17156},"file":"connector.rb","text":"elds: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        behavior = cfg['behavior']\n        behavior ? call('get_behavior_input_fields', behavior, cfg['advanced_config'], cfg) : []\n   "}
{"id":"lam:b04826590cf705e8","kind":"lambda","name":"execute","fqname":"action:vertex_operation#execute","loc":{"line":409,"column":15,"length":477,"begin":17461,"end":17938},"file":"connector.rb","text":"cute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior     = config_fields['behavior']\n        user_config  = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input   = call('deep_copy', input) # do NOT mutate Workato’s input\n\n        # Leave advanced fields in safe_input; pipeline reads only what it needs\n        call('execute_behavior', connection, behavior, safe_input, user_config)\n "}
{"id":"lam:928ad1b2946fa1c1","kind":"lambda","name":"output_fields","fqname":"action:vertex_operation#output_fields","loc":{"line":404,"column":21,"length":234,"begin":17194,"end":17428},"file":"connector.rb","text":"elds: lambda do |_object_definitions, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('telemetry_envelope_fields') + (call('get_behavior_output_fields', cfg['behavior']) || [])\n   "}
{"id":"lam:4ecbbb774415d5b8","kind":"lambda","name":"sample_output","fqname":"action:vertex_operation#sample_output","loc":{"line":418,"column":21,"length":2253,"begin":17976,"end":20229},"file":"connector.rb","text":"output: lambda do |_connection, config_fields|\n        behavior = (config_fields.is_a?(Hash) ? config_fields : {})['behavior']\n        case behavior\n        when 'text.generate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.generate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hello world.\" }\n        when 'text.translate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.translate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hola mundo.\", \"detected_language\"=>\"en\" }\n        when 'text.summarize'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.summarize\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Concise summary.\", \"word_count\"=>2 }\n        when 'text.classify'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.classify\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"category\"=>\"Support\", \"confidence\"=>0.98 }\n        when 'text.embed'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.embed\", \"model\"=>\"text-embedding-004\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"embeddings\"=>[[0.01,0.02,0.03]] }\n        when 'multimodal.analyze'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"multimodal.analyze\", \"model\"=>\"gemini-1.5-pro\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"The image shows a tabby cat on a desk.\" }\n        else\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"unknown\", \"model\"=>\"gemini-1.5-flash\" } }\n        end\n "}
{"id":"lam:cfb99e39c792c27c","kind":"lambda","name":"input_fields","fqname":"action:discover_index_config#input_fields","loc":{"line":466,"column":20,"length":311,"begin":20609,"end":20920},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        [\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false }\n        ]\n "}
{"id":"lam:5d4343699975d1d1","kind":"lambda","name":"execute","fqname":"action:discover_index_config#execute","loc":{"line":481,"column":15,"length":292,"begin":21230,"end":21522},"file":"connector.rb","text":"xecute: lambda do |connection, input|\n        safe = call('deep_copy', input)\n        disc = call('discover_index_config', connection, safe)\n        call('enrich_response',\n          response: disc,\n          metadata: { 'operation' => 'vector.discover_config', 'model' => 'n/a' }\n        )\n "}
{"id":"lam:836c170ab84fa50b","kind":"lambda","name":"output_fields","fqname":"action:discover_index_config#output_fields","loc":{"line":473,"column":21,"length":239,"begin":20958,"end":21197},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'index', label: 'Index resource' },\n          { name: 'distance_metric' },\n          { name: 'feature_norm_type' }\n        ]\n "}
{"id":"lam:6c8bba19ecc427ad","kind":"lambda","name":"sample_output","fqname":"action:discover_index_config#sample_output","loc":{"line":490,"column":21,"length":448,"begin":21560,"end":22008},"file":"connector.rb","text":"output: lambda do |_connection, _cfg|\n        {\n          \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n          \"metadata\"=>{ \"operation\"=>\"vector.discover_config\", \"model\"=>\"n/a\" },\n          \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>12, \"attempt\"=>1 },\n          \"index\"=>\"projects/.../locations/us-central1/indexes/123\",\n          \"distance_metric\"=>\"COSINE_DISTANCE\",\n          \"feature_norm_type\"=>\"UNIT_L2_NORM\"\n        }\n "}
{"id":"lam:8b28ea7354bcba31","kind":"lambda","name":"input_fields","fqname":"action:classify_text#input_fields","loc":{"line":531,"column":20,"length":208,"begin":23594,"end":23802},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.classify', cfg['advanced_config'], cfg)\n "}
{"id":"lam:f1070c8589ce6dd8","kind":"lambda","name":"execute","fqname":"action:classify_text#execute","loc":{"line":542,"column":15,"length":335,"begin":24021,"end":24356},"file":"connector.rb","text":"xecute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input) # never mutate Workato’s input\n        call('execute_behavior', connection, 'text.classify', safe, user_cfg)"}
{"id":"lam:96720ba54be012ca","kind":"lambda","name":"output_fields","fqname":"action:classify_text#output_fields","loc":{"line":537,"column":21,"length":146,"begin":23841,"end":23987},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.classify')\n "}
{"id":"lam:b63b7d43b01735ba","kind":"lambda","name":"sample_output","fqname":"action:classify_text#sample_output","loc":{"line":549,"column":21,"length":392,"begin":24396,"end":24788},"file":"connector.rb","text":"e_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"text.classify\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"category\"  => \"Support\",\n          \"confidence\"=> 0.98\n        }"}
{"id":"lam:83ed9767fd8ce2fb","kind":"lambda","name":"input_fields","fqname":"action:generate_text#input_fields","loc":{"line":592,"column":20,"length":3383,"begin":26885,"end":30268},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg       = config_fields.is_a?(Hash) ? config_fields : {}\n        mode      = (cfg['prompt_mode'] || 'simple').to_s\n        show_adv  = !!cfg['advanced_config']\n\n        case mode\n        when 'contents'\n          fields = [\n            # Prompt structure\n            { name: 'contents', label: 'Contents', type: 'array', of: 'object', group: 'Prompt structure', optional: false,\n              properties: [\n                { name: 'role', label: 'Role', control_type: 'select',\n                  options: [['User','user'], ['Model','model']], optional: true },\n                { name: 'parts', label: 'Parts', type: 'array', of: 'object', properties: [\n                  { name: 'text',        label: 'Text' },\n                  { name: 'inline_data', label: 'Inline data', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'data',      label: 'Base64 data', control_type: 'text-area' }\n                  ]},\n                  { name: 'file_data',   label: 'File data (URI)', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'file_uri',  label: 'File URI' }\n                  ]}\n                ]}\n              ]\n            }\n          ]\n          if show_adv\n            fields += [\n              { name: 'system', label: 'System instruction', control_type: 'text-area', group: 'Advanced' },\n              { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n                properties: [\n                  { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                  { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                  { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n                ]\n              },\n              { name: 'response_mime_type', label: 'Response MIME type', group: 'Advanced',\n                hint: 'e.g., application/json for JSON mode' },\n              { name: 'response_schema', label: 'Response schema (object)', type: 'object', group: 'Advanced',\n                hint: 'When set, a compatible response_mime_type is required' },\n              { name: 'temperature', label: 'Temperature', type: 'number', group: 'Advanced', hint: '0.0 to 1.0' },\n              { name: 'max_tokens',  label: 'Max Tokens',  type: 'integer', group: 'Advanced' },\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        when 'raw_json'\n          fields = [\n            { name: 'payload_json', label: 'Full request JSON', control_type: 'text-area',\n              optional: false, group: 'Prompt (raw JSON)',\n              hint: 'Paste the entire models.generateContent request body including contents[].' }\n          ]\n          if show_adv\n            fields += [\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        else # 'simple'\n          call('get_behavior_input_fields', 'text.generate', show_adv, cfg)\n        end"}
{"id":"lam:ba5700ad83c28e67","kind":"lambda","name":"execute","fqname":"action:generate_text#execute","loc":{"line":662,"column":15,"length":481,"begin":30485,"end":30966},"file":"connector.rb","text":" execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg   = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input = call('deep_copy', input)\n        # Make prompt mode visible to the pipeline selector without mutating recipe input\n        safe_input['prompt_mode'] = config_fields['prompt_mode'] || 'simple'\n        call('execute_behavior', connection, 'text.generate', safe_input, user_cfg)"}
{"id":"lam:46471c377a0ef176","kind":"lambda","name":"output_fields","fqname":"action:generate_text#output_fields","loc":{"line":658,"column":21,"length":146,"begin":30306,"end":30452},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.generate')"}
{"id":"lam:3b6bd785fd5b6a22","kind":"lambda","name":"sample_output","fqname":"action:generate_text#sample_output","loc":{"line":670,"column":21,"length":346,"begin":31008,"end":31354},"file":"connector.rb","text":"e_output: lambda do |_connection, _cfg|\n        {\n          \"success\" => true, \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\" => { \"operation\" => \"text.generate\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\" => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"result\" => \"Hello world.\"\n        }"}
{"id":"lam:43d77a91520e8fa2","kind":"lambda","name":"input_fields","fqname":"action:find_neighbors#input_fields","loc":{"line":684,"column":20,"length":123,"begin":31552,"end":31675},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.find_neighbors', true)"}
{"id":"lam:e00073fb58a620d5","kind":"lambda","name":"execute","fqname":"action:find_neighbors#execute","loc":{"line":692,"column":15,"length":166,"begin":31900,"end":32066},"file":"connector.rb","text":" execute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        call('execute_behavior', connection, 'vector.find_neighbors', call('deep_copy', input))"}
{"id":"lam:d12e67a1b4a9746a","kind":"lambda","name":"output_fields","fqname":"action:find_neighbors#output_fields","loc":{"line":688,"column":21,"length":154,"begin":31713,"end":31867},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.find_neighbors')"}
{"id":"lam:796b2b96f318b276","kind":"lambda","name":"input_fields","fqname":"action:read_index_datapoints#input_fields","loc":{"line":713,"column":20,"length":128,"begin":32805,"end":32933},"file":"connector.rb","text":"put_fields: lambda do |_obj_defs, _connection, cfg|\n        call('get_behavior_input_fields', 'vector.read_datapoints', true, cf"}
{"id":"lam:42e1de3a9b43f8a9","kind":"lambda","name":"execute","fqname":"action:read_index_datapoints#execute","loc":{"line":721,"column":15,"length":334,"begin":33159,"end":33493},"file":"connector.rb","text":"   execute: lambda do |connection, input, _in_schema, _out_schema, cfg|\n        safe = call('deep_copy', input)\n        # Pass the chosen mode to the behavior without mutating the original input\n        safe['id_source'] = cfg['id_source'] if cfg['id_source']\n        call('execute_behavior', connection, 'vector.read_datapoints', saf"}
{"id":"lam:d8a36e98861e9f8a","kind":"lambda","name":"output_fields","fqname":"action:read_index_datapoints#output_fields","loc":{"line":717,"column":21,"length":155,"begin":32971,"end":33126},"file":"connector.rb","text":"put_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.read_datapoints"}
{"id":"lam:c56c05129794ec12","kind":"lambda","name":"sample_output","fqname":"action:read_index_datapoints#sample_output","loc":{"line":728,"column":21,"length":443,"begin":33535,"end":33978},"file":"connector.rb","text":"ple_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"vector.read_datapoints\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 12, \"attempt\" => 1 },\n          \"datapoints\"=> [\n            { \"datapoint_id\" => \"dp_000001\", \"feature_vector\" => [0.01, 0.02, 0.03] }\n          ]\n       "}
{"id":"lam:0ae8462a1409d0fa","kind":"lambda","name":"input_fields","fqname":"action:upsert_index_datapoints#input_fields","loc":{"line":744,"column":20,"length":126,"begin":34179,"end":34305},"file":"connector.rb","text":"put_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.upsert_datapoints', tru"}
{"id":"lam:41ff33861bbc0e98","kind":"lambda","name":"execute","fqname":"action:upsert_index_datapoints#execute","loc":{"line":752,"column":15,"length":169,"begin":34533,"end":34702},"file":"connector.rb","text":"   execute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        call('execute_behavior', connection, 'vector.upsert_datapoints', call('deep_copy', input"}
{"id":"lam:90306b209fc0b9cf","kind":"lambda","name":"output_fields","fqname":"action:upsert_index_datapoints#output_fields","loc":{"line":748,"column":21,"length":157,"begin":34343,"end":34500},"file":"connector.rb","text":"put_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.upsert_datapoints"}
{"id":"lam:fd80c3ec58436af6","kind":"lambda","name":"input_fields","fqname":"action:generate_embeddings#input_fields","loc":{"line":785,"column":20,"length":205,"begin":36307,"end":36512},"file":"connector.rb","text":"put_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.embed', cfg['advanced_config'], cf"}
{"id":"lam:ff56518dab58f2f0","kind":"lambda","name":"execute","fqname":"action:generate_embeddings#execute","loc":{"line":800,"column":15,"length":301,"begin":36950,"end":37251},"file":"connector.rb","text":"   execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input)\n        call('execute_behavior', connection, 'text.embed', safe, user_cf"}
{"id":"lam:7ec1387d73fdfa81","kind":"lambda","name":"output_fields","fqname":"action:generate_embeddings#output_fields","loc":{"line":790,"column":21,"length":367,"begin":36550,"end":36917},"file":"connector.rb","text":"put_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'embeddings', type: 'array', of: 'array' },\n          { name: 'vectors', type: 'array', of: 'object', properties: [\n            { name: 'feature_vector', type: 'array', of: 'number' }\n          ]},\n          { name: 'count', type: 'integer' }\n       "}
