{"id":"n_f3bcb34adb8926a4","kind":"connector","name":"Vertex AI","fqname":"connector.Vertex AI","loc":{"line":1,"column":0,"length":148847,"begin":0,"end":148847},"file":"connector.rb","keys":["actions","connection","custom_action","custom_action_help","methods","object_definitions","pick_lists","test","title","triggers","version"],"http":{},"text":"{\n  title: 'Vertex AI',\n  version: '0.7.4',\n  \n  # ============================================================\n  # CONNECTION & AUTHENTICATION\n  # ============================================================\n  connection: {\n    fields: [\n      # Authentication type\n      { name: 'auth_type', label: 'Authentication type', group: 'Authentication', control_type: 'select', default: 'custom',\n        optional: false, extends_schema: true, hint: 'Select the authentication type for connecting to Google Vertex AI.',\n        options: [ ['Service account (JWT)', 'custom'], ['OAuth 2.0 (Auth code)', 'oauth2'] ]},\n      # Google Cloud Configuration\n      { name: 'project', label: 'Project ID', group: 'Google Cloud Platform', optional: false },\n      { name: 'region',  label: 'Region',     group: 'Google Cloud Platform', optional: false, control_type: 'select', \n        options: [\n          ['Global', 'global'],\n          ['US central 1', 'us-central1'],\n          ['US east 1', 'us-east1'],\n          ['US east 4', 'us-east4'],\n          ['US east 5', 'us-east5'],\n          ['US west 1', 'us-west1'],\n          ['US west 4', 'us-west4'],\n          ['US south 1', 'us-south1'],\n        ],\n        hint: 'Vertex AI region for model execution.', toggle_hint: 'Select from list',\n        toggle_field: {\n          name: 'region', label: 'Region', type: 'string', control_type: 'text', optional: false,\n          toggle_hint: 'Use custom value', hint: \"See Vertex AI locations docs for allowed regions.\" } },\n      { name: 'version', label: 'API version', group: 'Google Cloud Platform', optional: false, default: 'v1', hint: 'e.g. v1beta1' },\n      \n      # Optional Configurations\n      { name: 'vector_search_endpoint', label: 'Vector Search Endpoint', optional: true, hint: 'Public Vector Search domain host for queries' },\n      \n      # Default Behaviors\n      { name: 'default_model', label: 'Default Model', control_type: 'select', optional: true,\n        options: [\n          ['Gemini 1.5 Flash', 'gemini-1.5-flash'],\n          ['Gemini 1.5 Pro',   'gemini-1.5-pro'],\n          ['Text Embedding 004', 'text-embedding-004'],\n          ['Text Embedding Gecko', 'textembedding-gecko']\n        ] },\n      { name: 'optimization_mode', label: 'Optimization Mode', control_type: 'select', default: 'balanced',\n        options: [['Balanced', 'balanced'], ['Cost', 'cost'], ['Performance', 'performance']] },\n      { name: 'enable_caching', label: 'Enable Response Caching', control_type: 'checkbox', default: true },\n      { name: 'enable_logging', label: 'Enable Debug Logging', control_type: 'checkbox', default: false },\n      # Allow admin discovery\n      { name: 'allow_admin_discovery', label: 'Allow admin discovery of index config', group: 'Advanced', control_type: 'checkbox', default: false,\n        hint: 'When enabled, the connector may read Index/IndexEndpoint metadata to compute confidence.' }\n    ],\n    \n    authorization: {\n      type: 'multi',\n      selected: lambda do |connection|\n        connection['auth_type'] || 'custom'\n      end,\n      identity: lambda do |connection|\n        selected = connection['auth_type'] || 'custom'\n        if selected == 'oauth2'\n          begin\n            info = call('http_request',\n              connection,\n              method: 'GET',\n              url: 'https://openidconnect.googleapis.com/v1/userinfo',\n              headers: {}, # Authorization comes from apply()\n              retry_config: { max_attempts: 2, backoff: 0.5, retry_on: [429,500,502,503,504] }\n            )\n            email = info['email'] || '(no email)'\n            name  = info['name']\n            sub   = info['sub']\n            [name, email, sub].compact.join(' / ')\n          rescue\n            'OAuth2 (Google) â€“ identity unavailable'\n          end\n        else\n          connection['service_account_email']\n        end\n      end,\n      options: {\n        oauth2: {\n          type: 'oauth2',\n          fields: [\n            { name: 'client_id', label: 'Client ID', group: 'OAuth 2.0', optional: false },\n            { name: 'client_secret', label: 'Client Secret', group: 'OAuth 2.0', optional: false, control_type: 'password' },\n            { name: 'oauth_refresh_token_ttl', label: 'Refresh token TTL (seconds)', group: 'OAuth 2.0', type: 'integer', optional: true,\n              hint: 'Used only if Google does not return refresh_token_expires_in; enables background refresh.' }\n          ],\n          # AUTH URL\n          authorization_url: lambda do |connection|\n            scopes = [\n              'https://www.googleapis.com/auth/cloud-platform',\n              'openid', 'email', 'profile' # needed for /userinfo claims\n            ].join(' ')\n\n            params = {\n              client_id: connection['client_id'],\n              response_type: 'code',\n              scope: scopes,\n              access_type: 'offline',\n              include_granted_scopes: 'true',\n              prompt: 'consent'\n            }\n\n            qs = call('to_query', params)\n            \"https://accounts.google.com/o/oauth2/v2/auth?#{qs}\"\n          end,\n          # ACQUIRE\n          acquire: lambda do |connection, auth_code|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'authorization_code',\n                code: auth_code,\n                redirect_uri: 'https://www.workato.com/oauth/callback'\n              },\n              headers: { },                 # no X-Goog-User-Project on token exchange\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            body = resp # JSON Hash\n            ttl = body['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n\n            [\n              {\n                access_token: body['access_token'],\n                refresh_token: body['refresh_token'],\n                refresh_token_expires_in: ttl\n              },\n              nil,\n              {}\n            ]\n          end,\n\n          # REFRESH\n          refresh: lambda do |connection, refresh_token|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'refresh_token',\n                refresh_token: refresh_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            {\n              access_token: resp['access_token'],\n              refresh_token: resp['refresh_token'],\n              refresh_token_expires_in: resp['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n            }.compact\n          end,\n\n          # APPLY\n          apply: lambda do |_connection, access_token|\n            headers(Authorization: \"Bearer #{access_token}\")\n          end\n        },\n        custom: {\n          type: 'custom_auth',\n          fields: [\n            { name: 'service_account_email', label: 'Service Account Email', group: 'Service Account', optional: false },\n            { name: 'client_id', label: 'Client ID', group: 'Service Account', optional: false },\n            { name: 'private_key_id', label: 'Private Key ID', group: 'Service Account', optional: false },\n            { name: 'private_key', label: 'Private Key', group: 'Service Account', optional: false, multiline: true, control_type: 'password' }\n          ],\n          acquire: lambda do |connection|\n            issued_at = Time.now.to_i\n            jwt_body_claim = {\n              'iat' => issued_at,\n              'exp' => issued_at + 3600,\n              'aud' => 'https://oauth2.googleapis.com/token',\n              'iss' => connection['service_account_email'],\n              'scope' => 'https://www.googleapis.com/auth/cloud-platform'\n            }\n            private_key = connection['private_key'].to_s.gsub('\\\\n', \"\\n\")\n            jwt_token   = workato.jwt_encode(jwt_body_claim, private_key, 'RS256', kid: connection['private_key_id'])\n\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n                assertion: jwt_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            { access_token: resp['access_token'], expires_at: (Time.now + resp['expires_in'].to_i).iso8601 }\n          end,\n          refresh_on: [401],\n          apply: lambda do |connection|\n            headers(Authorization: \"Bearer #{connection['access_token']}\")\n          end\n        }\n      }\n    },\n    \n    base_uri: lambda do |connection|\n      ver = connection['version']\n      reg = connection['region']\n\n      version = (ver && !ver.to_s.strip.empty?) ? ver.to_s : 'v1'\n      region = (reg && !reg.to_s.strip.empty?) ? reg.to_s : 'us-east4'\n\n      host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n      \"https://#{host}/#{version}/\"\n    end\n  },\n  \n  test: lambda do |connection|\n    project = connection['project']\n    region  = connection['region']\n\n    # 1) Token + API enablement (global catalog)\n    call('list_publisher_models', connection) # raises normalized errors\n\n    # 2) Regional reachability / permissions\n    host = (region.to_s == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n    url = call('build_endpoint_url', connection,\n      { 'custom_path' => \"https://#{host}/#{connection['version']}/projects/{project}/locations/{region}/endpoints\" },\n      {}\n    )\n    call('http_request', connection, method: 'GET', url: url, headers: call('build_headers', connection))\n\n    true\n  rescue => e\n    # Keep the normalized, compact message asâ€‘is\n    error(e.message)\n  end,\n\n  # ============================================================\n  # ACTIONS\n  # ============================================================\n  # Listed alphabetically within each subsection.\n  actions: {\n\n    # ------ UNIVERSAL ACTIONS ----------------------------------\n    # Batch Operation\n    batch_operation: {\n      title: 'UNIVERSAL - Batch AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'batchable_behaviors', optional: false },\n        { name: 'batch_strategy', label: 'Batch Strategy', control_type: 'select', default: 'count', options: [['By Count', 'count'], ['By Token Limit', 'tokens']] },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens â‰ˆ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n      end,\n      # SAMPLE\n      sample_output: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n      end\n    },\n    # Vertex Operation\n    vertex_operation: {\n      title: 'UNIVERSAL - Vertex AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'available_behaviors', optional: false, extends_schema: true,\n          hint: 'Select the AI operation to perform' },\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select',\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          default: 'auto', optional: false, sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.'\n        },\n        # @note PATCH 2025-10-01-D removed ngIf, field behavior is configuration driven\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', sticky: true, \n          optional: true, extends_schema: true, pick_list: 'models_dynamic_for_behavior',  pick_list_params: { behavior: 'behavior' },\n          toggle_hint: 'Select from list', toggle_field: {\n            name: 'model', label: 'Model (custom id)',\n            type: 'string', control_type: 'text',\n            optional: true, toggle_hint: 'Provide custom value' }"}
{"id":"n_4bce2f0180140b0e","kind":"connection","name":"connection","fqname":"connector.Vertex AI/connection.connection","loc":{"line":8,"column":14,"length":9433,"begin":223,"end":9656},"file":null,"keys":[],"http":{},"text":"{\n    fields: [\n      # Authentication type\n      { name: 'auth_type', label: 'Authentication type', group: 'Authentication', control_type: 'select', default: 'custom',\n        optional: false, extends_schema: true, hint: 'Select the authentication type for connecting to Google Vertex AI.',\n        options: [ ['Service account (JWT)', 'custom'], ['OAuth 2.0 (Auth code)', 'oauth2'] ]},\n      # Google Cloud Configuration\n      { name: 'project', label: 'Project ID', group: 'Google Cloud Platform', optional: false },\n      { name: 'region',  label: 'Region',     group: 'Google Cloud Platform', optional: false, control_type: 'select', \n        options: [\n          ['Global', 'global'],\n          ['US central 1', 'us-central1'],\n          ['US east 1', 'us-east1'],\n          ['US east 4', 'us-east4'],\n          ['US east 5', 'us-east5'],\n          ['US west 1', 'us-west1'],\n          ['US west 4', 'us-west4'],\n          ['US south 1', 'us-south1'],\n        ],\n        hint: 'Vertex AI region for model execution.', toggle_hint: 'Select from list',\n        toggle_field: {\n          name: 'region', label: 'Region', type: 'string', control_type: 'text', optional: false,\n          toggle_hint: 'Use custom value', hint: \"See Vertex AI locations docs for allowed regions.\" } },\n      { name: 'version', label: 'API version', group: 'Google Cloud Platform', optional: false, default: 'v1', hint: 'e.g. v1beta1' },\n      \n      # Optional Configurations\n      { name: 'vector_search_endpoint', label: 'Vector Search Endpoint', optional: true, hint: 'Public Vector Search domain host for queries' },\n      \n      # Default Behaviors\n      { name: 'default_model', label: 'Default Model', control_type: 'select', optional: true,\n        options: [\n          ['Gemini 1.5 Flash', 'gemini-1.5-flash'],\n          ['Gemini 1.5 Pro',   'gemini-1.5-pro'],\n          ['Text Embedding 004', 'text-embedding-004'],\n          ['Text Embedding Gecko', 'textembedding-gecko']\n        ] },\n      { name: 'optimization_mode', label: 'Optimization Mode', control_type: 'select', default: 'balanced',\n        options: [['Balanced', 'balanced'], ['Cost', 'cost'], ['Performance', 'performance']] },\n      { name: 'enable_caching', label: 'Enable Response Caching', control_type: 'checkbox', default: true },\n      { name: 'enable_logging', label: 'Enable Debug Logging', control_type: 'checkbox', default: false },\n      # Allow admin discovery\n      { name: 'allow_admin_discovery', label: 'Allow admin discovery of index config', group: 'Advanced', control_type: 'checkbox', default: false,\n        hint: 'When enabled, the connector may read Index/IndexEndpoint metadata to compute confidence.' }\n    ],\n    \n    authorization: {\n      type: 'multi',\n      selected: lambda do |connection|\n        connection['auth_type'] || 'custom'\n      end,\n      identity: lambda do |connection|\n        selected = connection['auth_type'] || 'custom'\n        if selected == 'oauth2'\n          begin\n            info = call('http_request',\n              connection,\n              method: 'GET',\n              url: 'https://openidconnect.googleapis.com/v1/userinfo',\n              headers: {}, # Authorization comes from apply()\n              retry_config: { max_attempts: 2, backoff: 0.5, retry_on: [429,500,502,503,504] }\n            )\n            email = info['email'] || '(no email)'\n            name  = info['name']\n            sub   = info['sub']\n            [name, email, sub].compact.join(' / ')\n          rescue\n            'OAuth2 (Google) â€“ identity unavailable'\n          end\n        else\n          connection['service_account_email']\n        end\n      end,\n      options: {\n        oauth2: {\n          type: 'oauth2',\n          fields: [\n            { name: 'client_id', label: 'Client ID', group: 'OAuth 2.0', optional: false },\n            { name: 'client_secret', label: 'Client Secret', group: 'OAuth 2.0', optional: false, control_type: 'password' },\n            { name: 'oauth_refresh_token_ttl', label: 'Refresh token TTL (seconds)', group: 'OAuth 2.0', type: 'integer', optional: true,\n              hint: 'Used only if Google does not return refresh_token_expires_in; enables background refresh.' }\n          ],\n          # AUTH URL\n          authorization_url: lambda do |connection|\n            scopes = [\n              'https://www.googleapis.com/auth/cloud-platform',\n              'openid', 'email', 'profile' # needed for /userinfo claims\n            ].join(' ')\n\n            params = {\n              client_id: connection['client_id'],\n              response_type: 'code',\n              scope: scopes,\n              access_type: 'offline',\n              include_granted_scopes: 'true',\n              prompt: 'consent'\n            }\n\n            qs = call('to_query', params)\n            \"https://accounts.google.com/o/oauth2/v2/auth?#{qs}\"\n          end,\n          # ACQUIRE\n          acquire: lambda do |connection, auth_code|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'authorization_code',\n                code: auth_code,\n                redirect_uri: 'https://www.workato.com/oauth/callback'\n              },\n              headers: { },                 # no X-Goog-User-Project on token exchange\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            body = resp # JSON Hash\n            ttl = body['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n\n            [\n              {\n                access_token: body['access_token'],\n                refresh_token: body['refresh_token'],\n                refresh_token_expires_in: ttl\n              },\n              nil,\n              {}\n            ]\n          end,\n\n          # REFRESH\n          refresh: lambda do |connection, refresh_token|\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                client_id: connection['client_id'],\n                client_secret: connection['client_secret'],\n                grant_type: 'refresh_token',\n                refresh_token: refresh_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            {\n              access_token: resp['access_token'],\n              refresh_token: resp['refresh_token'],\n              refresh_token_expires_in: resp['refresh_token_expires_in'] || connection['oauth_refresh_token_ttl']\n            }.compact\n          end,\n\n          # APPLY\n          apply: lambda do |_connection, access_token|\n            headers(Authorization: \"Bearer #{access_token}\")\n          end\n        },\n        custom: {\n          type: 'custom_auth',\n          fields: [\n            { name: 'service_account_email', label: 'Service Account Email', group: 'Service Account', optional: false },\n            { name: 'client_id', label: 'Client ID', group: 'Service Account', optional: false },\n            { name: 'private_key_id', label: 'Private Key ID', group: 'Service Account', optional: false },\n            { name: 'private_key', label: 'Private Key', group: 'Service Account', optional: false, multiline: true, control_type: 'password' }\n          ],\n          acquire: lambda do |connection|\n            issued_at = Time.now.to_i\n            jwt_body_claim = {\n              'iat' => issued_at,\n              'exp' => issued_at + 3600,\n              'aud' => 'https://oauth2.googleapis.com/token',\n              'iss' => connection['service_account_email'],\n              'scope' => 'https://www.googleapis.com/auth/cloud-platform'\n            }\n            private_key = connection['private_key'].to_s.gsub('\\\\n', \"\\n\")\n            jwt_token   = workato.jwt_encode(jwt_body_claim, private_key, 'RS256', kid: connection['private_key_id'])\n\n            resp = call('http_request',\n              connection,\n              method: 'POST',\n              url: 'https://oauth2.googleapis.com/token',\n              payload: {\n                grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n                assertion: jwt_token\n              },\n              headers: {},\n              request_format: 'form',\n              retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429,500,502,503,504] }\n            )\n\n            { access_token: resp['access_token'], expires_at: (Time.now + resp['expires_in'].to_i).iso8601 }\n          end,\n          refresh_on: [401],\n          apply: lambda do |connection|\n            headers(Authorization: \"Bearer #{connection['access_token']}\")\n          end\n        }\n      }\n    },\n    \n    base_uri: lambda do |connection|\n      ver = connection['version']\n      reg = connection['region']\n\n      version = (ver && !ver.to_s.strip.empty?) ? ver.to_s : 'v1'\n      region = (reg && !reg.to_s.strip.empty?) ? reg.to_s : 'us-east4'\n\n      host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n      \"https://#{host}/#{version}/\"\n    end\n "}
{"id":"n_535e73b867d9d963","kind":"test","name":"test","fqname":"connector.Vertex AI/test.test","loc":{"line":224,"column":8,"length":758,"begin":9669,"end":10427},"file":null,"keys":[],"http":{},"text":": lambda do |connection|\n    project = connection['project']\n    region  = connection['region']\n\n    # 1) Token + API enablement (global catalog)\n    call('list_publisher_models', connection) # raises normalized errors\n\n    # 2) Regional reachability / permissions\n    host = (region.to_s == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n    url = call('build_endpoint_url', connection,\n      { 'custom_path' => \"https://#{host}/#{connection['version']}/projects/{project}/locations/{region}/endpoints\" },\n      {}\n    )\n    call('http_request', connection, method: 'GET', url: url, headers: call('build_headers', connection))\n\n    true\n  rescue => e\n    # Keep the normalized, compact message asâ€‘is\n    error(e.message)\n "}
{"id":"n_c1e17c41cf1cfd7d","kind":"methods","name":"methods","fqname":"connector.Vertex AI/methods.methods","loc":{"line":788,"column":11,"length":104068,"begin":37534,"end":141602},"file":null,"keys":[],"http":{},"text":" methods: {\n    # ------ LAYER 1: CORE METHODS (Foundation) ----------------\n    # --- Payload Building\n    build_payload: lambda do |template:, variables:, format:|\n      case format\n      \n      # Direct\n      when 'direct'\n        variables\n      # Template\n      when 'template'\n        result = template.dup\n        variables.each { |k, v| result = result.gsub(\"{#{k}}\", v.to_s) }\n        result\n      # Vertex prompt\n      when 'vertex_prompt'\n        payload = {\n          'contents' => [{\n            'role'  => 'user',\n            'parts' => [{ 'text' => call('apply_template', template, variables) }]\n          }],\n          'generationConfig' => call('build_generation_config', variables)\n        }.compact\n\n        # Variables\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        # @note PATCH 2025-10-01-C post-normalized safety settings \n        sys = variables['system']\n        payload['systemInstruction'] = { 'parts' => [{ 'text' => sys }] } if sys && !sys.to_s.strip.empty?\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        if call('value_present', variables['response_mime_type']) || call('value_present', variables['response_schema'])\n          gc = (payload['generationConfig'] ||= {})\n          gc['responseMimeType'] = variables['response_mime_type'] if call('value_present', variables['response_mime_type'])\n          gc['responseSchema']   = variables['response_schema']     if call('value_present', variables['response_schema'])\n        end\n\n        payload['labels'] = variables['labels'] if call('value_present', variables['labels'])\n        payload\n\n      when 'vertex_contents'\n        contents = Array(variables['contents']).map do |c|\n          role = c['role'] || c[:role] || 'user'\n          parts = Array(c['parts']).map do |p|\n            if p['text'] || p[:text]\n              { 'text' => p['text'] || p[:text] }\n            elsif p['inline_data'] || p[:inline_data] || p['inlineData']\n              src = p['inline_data'] || p[:inline_data] || p['inlineData']\n              { 'inlineData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'data'     => src['data'] || src[:data]\n                }.compact\n              }\n            elsif p['file_data'] || p[:file_data] || p['fileData']\n              src = p['file_data'] || p[:file_data] || p['fileData']\n              { 'fileData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'fileUri'  => src['file_uri']  || src[:file_uri]  || src['fileUri']\n                }.compact\n              }\n            else\n              {} # ignored\n            end\n          end.compact\n\n          { 'role' => role, 'parts' => parts }\n        end\n\n        payload = {\n          'contents'         => contents,\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        # Variables\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        # @note PATCH 2025-10-01-C post-normalized safety settings \n        if call('value_present', variables['system'])\n          payload['systemInstruction'] = { 'parts' => [{ 'text' => variables['system'] }] }\n        end\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        gc = (payload['generationConfig'] ||= {})\n        gc['responseMimeType'] = variables['response_mime_type'] if call('value_present', variables['response_mime_type'])\n        gc['responseSchema']   = variables['response_schema']     if call('value_present', variables['response_schema'])\n\n        payload['labels'] = variables['labels'] if call('value_present', variables['labels'])\n        payload\n      when 'vertex_passthrough'\n        src = variables['payload'] || variables['payload_json'] || variables['fully_formed'] || variables['request_json']\n        obj =\n          if src.is_a?(String)\n            begin\n              JSON.parse(src)\n            rescue\n              corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n              error(\"Invalid payload_json (must be valid JSON object). [corr_id=#{corr}]\")\n            end\n          else\n            src\n          end\n        error('payload_json must be a JSON object') unless obj.is_a?(Hash)\n        obj\n\n      # Embedding\n      when 'embedding'\n        # Normalize and filter\n        texts = Array(variables['texts']).map { |t| t.to_s.strip }.reject(&:empty?)\n        error('No non-empty texts provided') if texts.empty?\n\n        # Enforce model-aware limits\n        model_id = variables['model'].to_s\n        max_per_request =\n          if model_id.start_with?('text-embedding-005') then 100\n          else 100 # safe default for older models as well\n          end\n        if texts.length > max_per_request\n          corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          error(\"Too many texts for a single request (#{texts.length} > #{max_per_request}). Split the batch or use Batch AI Operation. [corr_id=#{corr}]\")\n        end\n\n        # Task/type normalization\n        task_type = variables['task_type'] || 'RETRIEVAL_DOCUMENT'\n        include_title = (task_type == 'RETRIEVAL_DOCUMENT')\n\n        # Build instances\n        body = {\n          'instances' => texts.map { |text|\n            inst = { 'content' => text, 'task_type' => task_type }\n            inst['title'] = variables['title'] if include_title && variables['title']\n            inst\n          }\n        }\n\n        # Parameters\n        params = {}\n        supports_dimensionality = model_id.start_with?('text-embedding-005')\n        supports_auto_truncate  = supports_dimensionality || model_id.start_with?('textembedding-gecko')\n\n        if supports_auto_truncate && call('value_present', variables['auto_truncate'])\n          params['autoTruncate'] = variables['auto_truncate']\n        end\n        if supports_dimensionality && call('value_present', variables['output_dimensionality'])\n          params['outputDimensionality'] = variables['output_dimensionality']\n        end\n        body['parameters'] = params unless params.empty?\n\n        body\n\n      # Vector search\n      when 'find_neighbors'\n        queries = Array(variables['queries']).map do |q|\n          dp =\n            if q['feature_vector']\n              { 'featureVector' => Array(q['feature_vector']).map(&:to_f) }\n            elsif q['vector'] # alias\n              { 'featureVector' => Array(q['vector']).map(&:to_f) }\n            elsif q['datapoint_id']\n              { 'datapointId' => q['datapoint_id'] }\n            else\n              {}\n            end\n\n          {\n            'datapoint'         => dp,\n            'neighborCount'     => (q['neighbor_count'] || variables['neighbor_count'] || 10).to_i,\n            'restricts'         => q['restricts'],\n            'numericRestricts'  => q['numeric_restricts'] # keep input snake_case; map to camel here\n          }.compact\n        end\n\n        {\n          'deployedIndexId'     => variables['deployed_index_id'],\n          'queries'             => queries,\n          'returnFullDatapoint' => variables['return_full_datapoint']\n        }.compact\n\n      when 'upsert_datapoints'\n        datapoints =\n          if Array(variables['datapoints']).any?\n            Array(variables['datapoints']).map do |d|\n              {\n                'datapointId'      => d['datapoint_id'] || d['id'],\n                'featureVector'    => Array(d['feature_vector'] || d['vector']).map(&:to_f),\n                'sparseEmbedding'  => d['sparse_embedding'],\n                'restricts'        => d['restricts'],\n                'numericRestricts' => d['numeric_restricts'],\n                'crowdingTag'      => d['crowding_tag'],\n                'embeddingMetadata'=> d['embedding_metadata']\n              }.compact\n            end\n          elsif Array(variables['embeddings']).any?\n            call('coerce_embeddings_to_datapoints', variables)\n          else\n            []\n          end\n\n        { 'datapoints' => datapoints }\n      when 'read_index_datapoints'\n        ids = call('extract_ids_for_read', variables)\n        { 'deployedIndexId' => variables['deployed_index_id'], 'ids' => ids }\n      # Multimodal\n      when 'multimodal'\n        parts = []\n        parts << { 'text' => variables['text'] } if variables['text']\n        if variables['images']\n          variables['images'].each do |img|\n            parts << { 'inlineData' => { 'mimeType' => img['mime_type'] || 'image/jpeg', 'data' => img['data'] } }\n          end\n        end\n\n        payload = {\n          'contents' => [{ 'role' => 'user', 'parts' => parts }],\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        payload\n        \n      else\n        variables\n      end\n    end,\n    \n    # --- Response Enrichment\n    # @note PATCH 2025-10-01-D expose http trace for debugging purposes\n    enrich_response: lambda do |response:, metadata: {}|\n      base  = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'result' => response }\n      trace = base.delete('_trace') || (base['result'].is_a?(Hash) ? base['result'].delete('_trace') : nil)\n\n      # Preserve success if caller provided it; otherwise assume true\n      success = base.key?('success') ? base['success'] : true\n\n      # Build a uniform trace object (always present)\n      trace_hash  = trace.is_a?(Hash) ? trace : {}\n      final_trace = {\n        'correlation_id'    => trace_hash['correlation_id'] || SecureRandom.hex(8),\n        'duration_ms'       => (trace_hash['duration_ms'] || 0).to_i,\n        'attempt'           => (trace_hash['attempt'] || 1).to_i,\n        'http_status'       => trace_hash['http_status'],\n        'remote_request_id' => trace_hash['remote_request_id'],\n        'rate_limit'        => trace_hash['rate_limit']\n      }.compact\n\n      base.merge(\n        'success'   => success,\n        'timestamp' => base['timestamp'] || Time.now.utc.iso8601,\n        'metadata'  => { 'operation' => metadata['operation'], 'model' => metadata['model'] }.compact,\n        'trace'     => final_trace\n      ).compact\n    end,\n\n    # --- Response Extraction\n    extract_response: lambda do |data:, path: nil, format: 'raw'|\n      case format\n      # RAW\n      when 'raw' then data\n      # JSON_FIELD\n      when 'json_field'\n        return data unless path\n        path.split('.').reduce(data) { |acc, seg| acc.is_a?(Array) && seg =~ /^\\d+$/ ? acc[seg.to_i] : (acc || {})[seg] }\n      # VERTEX_TEXT\n      when 'vertex_text'\n        parts = data.dig('candidates', 0, 'content', 'parts') || []\n        text  = parts.select { |p| p['text'] }.map { |p| p['text'] }.join\n        text.empty? ? data.dig('predictions', 0, 'content').to_s : text\n      \n      # VERTEX_JSON\n      when 'vertex_json'\n        raw = (data.dig('candidates', 0, 'content', 'parts') || []).map { |p| p['text'] }.compact.join\n        return {} if raw.nil? || raw.empty?\n        m = raw.match(/```(?:json)?\\s*(\\{.*?\\})\\s*```/m) || raw.match(/\\{.*\\}/m)\n        m ? (JSON.parse(m[1] || m[0]) rescue {}) : {}\n\n      # EMBEDDINGS  \n      when 'embeddings'\n        # Normalize all known Vertex shapes to an array of Float arrays\n        preds = Array(data['predictions'])\n\n        vectors = preds.map do |p|\n          next p if p.is_a?(Array) && p.all? { |x| x.is_a?(Numeric) } # raw numeric array\n\n          next unless p.is_a?(Hash)\n          v = nil\n\n          emb = p['embeddings'] || p['embedding']\n\n          # Preferred: { \"embeddings\": { \"values\": [...] } }\n          if emb.is_a?(Hash) && emb['values'].is_a?(Array)\n            v = emb['values']\n\n          # Sometimes: { \"embeddings\": [ { \"values\": [...] } ] }\n          elsif emb.is_a?(Array) && emb.first.is_a?(Hash) && emb.first['values'].is_a?(Array)\n            v = emb.first['values']\n\n          # Legacy: { \"embeddings\": [ ...numbers... ] } OR { \"embedding\": [ ...numbers... ] }\n          elsif emb.is_a?(Array) && emb.first.is_a?(Numeric)\n            v = emb\n\n          # Fallbacks occasionally seen in older/experimental endpoints\n          elsif p['denseEmbedding'].is_a?(Array)\n            v = p['denseEmbedding']\n          elsif p['values'].is_a?(Array)\n            v = p['values']\n          end\n\n          # Only accept a clean numeric vector\n          (v.is_a?(Array) && v.all? { |x| x.is_a?(Numeric) }) ? v : nil\n        end.compact\n\n        vectors\n      else data\n      end\n    end,\n\n    # --- HTTP Request Execution\n    http_request: lambda do |connection, method:, url:, payload: nil, headers: {}, retry_config: {}, request_format: 'json'|\n      max_attempts = (retry_config['max_attempts'] || retry_config['max_retries'] || 3).to_i\n      base_backoff = (retry_config['backoff'] || 1.0).to_f\n      retry_on     = Array(retry_config['retry_on'] || [408, 429, 500, 502, 503, 504]).map(&:to_i)\n      do_not_retry = Array(retry_config['do_not_retry']).map(&:to_i)\n\n      attempt = 0\n      last_error = nil\n\n      while attempt < max_attempts\n        attempt += 1\n        begin\n          hdrs = (headers || {}).dup\n          corr = hdrs['X-Correlation-Id'] ||= \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n          last_error = nil\n\n          req = case method.to_s.upcase\n                when 'GET'    then get(url)\n                when 'POST'   then post(url, payload)\n                when 'PUT'    then put(url, payload)\n                when 'DELETE' then delete(url)\n                else error(\"Unsupported HTTP method: #{method}\")\n                end\n\n          # Respect application/x-www-form-urlencoded when requested\n          if request_format.to_s == 'form'\n            hdrs['Content-Type'] ||= 'application/x-www-form-urlencoded'\n            req = req.request_format_www_form_urlencoded\n          end\n\n          response =\n            req.headers(hdrs)\n              .after_error_response(/.*/) { |code, body, rheaders, message|\n                 dur_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n                 err = call('normalize_http_error',\n                            connection,\n                            code: code, body: body, headers: (rheaders || {}),\n                            message: message, url: url, corr_id: corr, attempt: attempt, duration_ms: dur_ms)\n                 last_error = err\n                 error(call('format_user_error', err))\n               }\n              .after_response { |code, body, rheaders|\n                # Always return a Hash payload with HTTP metadata, even when the API returns a raw string/bytes.\n                payload = body.is_a?(Hash) ? body : { 'raw' => body }\n                payload['_http'] = { 'status' => code, 'headers' => rheaders }\n                payload\n              }\n\n          duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n          out = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'raw' => response }\n\n          # Extract a useful Google/Vertex request id when present.\n          hdrs_out = (out.dig('_http', 'headers') || {})\n          rid = hdrs_out['x-request-id'] || hdrs_out['x-cloud-trace-context'] || hdrs_out['x-guploader-uploadid']\n\n          out['_trace'] = {\n            'correlation_id'    => corr,\n            'duration_ms'       => duration_ms,\n            'attempt'           => attempt,\n            'http_status'       => out.dig('_http','status'),\n            'remote_request_id' => rid\n          }.compact\n\n          return out\n\n        rescue => _e\n          code = last_error ? last_error['code'].to_i : 0\n          retryable = last_error ? last_error['retryable'] : retry_on.include?(code)\n          retryable &&= !do_not_retry.include?(code)\n\n          # Break if not retryable or out of attempts\n          break unless retryable && attempt < max_attempts\n\n          # Retry-After (seconds) takes pr"}
{"id":"n_368592796c4c03f6","kind":"method","name":"build_payload","fqname":"connector.Vertex AI/methods.methods/method.build_payload","loc":{"line":791,"column":19,"length":9158,"begin":37647,"end":46805},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"_payload: lambda do |template:, variables:, format:|\n      case format\n      \n      # Direct\n      when 'direct'\n        variables\n      # Template\n      when 'template'\n        result = template.dup\n        variables.each { |k, v| result = result.gsub(\"{#{k}}\", v.to_s) }\n        result\n      # Vertex prompt\n      when 'vertex_prompt'\n        payload = {\n          'contents' => [{\n            'role'  => 'user',\n            'parts' => [{ 'text' => call('apply_template', template, variables) }]\n          }],\n          'generationConfig' => call('build_generation_config', variables)\n        }.compact\n\n        # Variables\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        # @note PATCH 2025-10-01-C post-normalized safety settings \n        sys = variables['system']\n        payload['systemInstruction'] = { 'parts' => [{ 'text' => sys }] } if sys && !sys.to_s.strip.empty?\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        if call('value_present', variables['response_mime_type']) || call('value_present', variables['response_schema'])\n          gc = (payload['generationConfig'] ||= {})\n          gc['responseMimeType'] = variables['response_mime_type'] if call('value_present', variables['response_mime_type'])\n          gc['responseSchema']   = variables['response_schema']     if call('value_present', variables['response_schema'])\n        end\n\n        payload['labels'] = variables['labels'] if call('value_present', variables['labels'])\n        payload\n\n      when 'vertex_contents'\n        contents = Array(variables['contents']).map do |c|\n          role = c['role'] || c[:role] || 'user'\n          parts = Array(c['parts']).map do |p|\n            if p['text'] || p[:text]\n              { 'text' => p['text'] || p[:text] }\n            elsif p['inline_data'] || p[:inline_data] || p['inlineData']\n              src = p['inline_data'] || p[:inline_data] || p['inlineData']\n              { 'inlineData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'data'     => src['data'] || src[:data]\n                }.compact\n              }\n            elsif p['file_data'] || p[:file_data] || p['fileData']\n              src = p['file_data'] || p[:file_data] || p['fileData']\n              { 'fileData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'fileUri'  => src['file_uri']  || src[:file_uri]  || src['fileUri']\n                }.compact\n              }\n            else\n              {} # ignored\n            end\n          end.compact\n\n          { 'role' => role, 'parts' => parts }\n        end\n\n        payload = {\n          'contents'         => contents,\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        # Variables\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        # @note PATCH 2025-10-01-C post-normalized safety settings \n        if call('value_present', variables['system'])\n          payload['systemInstruction'] = { 'parts' => [{ 'text' => variables['system'] }] }\n        end\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        gc = (payload['generationConfig'] ||= {})\n        gc['responseMimeType'] = variables['response_mime_type'] if call('value_present', variables['response_mime_type'])\n        gc['responseSchema']   = variables['response_schema']     if call('value_present', variables['response_schema'])\n\n        payload['labels'] = variables['labels'] if call('value_present', variables['labels'])\n        payload\n      when 'vertex_passthrough'\n        src = variables['payload'] || variables['payload_json'] || variables['fully_formed'] || variables['request_json']\n        obj =\n          if src.is_a?(String)\n            begin\n              JSON.parse(src)\n            rescue\n              corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n              error(\"Invalid payload_json (must be valid JSON object). [corr_id=#{corr}]\")\n            end\n          else\n            src\n          end\n        error('payload_json must be a JSON object') unless obj.is_a?(Hash)\n        obj\n\n      # Embedding\n      when 'embedding'\n        # Normalize and filter\n        texts = Array(variables['texts']).map { |t| t.to_s.strip }.reject(&:empty?)\n        error('No non-empty texts provided') if texts.empty?\n\n        # Enforce model-aware limits\n        model_id = variables['model'].to_s\n        max_per_request =\n          if model_id.start_with?('text-embedding-005') then 100\n          else 100 # safe default for older models as well\n          end\n        if texts.length > max_per_request\n          corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          error(\"Too many texts for a single request (#{texts.length} > #{max_per_request}). Split the batch or use Batch AI Operation. [corr_id=#{corr}]\")\n        end\n\n        # Task/type normalization\n        task_type = variables['task_type'] || 'RETRIEVAL_DOCUMENT'\n        include_title = (task_type == 'RETRIEVAL_DOCUMENT')\n\n        # Build instances\n        body = {\n          'instances' => texts.map { |text|\n            inst = { 'content' => text, 'task_type' => task_type }\n            inst['title'] = variables['title'] if include_title && variables['title']\n            inst\n          }\n        }\n\n        # Parameters\n        params = {}\n        supports_dimensionality = model_id.start_with?('text-embedding-005')\n        supports_auto_truncate  = supports_dimensionality || model_id.start_with?('textembedding-gecko')\n\n        if supports_auto_truncate && call('value_present', variables['auto_truncate'])\n          params['autoTruncate'] = variables['auto_truncate']\n        end\n        if supports_dimensionality && call('value_present', variables['output_dimensionality'])\n          params['outputDimensionality'] = variables['output_dimensionality']\n        end\n        body['parameters'] = params unless params.empty?\n\n        body\n\n      # Vector search\n      when 'find_neighbors'\n        queries = Array(variables['queries']).map do |q|\n          dp =\n            if q['feature_vector']\n              { 'featureVector' => Array(q['feature_vector']).map(&:to_f) }\n            elsif q['vector'] # alias\n              { 'featureVector' => Array(q['vector']).map(&:to_f) }\n            elsif q['datapoint_id']\n              { 'datapointId' => q['datapoint_id'] }\n            else\n              {}\n            end\n\n          {\n            'datapoint'         => dp,\n            'neighborCount'     => (q['neighbor_count'] || variables['neighbor_count'] || 10).to_i,\n            'restricts'         => q['restricts'],\n            'numericRestricts'  => q['numeric_restricts'] # keep input snake_case; map to camel here\n          }.compact\n        end\n\n        {\n          'deployedIndexId'     => variables['deployed_index_id'],\n          'queries'             => queries,\n          'returnFullDatapoint' => variables['return_full_datapoint']\n        }.compact\n\n      when 'upsert_datapoints'\n        datapoints =\n          if Array(variables['datapoints']).any?\n            Array(variables['datapoints']).map do |d|\n              {\n                'datapointId'      => d['datapoint_id'] || d['id'],\n                'featureVector'    => Array(d['feature_vector'] || d['vector']).map(&:to_f),\n                'sparseEmbedding'  => d['sparse_embedding'],\n                'restricts'        => d['restricts'],\n                'numericRestricts' => d['numeric_restricts'],\n                'crowdingTag'      => d['crowding_tag'],\n                'embeddingMetadata'=> d['embedding_metadata']\n              }.compact\n            end\n          elsif Array(variables['embeddings']).any?\n            call('coerce_embeddings_to_datapoints', variables)\n          else\n            []\n          end\n\n        { 'datapoints' => datapoints }\n      when 'read_index_datapoints'\n        ids = call('extract_ids_for_read', variables)\n        { 'deployedIndexId' => variables['deployed_index_id'], 'ids' => ids }\n      # Multimodal\n      when 'multimodal'\n        parts = []\n        parts << { 'text' => variables['text'] } if variables['text']\n        if variables['images']\n          variables['images'].each do |img|\n            parts << { 'inlineData' => { 'mimeType' => img['mime_type'] || 'image/jpeg', 'data' => img['data'] } }\n          end\n        end\n\n        payload = {\n          'contents' => [{ 'role' => 'user', 'parts' => parts }],\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        payload\n        \n      else\n        variables\n      e"}
{"id":"n_f1205e4be60792f8","kind":"method","name":"enrich_response","fqname":"connector.Vertex AI/methods.methods/method.enrich_response","loc":{"line":1019,"column":21,"length":1193,"begin":46935,"end":48128},"file":null,"keys":[],"http":{"verbs":["DELETE"],"endpoints":["DELETE _trace"]},"text":"response: lambda do |response:, metadata: {}|\n      base  = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'result' => response }\n      trace = base.delete('_trace') || (base['result'].is_a?(Hash) ? base['result'].delete('_trace') : nil)\n\n      # Preserve success if caller provided it; otherwise assume true\n      success = base.key?('success') ? base['success'] : true\n\n      # Build a uniform trace object (always present)\n      trace_hash  = trace.is_a?(Hash) ? trace : {}\n      final_trace = {\n        'correlation_id'    => trace_hash['correlation_id'] || SecureRandom.hex(8),\n        'duration_ms'       => (trace_hash['duration_ms'] || 0).to_i,\n        'attempt'           => (trace_hash['attempt'] || 1).to_i,\n        'http_status'       => trace_hash['http_status'],\n        'remote_request_id' => trace_hash['remote_request_id'],\n        'rate_limit'        => trace_hash['rate_limit']\n      }.compact\n\n      base.merge(\n        'success'   => success,\n        'timestamp' => base['timestamp'] || Time.now.utc.iso8601,\n        'metadata'  => { 'operation' => metadata['operation'], 'model' => metadata['model'] }.compact,\n        'trace'     => final_trace\n      ).compa"}
{"id":"n_4d048a6e9a632a44","kind":"method","name":"extract_response","fqname":"connector.Vertex AI/methods.methods/method.extract_response","loc":{"line":1046,"column":22,"length":2238,"begin":48183,"end":50421},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"response: lambda do |data:, path: nil, format: 'raw'|\n      case format\n      # RAW\n      when 'raw' then data\n      # JSON_FIELD\n      when 'json_field'\n        return data unless path\n        path.split('.').reduce(data) { |acc, seg| acc.is_a?(Array) && seg =~ /^\\d+$/ ? acc[seg.to_i] : (acc || {})[seg] }\n      # VERTEX_TEXT\n      when 'vertex_text'\n        parts = data.dig('candidates', 0, 'content', 'parts') || []\n        text  = parts.select { |p| p['text'] }.map { |p| p['text'] }.join\n        text.empty? ? data.dig('predictions', 0, 'content').to_s : text\n      \n      # VERTEX_JSON\n      when 'vertex_json'\n        raw = (data.dig('candidates', 0, 'content', 'parts') || []).map { |p| p['text'] }.compact.join\n        return {} if raw.nil? || raw.empty?\n        m = raw.match(/```(?:json)?\\s*(\\{.*?\\})\\s*```/m) || raw.match(/\\{.*\\}/m)\n        m ? (JSON.parse(m[1] || m[0]) rescue {}) : {}\n\n      # EMBEDDINGS  \n      when 'embeddings'\n        # Normalize all known Vertex shapes to an array of Float arrays\n        preds = Array(data['predictions'])\n\n        vectors = preds.map do |p|\n          next p if p.is_a?(Array) && p.all? { |x| x.is_a?(Numeric) } # raw numeric array\n\n          next unless p.is_a?(Hash)\n          v = nil\n\n          emb = p['embeddings'] || p['embedding']\n\n          # Preferred: { \"embeddings\": { \"values\": [...] } }\n          if emb.is_a?(Hash) && emb['values'].is_a?(Array)\n            v = emb['values']\n\n          # Sometimes: { \"embeddings\": [ { \"values\": [...] } ] }\n          elsif emb.is_a?(Array) && emb.first.is_a?(Hash) && emb.first['values'].is_a?(Array)\n            v = emb.first['values']\n\n          # Legacy: { \"embeddings\": [ ...numbers... ] } OR { \"embedding\": [ ...numbers... ] }\n          elsif emb.is_a?(Array) && emb.first.is_a?(Numeric)\n            v = emb\n\n          # Fallbacks occasionally seen in older/experimental endpoints\n          elsif p['denseEmbedding'].is_a?(Array)\n            v = p['denseEmbedding']\n          elsif p['values'].is_a?(Array)\n            v = p['values']\n          end\n\n          # Only accept a clean numeric vector\n          (v.is_a?(Array) && v.all? { |x| x.is_a?(Numeric) }) ? v : nil\n        end.compact\n\n        vectors\n      else data\n      e"}
{"id":"n_88da4b7527155a27","kind":"method","name":"http_request","fqname":"connector.Vertex AI/methods.methods/method.http_request","loc":{"line":1109,"column":18,"length":3959,"begin":50475,"end":54434},"file":null,"keys":[],"http":{"verbs":["GET","POST","PUT","DELETE"],"endpoints":["GET (dynamic)","POST (dynamic)","PUT (dynamic)","DELETE (dynamic)"]},"text":"_request: lambda do |connection, method:, url:, payload: nil, headers: {}, retry_config: {}, request_format: 'json'|\n      max_attempts = (retry_config['max_attempts'] || retry_config['max_retries'] || 3).to_i\n      base_backoff = (retry_config['backoff'] || 1.0).to_f\n      retry_on     = Array(retry_config['retry_on'] || [408, 429, 500, 502, 503, 504]).map(&:to_i)\n      do_not_retry = Array(retry_config['do_not_retry']).map(&:to_i)\n\n      attempt = 0\n      last_error = nil\n\n      while attempt < max_attempts\n        attempt += 1\n        begin\n          hdrs = (headers || {}).dup\n          corr = hdrs['X-Correlation-Id'] ||= \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n          last_error = nil\n\n          req = case method.to_s.upcase\n                when 'GET'    then get(url)\n                when 'POST'   then post(url, payload)\n                when 'PUT'    then put(url, payload)\n                when 'DELETE' then delete(url)\n                else error(\"Unsupported HTTP method: #{method}\")\n                end\n\n          # Respect application/x-www-form-urlencoded when requested\n          if request_format.to_s == 'form'\n            hdrs['Content-Type'] ||= 'application/x-www-form-urlencoded'\n            req = req.request_format_www_form_urlencoded\n          end\n\n          response =\n            req.headers(hdrs)\n              .after_error_response(/.*/) { |code, body, rheaders, message|\n                 dur_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n                 err = call('normalize_http_error',\n                            connection,\n                            code: code, body: body, headers: (rheaders || {}),\n                            message: message, url: url, corr_id: corr, attempt: attempt, duration_ms: dur_ms)\n                 last_error = err\n                 error(call('format_user_error', err))\n               }\n              .after_response { |code, body, rheaders|\n                # Always return a Hash payload with HTTP metadata, even when the API returns a raw string/bytes.\n                payload = body.is_a?(Hash) ? body : { 'raw' => body }\n                payload['_http'] = { 'status' => code, 'headers' => rheaders }\n                payload\n              }\n\n          duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n          out = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'raw' => response }\n\n          # Extract a useful Google/Vertex request id when present.\n          hdrs_out = (out.dig('_http', 'headers') || {})\n          rid = hdrs_out['x-request-id'] || hdrs_out['x-cloud-trace-context'] || hdrs_out['x-guploader-uploadid']\n\n          out['_trace'] = {\n            'correlation_id'    => corr,\n            'duration_ms'       => duration_ms,\n            'attempt'           => attempt,\n            'http_status'       => out.dig('_http','status'),\n            'remote_request_id' => rid\n          }.compact\n\n          return out\n\n        rescue => _e\n          code = last_error ? last_error['code'].to_i : 0\n          retryable = last_error ? last_error['retryable'] : retry_on.include?(code)\n          retryable &&= !do_not_retry.include?(code)\n\n          # Break if not retryable or out of attempts\n          break unless retryable && attempt < max_attempts\n\n          # Retry-After (seconds) takes precedence when present\n          delay =\n            if last_error && last_error['retry_after_s'].to_i > 0\n              last_error['retry_after_s'].to_i\n            else\n              # exp backoff with small jitter\n              (base_backoff * (2 ** (attempt - 1))).to_f + rand * 0.25\n            end\n\n          sleep(delay)\n        end\n      end\n\n      # Exhausted: bubble the last normalized message if present\n      msg = last_error ? call('format_user_error', last_error) : 'HTTP request failed'\n      error(ms"}
{"id":"n_b5f3f5bb33a4ffa4","kind":"method","name":"transform_data","fqname":"connector.Vertex AI/methods.methods/method.transform_data","loc":{"line":1202,"column":20,"length":1056,"begin":54487,"end":55543},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"orm_data: lambda do |input:, from_format:, to_format:, connection: nil|\n      case \"#{from_format}_to_#{to_format}\"\n      when 'url_to_base64'\n        # Use centralized http_request for retries and telemetry\n        resp = call('http_request', connection, method: 'GET', url: input, headers: {})\n        raw  = resp['raw'] || resp.to_s\n        require 'base64'\n        Base64.strict_encode64(raw.to_s)\n      when 'base64_to_bytes'\n        require 'base64'\n        Base64.decode64(input.to_s)\n      when 'language_code_to_name'\n        names = {\n          'en'=>'English','es'=>'Spanish','fr'=>'French','de'=>'German','it'=>'Italian','pt'=>'Portuguese',\n          'ja'=>'Japanese','ko'=>'Korean','zh-CN'=>'Chinese (Simplified)','zh-TW'=>'Chinese (Traditional)'\n        }\n        return 'auto-detected' if input == 'auto'\n        names[input] || input\n      when 'categories_to_text'\n        input.map { |c| \"#{c['name']}: #{c['description']}\" }.join(\"\\n\")\n      when 'distance_to_similarity'\n        1.0 - (input.to_f / 2.0)\n      else\n        input\n      e"}
{"id":"n_c4a333c702c12a63","kind":"method","name":"validate_input","fqname":"connector.Vertex AI/methods.methods/method.validate_input","loc":{"line":1230,"column":20,"length":4327,"begin":55597,"end":59924},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"te_input: lambda do |data:, schema: [], constraints: []|\n      errors = []\n      \n      # Schema validation\n      schema.each do |field|\n        field_name = field['name']\n        field_value = data[field_name]\n        \n        # Required check\n        # @note PATCH 2025-10-01-C updated to treat [] {} as missing for required fields\n        if field['required'] && !call('value_present', field_value)\n          errors << \"#{field_name} is required\"\n        end\n        \n        # Length validation\n        if field['max_length'] && field_value.to_s.length > field['max_length']\n          errors << \"#{field_name} exceeds maximum length of #{field['max_length']}\"\n        end\n        \n        # Pattern validation\n        if field['pattern'] && field_value && !field_value.match?(Regexp.new(field['pattern']))\n          errors << \"#{field_name} format is invalid\"\n        end\n      end\n      \n      # Constraint validation\n      constraints.each do |constraint|\n        ctype = (constraint['type'] || constraint[:type]).to_s\n\n        case ctype\n        when 'min_value'\n          value = data[(constraint['field'] || constraint[:field]).to_s].to_f\n          if value < constraint['value'].to_f\n            errors << \"#{constraint['field'] || constraint[:field]} must be at least #{constraint['value']}\"\n          end\n\n        when 'max_items'\n          field = (constraint['field'] || constraint[:field]).to_s\n          items = data[field] || []\n          if Array(items).size > constraint['value'].to_i\n            errors << \"#{field} cannot exceed #{constraint['value']} items\"\n          end\n\n        # XOR/ONE-OF across fields (root or per-item scope)\n        when 'xor', 'one_of'\n          scope   = (constraint['scope'] || constraint[:scope]).to_s # e.g., 'queries[]' or ''\n          fields  = Array(constraint['fields'] || constraint[:fields]).map(&:to_s)\n          aliases = (constraint['aliases'] || constraint[:aliases] || {}) # { 'feature_vector' => ['vector'] }\n          exactly_one = (ctype == 'xor') || (constraint['exactly_one'] == true)\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            count = 0\n            fields.each do |f|\n              keys = [f] + Array(aliases[f] || aliases[f.to_sym]).map(&:to_s)\n              present = keys.any? { |k| call('value_present', ctx[k]) }\n              count += 1 if present\n            end\n\n            if exactly_one\n              if count != 1\n                display = fields.map { |f|\n                  al = Array(aliases[f] || aliases[f.to_sym])\n                  al.any? ? \"#{f} (alias: #{al.join(', ')})\" : f\n                }.join(', ')\n                errors << \"#{label}: exactly one of #{display} must be provided\"\n              end\n            else\n              if count < 1\n                errors << \"#{label}: at least one of #{fields.join(', ')} must be provided\"\n              end\n            end\n          end\n\n        # Conditional required with root-level fallback and optional default\n        # Example: each queries[].neighbor_count is optional if top-level neighbor_count is present,\n        # or if a default is defined; else required.\n        when 'fallback_required', 'conditional_required'\n          scope    = (constraint['scope'] || constraint[:scope]).to_s       # e.g., 'queries[]'\n          field    = (constraint['field'] || constraint[:field]).to_s       # e.g., 'neighbor_count'\n          fallback = (constraint['fallback_to_root'] || constraint[:fallback_to_root]).to_s # e.g., 'neighbor_count'\n          default_ok = constraint.key?('default_if_absent') || constraint.key?(:default_if_absent)\n\n          root_has_fallback = fallback.empty? ? false : call('value_present', data[fallback])\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            item_has = call('value_present', ctx[field])\n            unless item_has || root_has_fallback || default_ok\n              if fallback.empty?\n                errors << \"#{label}.#{field} is required\"\n              else\n                errors << \"#{label}.#{field} is required when top-level #{fallback} is not provided\"\n              end\n            end\n          end\n\n        else\n          # unknown constraint type: ignore silently (forward-compatible)\n        end\n      end\n      \n      error(errors.join('; ')) if errors.any?\n      tr"}
{"id":"n_afe744327b8914a0","kind":"method","name":"with_resilience","fqname":"connector.Vertex AI/methods.methods/method.with_resilience","loc":{"line":1335,"column":21,"length":2187,"begin":59977,"end":62164},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"silience: lambda do |operation:, config: {}, task: {}, connection: nil, &blk|\n      # Rate limiting (per-job) â€” always initialize and use a unique name\n      rate_limit_info = nil\n      if config['rate_limit']\n        rate_limit_info = call('check_rate_limit', operation, config['rate_limit'])\n      end\n\n      circuit_key   = \"circuit_#{operation}\"\n      circuit_state = call('memo_get', circuit_key) || { 'failures' => 0 }\n      error(\"Circuit breaker open for #{operation}. Too many recent failures.\") if circuit_state['failures'] >= 5\n\n      begin\n        result =\n          if blk\n            # Instrument the block path so trace is still present\n            corr    = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n            started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n            raw     = blk.call\n            dur_ms  = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n            out     = raw.is_a?(Hash) ? JSON.parse(JSON.dump(raw)) : { 'result' => raw }\n            out['_trace'] ||= {}\n            out['_trace'].merge!({ 'correlation_id' => corr, 'duration_ms' => dur_ms, 'attempt' => 1 })\n            out\n          else\n            error('with_resilience requires a task hash with url/method') unless task.is_a?(Hash) && task['url']\n\n            call('http_request',\n              connection,\n              method:       (task['method'] || 'GET'),\n              url:          task['url'],\n              payload:      task['payload'],\n              headers:      (task['headers'] || {}),\n              retry_config: (task['retry_config'] || {})\n            )\n          end\n\n        # Attach rate-limit counters to trace if present (guarded)\n        if rate_limit_info && result.is_a?(Hash)\n          result['_trace'] ||= {}\n          result['_trace']['rate_limit'] = rate_limit_info\n        end\n\n        # Reset circuit on success\n        call('memo_put', circuit_key, { 'failures' => 0 }, 300)\n        result\n\n      rescue => e\n        circuit_state['failures'] += 1\n        call('memo_put', circuit_key, circuit_state, 300)\n        # Keep normalized messages intact; do not blanket-retry non-retryables here\n        raise e\n     "}
{"id":"n_8bb1e821495d603e","kind":"method","name":"execute_pipeline","fqname":"connector.Vertex AI/methods.methods/method.execute_pipeline","loc":{"line":1391,"column":22,"length":3528,"begin":62255,"end":65783},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"e_pipeline: lambda do |connection, operation, input, config|\n      # Recursion guard\n      @pipeline_depth ||= 0\n      @pipeline_depth += 1\n      error(\"Pipeline recursion detected!\") if @pipeline_depth > 3\n      local = input\n\n      # 1. Validate\n      if config['validate']\n        call('validate_input',\n          data:         local,\n          schema:       config['validate']['schema'] || [],\n          constraints:  config['validate']['constraints'] || []\n        )\n      end\n      \n      # 2. Transform input\n      if config['transform_input']\n        config['transform_input'].each do |field, transform|\n          if local[field]\n            local[field] = call('transform_data',\n              input:        local[field],\n              from_format:  transform['from'],\n              to_format:    transform['to'],\n              connection:   connection\n            )\n          end\n        end\n      end\n\n      # -- Ensure selected model from ops config is visible to URL builder\n      local['model'] = config['model'] unless call('value_present', local['model'])\n\n      # 3. Build payload\n      payload = if config['payload']\n        call('build_payload',\n          template:   config['payload']['template'] || '',\n          variables:  local.merge('system' => config['payload']['system']),\n          format:     config['payload']['format'] || 'direct'\n        )\n      else\n        local\n      end\n      \n      # 4. Build URL\n      endpoint  = config['endpoint'] || {}\n      url       = call('build_endpoint_url', connection, endpoint, local)\n      \n      # 5. Execute with resilience\n      response = call('with_resilience',\n        operation:  operation,\n        config:     (config['resilience'] || {}),\n        task: {\n          'method'       => endpoint['method'] || 'POST',\n          'url'          => url,\n          'payload'      => payload,\n          'headers'      => call('build_headers', connection),\n          'retry_config' => (config.dig('resilience', 'retry') || {})\n        }, connection: connection\n      )\n      \n      trace_from_response = (response.is_a?(Hash) ? response['_trace'] : nil)\n\n      # 6. Extract response\n      extracted = if config['extract']\n        call('extract_response',\n          data:   response,\n          path:   config['extract']['path'],\n          format: config['extract']['format'] || 'raw'\n        )\n      else\n        response\n      end\n      \n      # 6.5 Attach trace ASAP so post_process can preserve/propagate it\n      if trace_from_response\n        if extracted.is_a?(Hash)\n          extracted['_trace'] ||= {}\n          extracted['_trace'].merge!(trace_from_response)\n        else\n          extracted = { 'result' => extracted, '_trace' => trace_from_response }\n        end\n      end\n\n      # 7. Post-process\n      if config['post_process']\n        extracted = call(config['post_process'], extracted, local)\n      end\n\n      # 7.5 Ensure trace still present after post_process (if function dropped it)\n      if trace_from_response\n        if extracted.is_a?(Hash)\n          extracted['_trace'] ||= {}\n          # Preserve any trace the post-processor may have added; don't overwrite it\n          extracted['_trace'].merge!(trace_from_response) { |_k, old, _new| old }\n        else\n          extracted = { 'result' => extracted, '_trace' => trace_from_response }\n        end\n      end\n      \n      # 8. Enrich\n      call('enrich_response',\n        response: extracted,\n        metadata: { 'operation' => operation, 'model' => config['model'] || local['model'] }\n   "}
{"id":"n_6c90238a11b665e1","kind":"method","name":"behavior_registry","fqname":"connector.Vertex AI/methods.methods/method.behavior_registry","loc":{"line":1501,"column":23,"length":10076,"begin":65929,"end":76005},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"r_registry: lambda do\n      {\n        # Text Operations\n        'text.generate' => {\n          description: 'Generate text from a prompt',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['streaming', 'caching'],\n          config_template: {\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => '{prompt}',\n              'system' => nil\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        'text.translate' => {\n          description: 'Translate text between languages',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true, 'max_length' => 10000 },\n                { 'name' => 'target_language', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'source_language' => { 'from' => 'language_code', 'to' => 'name' },\n              'target_language' => { 'from' => 'language_code', 'to' => 'name' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Translate the following text from {source_language} to {target_language}. Return only the translation:\\n\\n{text}',\n              'system' => 'You are a professional translator. Maintain tone and context.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          },\n          defaults: {\n            'temperature' => 0.3,\n            'max_tokens' => 2048\n          }\n        },\n        'text.summarize' => {\n          description: 'Summarize text content',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'max_words', 'required' => false }\n              ]\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Summarize the following text in {max_words} words:\\n\\n{text}',\n              'system' => 'You are an expert at creating clear, concise summaries.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            },\n            'post_process' => 'add_word_count'\n          },\n          defaults: {\n            'temperature' => 0.5,\n            'max_words' => 200\n          }\n        },\n        'text.classify' => {\n          description: 'Classify text into categories',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'categories', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'categories' => { 'from' => 'categories', 'to' => 'text' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Classify this text into one of these categories:\\n{categories}\\n\\nText: {text}\\n\\nRespond with JSON: {\"category\": \"name\", \"confidence\": 0.0-1.0}',\n              'system' => 'You are a classification expert. Always return valid JSON.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_json'\n            }\n          },\n          defaults: {\n            'temperature' => 0.1\n          }\n        },\n\n        # Embedding Operations\n        'text.embed' => {\n          description: 'Generate text embeddings',\n          capability: 'embedding',\n          supported_models: ['text-embedding-005', 'text-embedding-004', 'textembedding-gecko', 'gemini-embedding-001'],\n          features: ['batching', 'caching'],\n          # @note PATCH 2025-10-01-D aligned constraints.max_items.value with that of API\n          config_template: {\n            'validate' => {\n              'schema' => [ { 'name' => 'texts', 'required' => true } ],\n              'constraints' => [ { 'type' => 'max_items', 'field' => 'texts', 'value' => 100 } ]\n            },\n            'payload' => { 'format' => 'embedding' },\n            'endpoint' => { 'path' => ':predict', 'method' => 'POST' },\n            'extract' => { 'format' => 'embeddings' },\n            'post_process' => 'wrap_embeddings_vectors'\n          }\n        },\n        # Multimodal Operations\n        'multimodal.analyze' => {\n          description: 'Analyze images with text prompts',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-pro', 'gemini-1.5-flash'],\n          features: ['streaming'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'prompt', 'required' => true },\n                { 'name' => 'images', 'required' => true }\n              ]\n            },\n            'payload' => {\n              'format' => 'multimodal'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        # Vector Operations\n        'vector.upsert_datapoints' => {\n          description: 'Upsert datapoints into a Vector Search index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index', 'required' => true }\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['datapoints', 'embeddings'] }\n              ]\n            },\n            'payload'  => { 'format' => 'upsert_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_indexes',\n              'path'   => ':upsertDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' }, # empty body on success\n            'post_process' => 'add_upsert_ack'\n          }\n        },\n        'vector.find_neighbors' => {\n          description: 'Find nearest neighbors from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'queries',           'required' => true },\n                { 'name' => 'distance_metric' },        # optional\n                { 'name' => 'feature_norm_type' },      # optional\n                { 'name' => 'include_stats' }           # optional\n              ],\n              'constraints' => [\n                # Exactly one locator per query: vector OR datapoint_id\n                {\n                  'type'   => 'xor',\n                  'scope'  => 'queries[]',\n                  'fields' => ['feature_vector', 'datapoint_id'],\n                  'aliases'=> { 'feature_vector' => ['vector'] } # honor your alias\n                },\n                # If a query omits neighbor_count, allow top-level neighbor_count or the internal default (10)\n                {\n                  'type'               => 'fallback_required',\n                  'scope'              => 'queries[]',\n                  'field'              => 'neighbor_count',\n                  'fallback_to_root'   => 'neighbor_count',\n                  'default_if_absent'  => 10  # matches your payload fallback\n                }\n              ]\n            },\n            'payload'  => { 'format' => 'find_neighbors' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':findNeighbors',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_find_neighbors'\n          }\n        },\n        'vector.read_datapoints' => {\n          description: 'Read datapoints (vectors) by ID from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'ids' },        # manual ids\n                { 'name' => 'groups' },     # from find_neighbors (normalized)\n                { 'name' => 'neighbors' }   # flattened neighbors\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['ids', 'groups', 'neighbors'] },\n                { 'type' => 'max_items', 'field' => 'ids', 'value' => 1000 }\n              ]\n            },\n            'payload'  => { 'format' => 'read_index_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':readIndexDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_read_index_datapoints'\n          }\n        }     \n   "}
{"id":"n_68eb76aaa56522de","kind":"method","name":"configuration_registry","fqname":"connector.Vertex AI/methods.methods/method.configuration_registry","loc":{"line":1760,"column":28,"length":1079,"begin":76092,"end":77171},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"n_registry: lambda do |connection, user_config|\n      {\n        # Model selection\n        models: {\n          default: user_config['model'] || connection['default_model'] || 'gemini-1.5-flash',\n          strategy: connection['optimization_mode'] || 'balanced',\n          mode: user_config['model_mode'] || 'auto'\n        },\n        \n        # Generation settings\n        generation: {\n          temperature: user_config['temperature'],\n          max_tokens: user_config['max_tokens'],\n          top_p: user_config['top_p'],\n          top_k: user_config['top_k']\n        }.compact,\n        \n        # Features\n        features: {\n          caching: {\n            enabled: connection['enable_caching'] != false,\n            ttl: user_config['cache_ttl'] || 300\n          },\n          logging: {\n            enabled: connection['enable_logging'] == true\n          }\n        },\n        \n        # Execution\n        execution: {\n          retry: {\n            max_attempts: 3,\n            backoff: 1.0\n          },\n          rate_limit: {\n            rpm: 60\n          }\n        }\n   "}
{"id":"n_aa61e1994b4106be","kind":"method","name":"execute_behavior","fqname":"connector.Vertex AI/methods.methods/method.execute_behavior","loc":{"line":1802,"column":22,"length":3631,"begin":77253,"end":80884},"file":null,"keys":[],"http":{"verbs":["DELETE"],"endpoints":["DELETE model","DELETE model_override"]},"text":"e_behavior: lambda do |connection, behavior, input, user_config = {}|\n      behavior_def = call('behavior_registry')[behavior] or error(\"Unknown behavior: #{behavior}\")\n      local_input = input # @note PATCH 2025-10-03-A assume input deep copied at action boundary\n\n      # Apply defaults without side effects\n      if behavior_def[:defaults]\n        behavior_def[:defaults].each { |k, v| local_input[k] = local_input.key?(k) ? local_input[k] : v }\n      end\n\n      # Bring model-selection keys into local_input\n      %w[model model_mode lock_model_revision].each do |k|\n        if user_config.key?(k) && !user_config[k].nil?\n          local_input[k] = user_config[k]\n        end\n      end\n\n      cfg = call('configuration_registry', connection, user_config)\n      operation_config = JSON.parse(JSON.dump(behavior_def[:config_template] || {}))\n\n      # Bring generation settings into the local input\n      if cfg[:generation]\n        cfg[:generation].each { |k, v| local_input[k] = v unless v.nil? }\n      end\n\n      operation_config['model'] = call('select_model', behavior_def, cfg, local_input)\n\n      # Force correct model if needed\n      if behavior_def[:supported_models].any? && !behavior_def[:supported_models].include?(operation_config['model'])\n        scrubbed = call('deep_copy', local_input) # @note deep_copy here, but also deleting - should resolve over alloc\n        scrubbed.delete('model')\n        scrubbed.delete('model_override')\n        operation_config['model'] = call('select_model', behavior_def, cfg, scrubbed.merge('model_mode' => 'auto'))\n      end\n\n      operation_config['resilience'] = cfg[:execution]\n\n      # Embedding\n      # Guard against global region for embeddings\n      if behavior == 'text.embed' && connection['region'].to_s == 'global'\n        error(\"Embeddings are typically not served from the 'global' location. Choose a concrete region like 'us-central1'.\")\n      end\n\n      if behavior == 'text.embed' && connection['enable_logging'] == true\n        # Log what we're about to execute\n        debug_info = {\n          'input_texts' => local_input['texts'],\n          'model' => operation_config['model'],\n          'config' => operation_config\n        }\n        \n        # Execute pipeline WITH debugging\n        result = call('execute_pipeline', connection, behavior, local_input, operation_config)\n        \n        # Check if result has actual data\n        if result.is_a?(Hash)\n          debug_result = call('deep_copy', result)\n\n          result['trace'] ||= {}\n          result['trace']['debug'] = {\n            'input'  => debug_info,\n            'output' => debug_result\n          }\n        end\n        \n        return result\n      else\n        # Normal execution for non-embedding behaviors\n        result = call('execute_pipeline', connection, behavior, local_input, operation_config)\n      end\n\n      # Add model selection trace\n      selection_mode = (local_input['model_mode'] || cfg.dig(:models, :mode) || 'auto').to_s\n      strategy = (cfg.dig(:models, :strategy) || 'balanced').to_s\n      explicit_in = user_config['model']\n\n      if result.is_a?(Hash) && result['trace'].is_a?(Hash)\n        result['trace']['model_selection'] = {\n          'mode' => selection_mode,\n          'strategy' => strategy,\n          'explicit_model' => explicit_in,\n          'effective_model' => operation_config['model']\n        }.compact\n      end\n\n      # Cache if enabled\n      if cfg[:features][:caching][:enabled]\n        cache_key = \"vertex_#{behavior}_#{local_input.to_json.hash}\"\n        call('memo_put', cache_key, result, cfg[:features][:caching][:ttl] || 300)\n      end\n\n      re"}
{"id":"n_7b41fee00ddc673c","kind":"method","name":"debug_embedding_response","fqname":"connector.Vertex AI/methods.methods/method.debug_embedding_response","loc":{"line":1897,"column":30,"length":673,"begin":80996,"end":81669},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"g_response: lambda do |data|\n      return unless ENV['VERTEX_DEBUG'] == 'true'  # Only log when debugging enabled\n      \n      if data && data['predictions']\n        pred = data['predictions'].first\n        structure = if pred.is_a?(Hash)\n          pred.keys.join(', ')\n        else\n          pred.class.name\n        end\n        puts \"DEBUG: Embedding response structure - predictions[0] keys: #{structure}\"\n        \n        if pred.is_a?(Hash) && pred['embeddings']\n          emb_structure = pred['embeddings'].is_a?(Hash) ? pred['embeddings'].keys.join(', ') : pred['embeddings'].class.name\n          puts \"DEBUG: embeddings structure: #{emb_structure}\"\n        end\n     "}
{"id":"n_eee95286c4553375","kind":"method","name":"add_upsert_ack","fqname":"connector.Vertex AI/methods.methods/method.add_upsert_ack","loc":{"line":1916,"column":20,"length":320,"begin":81721,"end":82041},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"upsert_ack: lambda do |response, input|\n      # response is empty on success; return a useful ack\n      out = {\n        'ack'         => 'upserted',\n        'count'       => Array(input['datapoints']).size,\n        'index'       => input['index'],\n        'empty_body'  => (response.nil? || response == {})\n      }\n     "}
{"id":"n_a36d39ac7d8c40c2","kind":"method","name":"add_word_count","fqname":"connector.Vertex AI/methods.methods/method.add_word_count","loc":{"line":1927,"column":20,"length":290,"begin":82064,"end":82354},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"word_count: lambda do |response, input|\n      if response.is_a?(String)\n        { \n          'result' => response,\n          'word_count' => response.split.size\n        }\n      else\n        {\n          'result' => response,\n          'word_count' => response.to_s.split.size\n        }\n     "}
{"id":"n_9cff7eca6a4c75ed","kind":"method","name":"apply_template","fqname":"connector.Vertex AI/methods.methods/method.apply_template","loc":{"line":1942,"column":20,"length":238,"begin":82408,"end":82646},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"y_template: lambda do |template, variables|\n      return template unless template && variables\n      \n      result = template.dup\n      variables.each do |key, value|\n        result = result.gsub(\"{#{key}}\", value.to_s)\n      end\n      re"}
{"id":"n_9d2cd866f737081e","kind":"method","name":"approx_token_count","fqname":"connector.Vertex AI/methods.methods/method.approx_token_count","loc":{"line":1953,"column":24,"length":107,"begin":82708,"end":82815},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"oken_count: lambda do |text|\n      # Fast, side-effect-free approximation\n      ((text.to_s.length) / 4.0)."}
{"id":"n_08206b37c962f7c5","kind":"method","name":"augment_vector_context","fqname":"connector.Vertex AI/methods.methods/method.augment_vector_context","loc":{"line":1958,"column":28,"length":769,"begin":82846,"end":83615},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"or_context: lambda do |connection, local_input|\n      # @note PATCH 2025-10-03-A assume input deep copied at action boundary\n      out = local_input\n      need_metric = !call('value_present', out['distance_metric'])\n      need_norm   = !call('value_present', out['feature_norm_type'])\n      return out unless need_metric || need_norm\n\n      # Only attempt admin discovery if the connection is allowed\n      return out unless connection['allow_admin_discovery'] == true\n\n      begin\n        disc = call('discover_index_config', connection, out)\n        out['distance_metric']   ||= disc['distance_metric']\n        out['feature_norm_type'] ||= disc['feature_norm_type']\n      rescue\n        # Softâ€‘fail; confidence will be nil but neighbors still returned\n      end\n   "}
{"id":"n_ed73c01864b4988d","kind":"method","name":"build_endpoint_url","fqname":"connector.Vertex AI/methods.methods/method.build_endpoint_url","loc":{"line":1979,"column":24,"length":3200,"begin":83667,"end":86867},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"endpoint_url: lambda do |connection, endpoint_config, input|\n      v = connection['version']\n      api_version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n      region = connection['region']\n      base_regional = \"https://#{region}-aiplatform.googleapis.com/#{api_version}\"\n\n      family = endpoint_config['family']\n\n      case family\n      # PUBLISHER MODELS\n      when 'publisher_models'\n        api_version = (connection['version'].to_s.strip.empty? ? 'v1' : connection['version'])\n        publisher   = endpoint_config['publisher'] || 'google'\n        \"https://aiplatform.googleapis.com/#{api_version}/publishers/#{publisher}/models\"\n      # VECTOR INDEXES\n      when 'vector_indexes' # admin/data-plane ops on Index resources\n        index = call('qualify_resource', connection, 'index', input['index'] || endpoint_config['index'])\n        \"#{base_regional}/#{index}#{endpoint_config['path']}\" # e.g., ':upsertDatapoints'\n      # VECTOR INDEX ENDPOINTS\n      when 'vector_index_endpoints' # query via MatchService or admin reads\n        base =\n          if endpoint_config['admin'] == true\n            v = connection['version']; version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n            \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n          else\n            call('vector_search_base', connection, input) # uses vdb host when provided\n          end\n        ie = call('qualify_resource', connection, 'index_endpoint',\n                  input['index_endpoint'] || endpoint_config['index_endpoint'])\n        \"#{base}/#{ie}#{endpoint_config['path']}\" # e.g., ':findNeighbors' or ''\n\n\n      else\n        base_host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n        base_url  = \"https://#{base_host}/#{api_version}\"\n\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        model = call('value_present', input['model']) ? input['model'] : (connection['default_model'] || 'gemini-1.5-flash')\n        model_id = model.to_s\n\n        # Honor lock model revision input flag\n        lock_rev = input['lock_model_revision'] == true || endpoint_config['require_version'] == true\n        if lock_rev && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n        # Only resolve to a numeric version when explicitly requested by endpoint config\n        if endpoint_config['require_version'] == true && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n\n        model_path = \"projects/#{connection['project']}/locations/#{region}/publishers/google/models/#{model_id}\"\n\n        # If the user supplies a custom path, replace the the critical elements with those from the connection\n        if endpoint_config['custom_path']\n          endpoint_config['custom_path']\n            .gsub('{project}',  connection['project'])\n            .gsub('{region}',   region)\n            .gsub('{endpoint}', connection['vector_search_endpoint'] || '')\n        else\n          \"#{base_url}/#{model_path}#{endpoint_config['path'] || ':generateContent'}\"\n        end\n   "}
{"id":"n_c9c3b33a64d18adb","kind":"method","name":"build_generation_config","fqname":"connector.Vertex AI/methods.methods/method.build_generation_config","loc":{"line":2044,"column":29,"length":559,"begin":86991,"end":87550},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ation_config: lambda do |vars|\n      {\n        'temperature'     => call('value_present', vars['temperature']) ? vars['temperature'] : 0.7,\n        'maxOutputTokens' => call('value_present', vars['max_tokens'])  ? vars['max_tokens']  : 2048,\n        'topP'            => call('value_present', vars['top_p'])       ? vars['top_p']       : 0.95,\n        'topK'            => call('value_present', vars['top_k'])       ? vars['top_k']       : 40,\n        'stopSequences'   => call('value_present', vars['stop_sequences']) ? vars['stop_sequences'] : nil\n      }.c"}
{"id":"n_61a5c776d99be350","kind":"method","name":"build_headers","fqname":"connector.Vertex AI/methods.methods/method.build_headers","loc":{"line":2055,"column":19,"length":147,"begin":87600,"end":87747},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"uild_headers: lambda do |connection|\n      {\n        'Content-Type' => 'application/json',\n        'X-Goog-User-Project' => connection['project']\n "}
{"id":"n_a03fa7e7b462817c","kind":"method","name":"check_rate_limit","fqname":"connector.Vertex AI/methods.methods/method.check_rate_limit","loc":{"line":2063,"column":22,"length":661,"begin":87792,"end":88453},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"k_rate_limit: lambda do |operation, limits|\n      rpm  = (limits['rpm'] || limits[:rpm]).to_i\n      window_id     = Time.now.to_i / 60\n      window_start  = window_id * 60\n      key           = \"rate_#{operation}_#{window_id}\"\n\n      count = call('memo_get', key) || 0\n      error(\"Rate limit exceeded for #{operation}. Please wait before retrying.\") if count >= rpm\n\n      new_count = count + 1\n      reset_in  = (window_start + 60) - Time.now.to_i\n      reset_in  = 60 if reset_in <= 0\n\n      call('memo_put', key, new_count, reset_in)\n\n      { 'rpm' => rpm, 'count' => new_count, 'reset_in_s' => reset_in, 'window_started_at' => Time.at(window_start).utc.iso"}
{"id":"n_6185140d0756b8bf","kind":"method","name":"chunk_by_tokens","fqname":"connector.Vertex AI/methods.methods/method.chunk_by_tokens","loc":{"line":2081,"column":21,"length":1696,"begin":88477,"end":90173},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"nk_by_tokens: lambda do |items:, token_ceiling:, max_items:, max_body_bytes: nil|\n      token_cap = token_ceiling.to_i\n      token_cap = 8000 if token_cap <= 0 # conservative fallback if not provided\n      max_items = (max_items || 100).to_i\n      max_items = 1 if max_items <= 0\n      max_body  = max_body_bytes ? max_body_bytes.to_i : nil\n\n      batches   = []\n      oversized = []\n\n      current       = []\n      current_tokens= 0\n      current_bytes = 0\n\n      # crude but steady overheads so we donâ€™t undercount request size\n      per_item_overhead = 64\n      base_overhead     = 512\n\n      items.each do |item|\n        txt = item['text'].to_s\n        t   = call('approx_token_count', txt)\n        b   = txt.bytesize + per_item_overhead\n\n        # single-item guards\n        if t > token_cap\n          oversized << { 'item' => item, 'reason' => \"estimated tokens #{t} exceed ceiling #{token_cap}\" }\n          next\n        end\n        if max_body && (b + base_overhead) > max_body\n          oversized << { 'item' => item, 'reason' => \"approx body bytes #{b + base_overhead} exceed limit #{max_body}\" }\n          next\n        end\n\n        # would adding this item break any limit?\n        if !current.empty? &&\n          (current_tokens + t > token_cap ||\n            current.length + 1 > max_items ||\n            (max_body && current_bytes + b + base_overhead > max_body))\n          batches << current\n          current        = []\n          current_tokens = 0\n          current_bytes  = 0\n        end\n\n        current << item\n        current_tokens += t\n        current_bytes  += b\n      end\n\n      batches << current unless current.empty?\n\n      { 'batches' => batches, 'oversized' => ove"}
{"id":"n_ff0b2aa2fcd8d222","kind":"method","name":"coerce_embeddings_to_datapoints","fqname":"connector.Vertex AI/methods.methods/method.coerce_embeddings_to_datapoints","loc":{"line":2136,"column":37,"length":1241,"begin":90277,"end":91518},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"_to_datapoints: lambda do |vars|\n      embeddings = Array(vars['embeddings'])\n      error('No embeddings provided') if embeddings.empty?\n\n      ids     = Array(vars['datapoint_ids'])\n      prefix  = (vars['datapoint_id_prefix'] || 'dp_').to_s\n      start   = (vars['start_index'] || 1).to_i\n      pad_to  = (vars['pad_to'] || 6).to_i\n\n      if ids.empty?\n        ids = embeddings.each_index.map { |i| \"#{prefix}#{(start + i).to_s.rjust(pad_to, '0')}\" }\n      elsif ids.length != embeddings.length\n        error(\"datapoint_ids length (#{ids.length}) must match embeddings length (#{embeddings.length})\")\n      end\n\n      common_restricts        = vars['common_restricts']\n      common_numeric          = vars['common_numeric_restricts']\n      common_crowding_tag     = vars['common_crowding_tag']\n      common_embedding_meta   = vars['embedding_metadata']\n\n      embeddings.each_with_index.map do |vec, i|\n        {\n          'datapointId'       => ids[i],\n          'featureVector'     => Array(vec).map(&:to_f),\n          'restricts'         => common_restricts,\n          'numericRestricts'  => common_numeric,\n          'crowdingTag'       => common_crowding_tag,\n          'embeddingMetadata' => common_embedding_meta\n        }.compact\n "}
{"id":"n_1601b3725eadd6d8","kind":"method","name":"coerce_kwargs","fqname":"connector.Vertex AI/methods.methods/method.coerce_kwargs","loc":{"line":2168,"column":19,"length":886,"begin":91540,"end":92426},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" coerce_kwargs: lambda do |*args, **kwargs|\n      # Non-destructive copies\n      positional = args.dup\n      kw = kwargs.dup\n\n      # If caller passed a trailing Hash, treat it as kwargs (merged with explicit kwargs)\n      if positional.last.is_a?(Hash)\n        trailing = positional.pop\n        # deep copy to avoid side-effects\n        trailing_copy = JSON.parse(JSON.dump(trailing)) rescue trailing.dup\n        trailing_sym  = trailing_copy.each_with_object({}) do |(k, v), acc|\n          key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n          acc[key] = v\n        end\n        # Explicit kwargs take precedence\n        kw = trailing_sym.merge(kw) { |_key, left, right| right }\n      end\n\n      # Ensure symbolized keys for kwargs\n      kw = kw.each_with_object({}) do |(k, v), acc|\n        key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n        acc[key] = v\n      end\n\n      [positio"}
{"id":"n_2b1d3195806793f4","kind":"method","name":"confidence_from_distance","fqname":"connector.Vertex AI/methods.methods/method.confidence_from_distance","loc":{"line":2195,"column":30,"length":615,"begin":92463,"end":93078},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"_from_distance: lambda do |distance, metric, feature_norm_type|\n      return nil unless distance\n      m = metric.to_s\n      case m\n      when 'COSINE_DISTANCE'\n        # distance = 1 - cos_sim  => confidence = (1 + cos_sim)/2 = 1 - distance/2\n        c = 1.0 - (distance.to_f / 2.0)\n        [[c, 0.0].max, 1.0].min\n      when 'DOT_PRODUCT_DISTANCE'\n        # distance = -dot; if vectors were UNIT_L2_NORM, dot âˆˆ [-1,1] ~ cos_sim\n        if feature_norm_type.to_s == 'UNIT_L2_NORM'\n          dot = -distance.to_f\n          c = 0.5 * (1.0 + dot)\n          [[c, 0.0].max, 1.0].min\n        end\n      else\n        nil"}
{"id":"n_d9d99d96cdc4ca7c","kind":"method","name":"deep_copy","fqname":"connector.Vertex AI/methods.methods/method.deep_copy","loc":{"line":2216,"column":15,"length":43,"begin":93126,"end":93169},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ct\n    deep_copy: lambda { |obj| JSON.parse"}
{"id":"n_8e4ee8f78693443a","kind":"method","name":"discover_index_config","fqname":"connector.Vertex AI/methods.methods/method.discover_index_config","loc":{"line":2218,"column":27,"length":1500,"begin":93199,"end":94699},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ver_index_config: lambda do |connection, input|\n      ep = call('qualify_resource', connection, 'index_endpoint', input['index_endpoint'])\n      dep_id = input['deployed_index_id'].to_s\n      return {} if ep.to_s.empty? || dep_id.empty?\n\n      cache_key = \"idxcfg:#{ep}:#{dep_id}\"\n      if (hit = call('memo_get', cache_key)); return hit; end\n\n      # 1) Read IndexEndpoint (admin host)\n      url_ep = call('build_endpoint_url', connection, {\n        'family' => 'vector_index_endpoints', 'index_endpoint' => ep, 'method' => 'GET', 'admin' => true\n      }, input)\n      ep_body = call('http_request', connection, method: 'GET', url: url_ep, headers: call('build_headers', connection))\n      deployed = Array(ep_body['deployedIndexes']).find { |d| d['id'] == dep_id }\n      return {} unless deployed && deployed['index']\n\n      # 2) Read Index (admin host)\n      url_idx = call('build_endpoint_url', connection, {\n        'family' => 'vector_indexes', 'index' => deployed['index'], 'method' => 'GET'\n      }, input)\n      idx_body = call('http_request', connection, method: 'GET', url: url_idx, headers: call('build_headers', connection))\n\n      cfg = idx_body.dig('metadata', 'config') || {}\n      out = {\n        'index'              => deployed['index'],\n        'distance_metric'    => (cfg['distanceMeasureType'] || cfg['distance_measure_type']),\n        'feature_norm_type'  => (cfg['featureNormType']     || cfg['feature_norm_type'])\n      }.compact\n\n      call('memo_put', cache_key, out, 600)"}
{"id":"n_c9d591a5046c75b4","kind":"method","name":"each_in_scope","fqname":"connector.Vertex AI/methods.methods/method.each_in_scope","loc":{"line":2252,"column":19,"length":273,"begin":94793,"end":95066},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"   each_in_scope: lambda do |data, scope|\n      s = scope.to_s\n      if s.end_with?('[]')\n        key = s[0..-3] # strip []\n        arr = Array(data[key]) # safe\n        arr.each_with_index.map { |item, idx| [item || {}, \"#{key}[#{idx}]\"] }\n      else\n        [[data, '$']]"}
{"id":"n_ee074dafdc126b52","kind":"method","name":"error_hint","fqname":"connector.Vertex AI/methods.methods/method.error_hint","loc":{"line":2264,"column":16,"length":565,"begin":95104,"end":95669},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"R\n    error_hint: lambda do |connection, code, status|\n      c = code.to_i\n      case c\n      when 401\n        # keep small + actionable\n        'Unauthorized. Reâ€‘authenticate; then check project/region, API enablement, and roles.'\n      when 403\n        'Forbidden. Check project/region, API enablement, and roles.'\n      when 404\n        'Not found. Check project/region (feature/model availability) and the resource id.'\n      when 429\n        'Rate limit/quota. Reduce request rate or increase quota. Will honor Retryâ€‘After when present.'\n      else\n       "}
{"id":"n_bfc1dcfb92ab1185","kind":"method","name":"extract_ids_for_read","fqname":"connector.Vertex AI/methods.methods/method.extract_ids_for_read","loc":{"line":2281,"column":26,"length":984,"begin":95698,"end":96682},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"extract_ids_for_read: lambda do |vars|\n      mode = (vars['id_source'] || 'auto').to_s\n      pick = lambda do |source|\n        case source\n        when 'manual'\n          Array(vars['ids']).compact\n        when 'neighbors'\n          Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact\n        when 'groups'\n          Array(vars['groups'])\n            .flat_map { |g| Array(g['neighbors']) }\n            .map { |n| n['datapoint_id'] }.compact\n        else # auto: prefer manual â†’ neighbors â†’ groups\n          ids = Array(vars['ids']).compact\n          ids = Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids = Array(vars['groups']).flat_map { |g| Array(g['neighbors']) }.map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids\n        end\n      end\n\n      ids = pick.call(mode).map(&:to_s)\n      ids = ids.uniq if vars['unique'] != false\n      error('No datapoint IDs provided or derivable from neighbors/groups') if id"}
{"id":"n_5af4f46fd7251302","kind":"method","name":"extract_user_config","fqname":"connector.Vertex AI/methods.methods/method.extract_user_config","loc":{"line":2308,"column":25,"length":1634,"begin":96750,"end":98384},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"\n    extract_user_config: lambda do |input, cfg_enabled = false, config_ctx = {}|\n      cfg = {}\n      config_ctx ||= {}\n\n      # Prefer config_fields values; fall back to input (back-compat)\n      mode = (config_ctx['model_mode'] || input['model_mode'] || '').to_s\n      #explicit_model = config_ctx['model'] || input['model'] || input['model_override']\n      # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n      explicit_model = call('value_present', input['model']) ? input['model'] : config_ctx['model'] || input['model_override']\n\n      case mode\n      when 'explicit'\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      when 'connection', 'auto', ''\n        # no-op; use selection logic defaults\n      else\n        # unknown mode: treat as legacy explicit if model present\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      end\n\n      cfg['model_mode']          = mode unless mode.empty?\n      # After (prefer config_fields, fall back to input for completeness)\n      if config_ctx.key?('lock_model_revision')\n        cfg['lock_model_revision'] = config_ctx['lock_model_revision']\n      elsif input.key?('lock_model_revision')\n        cfg['lock_model_revision'] = input['lock_model_revision']\n      end\n\n      # Advanced tuning (unchanged)\n      if cfg_enabled\n        cfg['temperature'] = input['temperature'] if input.key?('temperature')\n        cfg['max_tokens']  = input['max_tokens']  if input.key?('max_tokens')\n        cfg['cache_ttl']   = input['cache_ttl']   if input.key?('cache_ttl')\n      end\n"}
{"id":"n_ca2eb35de7bfa067","kind":"method","name":"execute_batch_behavior","fqname":"connector.Vertex AI/methods.methods/method.execute_batch_behavior","loc":{"line":2347,"column":28,"length":2729,"begin":98439,"end":101168},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"  execute_batch_behavior: lambda do |connection, behavior, items, batch_size, strategy, options = {}|\n      results = []\n      errors = []\n      total_processed = 0\n\n      # @note PATCH 2025-10-03-A assume input deep copied at action boundary\n      local_items = Array(items)\n      \n      # 1) Build batches according to strategy\n      batches =\n        if strategy.to_s == 'tokens'\n          chunk = call('chunk_by_tokens',\n            items: local_items,\n            token_ceiling: (options['token_ceiling'] || options[:token_ceiling]),\n            max_items: (options['max_items_per_batch'] || options[:max_items_per_batch] || 100),\n            max_body_bytes: (options['max_body_bytes'] || options[:max_body_bytes])\n          )\n          # surface oversize items as per-batch errors (unchanged error shape: batch + error)\n          Array(chunk['oversized']).each do |o|\n            errors << { 'batch' => [o['item']], 'error' => \"Skipped item: #{o['reason']}\" }\n          end\n          chunk['batches'] || []\n        else\n          size  = (batch_size || 10).to_i\n          limit = (options['max_items_per_batch'] || options[:max_items_per_batch] || size).to_i\n          size  = [[size, limit].min, 1].max\n          local_items.each_slice(size).to_a\n        end\n\n      # 2) Execute batches\n      batches.each do |batch|\n        begin\n          if behavior.include?('embed')\n            texts = batch.map { |item| item['text'] }\n\n            payload = { 'texts' => texts }\n            unique_tasks = batch.map { |i| i['task_type'] }.compact.uniq\n            payload['task_type'] = unique_tasks.first if unique_tasks.length == 1\n\n            batch_result = call('execute_behavior', connection, behavior, payload)\n\n            # For embeddings, API is truly batchable: one result per batch (keep prior shape)\n            results.concat([batch_result])\n            total_processed += batch.length\n\n          else\n            # Non-embeddings: execute per-item so partial failures are surfaced\n            batch.each do |item|\n              begin\n                item_result = call('execute_behavior', connection, behavior, item)\n                results << item_result\n                total_processed += 1\n              rescue => e\n                errors << { 'batch' => [item], 'error' => e.message }\n              end\n            end\n          end\n\n        rescue => e\n          # catastrophic batch failure (network, quota, etc.)\n          errors << { 'batch' => batch, 'error' => e.message }\n        end\n      end\n      \n      {\n        'success'         => errors.empty?,\n        'results'         => results,\n        'errors'          => errors,\n        'total_processed' => total_processed,\n        'total_errors'    => err"}
{"id":"n_506961ae945e2030","kind":"method","name":"format_user_error","fqname":"connector.Vertex AI/methods.methods/method.format_user_error","loc":{"line":2421,"column":23,"length":400,"begin":101215,"end":101615},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ER\n    format_user_error: lambda do |err|\n      base = \"Vertex AI error #{err['code']}\"\n      base += \" #{err['status']}\" if err['status']\n      head = \"#{base}: #{err['summary']}\"\n      tags = [\"corr_id=#{err['correlation_id']}\"]\n      tags << \"remote_id=#{err['remote_request_id']}\" if err['remote_request_id']\n      msg = \"#{head} [#{tags.join(' ')}]\"\n      msg += \" â€” Hint: #{err['hint']}\" if e"}
{"id":"n_bd5be9b3a58c4169","kind":"method","name":"get_behavior_input_fields","fqname":"connector.Vertex AI/methods.methods/method.get_behavior_input_fields","loc":{"line":2433,"column":31,"length":19718,"begin":101693,"end":121411},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" get_behavior_input_fields: lambda do |behavior, show_advanced, ui_cfg = {}|\n      show_advanced = !!show_advanced\n      ui_cfg ||= {}\n      explicit      = (ui_cfg['model_mode'] == 'explicit')\n      legacy_mode   = !ui_cfg.key?('model_mode')\n      include_model = false\n\n      behavior_def = call('behavior_registry')[behavior]\n      return [] unless behavior_def\n      \n      # Map behavior to input fields\n      case behavior\n      when 'text.generate'\n        fields = [\n          { name: 'prompt', label: 'Prompt', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.translate'\n        fields = [\n          { name: 'text', label: 'Text to Translate', control_type: 'text-area', optional: false },\n          { name: 'target_language', label: 'Target Language', control_type: 'select', pick_list: 'languages', optional: false },\n          { name: 'source_language', label: 'Source Language', control_type: 'select', pick_list: 'languages', optional: true, hint: 'Leave blank for auto-detection' }\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.summarize'\n        fields = [\n          { name: 'text', label: 'Text to Summarize', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.classify'\n        fields = [\n          { name: 'text', label: 'Text to Classify', control_type: 'text-area', optional: false },\n          { name: 'categories', label: 'Categories', type: 'array', of: 'object', properties: [\n            { name: 'name', label: 'Category Name' },\n            { name: 'description', label: 'Description' }\n          ]}\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.embed'\n        fields = [\n          { name: 'texts', label: 'Texts to Embed', type: 'array', of: 'string', optional: false },\n          { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks',  optional: true, hint: 'Helps the model optimize embeddings for your use case.' },\n          { name: 'title', label: 'Title (for documents)', optional: true, hint: 'Used only with task_type = RETRIEVAL_DOCUMENT.' }\n        ]\n        # Advanced embedding controls\n        if show_advanced\n          fields += [\n            { name: 'output_dimensionality', label: 'Output dimensionality', type: 'integer', group: 'Advanced',\n              hint: 'Truncate embedding size to this dimension (e.g., 256/512/768/3072 depending on model).' },\n            { name: 'auto_truncate', label: 'Auto truncate long inputs', control_type: 'checkbox', default: true, group: 'Advanced',\n              hint: 'Set false to error on over-limit inputs instead of silent truncation.' }\n          ]\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n         end\n         fields\n      when 'vector.upsert_datapoints'\n        fields = [\n          # Target\n          { name: 'index', label: 'Index', group: 'Target', hint: 'Index resource or ID (e.g., projects/.../indexes/IDX or just IDX)', optional: false },\n          # Source: from embeddings (recommended path from Generate embeddings)\n          { name: 'embeddings', label: 'Embeddings', group: 'Source (from embeddings)', type: 'array', of: 'array', optional: true,\n            hint: 'Map from Generate embeddings â†’ vectors or embeddings' },\n          { name: 'datapoint_ids', label: 'Datapoint IDs', group: 'Source (from embeddings)', type: 'array', of: 'string', \n            optional: true, hint: 'Optional; if omitted, IDs are auto-generated' },\n          { name: 'datapoint_id_prefix', label: 'Auto ID prefix', group: 'Source (from embeddings)', optional: true, default: 'dp_' },\n          { name: 'start_index', label: 'Starting index (1-based)', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 1 },\n          { name: 'pad_to', label: 'Pad IDs to N digits', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 6 },\n\n          # Datapoint defaults applied to all when using embeddings\n          { name: 'common_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n          ]},\n          { name: 'common_numeric_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n            { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n          ]},\n          { name: 'common_crowding_tag', group: 'Datapoint defaults', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n          { name: 'embedding_metadata', group: 'Datapoint defaults', type: 'object' },\n\n          # Advanced: provide full datapoints directly (legacy / power-user)\n          { name: 'datapoints', label: 'Datapoints (advanced)', group: 'Provide full datapoints',\n            type: 'array', of: 'object', optional: true, properties: [\n              { name: 'datapoint_id', label: 'Datapoint ID', optional: false },\n              { name: 'feature_vector', label: 'Feature vector', type: 'array', of: 'number', optional: false },\n              { name: 'restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n              ]},\n              { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n                { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n              ]},\n              { name: 'crowding_tag', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n              { name: 'embedding_metadata', type: 'object' }\n            ]}\n        ]\n      when 'vector.find_neighbors'\n        fields = [\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Overrides connection host just for this call (e.g. <hash>....vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', hint: 'Resource or ID (e.g. projects/.../indexEndpoints/IEP or IEP)', optional: false, group: 'Target' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n          { name: 'neighbor_count', label: 'Neighbors per query', type: 'integer', default: 10, group: 'Query' },\n          { name: 'return_full_datapoint', label: 'Return full datapoint', control_type: 'checkbox', group: 'Query' },\n\n          # NEW: scoring & aggregates\n          { name: 'distance_metric', label: 'Index distance metric', control_type: 'select',\n            pick_list: 'vector_distance_metrics', optional: true, group: 'Scoring & aggregates',\n            hint: 'Set if you want valid confidence scores. For DOT_PRODUCT, set Feature normalization to UNIT_L2_NORM.' },\n          { name: 'feature_norm_type', label: 'Feature normalization', control_type: 'select',\n            pick_list: 'vector_feature_norm_types', optional: true, group: 'Scoring & aggregates' },\n          { name: 'include_stats', label: 'Include aggregate stats', control_type: 'checkbox',\n            default: true, optional: true, group: 'Scoring & aggregates' },\n\n          { name: 'queries', label: 'Queries', type: 'array', of: 'object', group: 'Queries', properties: [\n            { name: 'datapoint_id', label: 'Query datapoint ID' },\n            { name: 'feature_vector', label: 'Query vector', type: 'array', of: 'number', hint: 'Use either vector or datapoint_id' },\n            { name: 'neighbor_count', label: 'Override neighbors for this query', type: 'integer' },\n            { name: 'restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n            ]},\n            { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' }, { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        mode = (ui_cfg['id_source'] || 'auto').to_s\n        fields = [\n          # Target\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Optional override (e.g. <hash>.vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, group: 'Target', hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n        ]\n        # Helper lambdas to append groups\n        add_manual = lambda {\n          fields << { name: 'ids', label: 'Datapoint IDs (manual)',\n                      type: 'array', of: 'string', optional: true, group: 'IDs' }\n        }\n        add_neighbors = lambda {\n          fields << { name: 'neighbors', label: 'kâ€‘NN neighbors (flattened)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [{ name: 'datapoint_id' }] }\n        }\n        add_groups = lambda {\n          fields << { name: 'groups', label: 'kâ€‘NN groups (from Find neighbors)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [\n                        { name: 'neighbors', type: 'array', of: 'object', properties: [{ name: 'datapoint_id"}
{"id":"n_85d5f57d69fa146f","kind":"method","name":"get_behavior_output_fields","fqname":"connector.Vertex AI/methods.methods/method.get_behavior_output_fields","loc":{"line":2751,"column":32,"length":3528,"begin":121483,"end":125011},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"s\n    get_behavior_output_fields: lambda do |behavior|\n      case behavior\n      # Text\n      when 'text.generate'\n        [{ name: 'result', label: 'Generated Text' }]\n      when 'text.translate'\n        [\n          { name: 'result', label: 'Translated Text' }\n        ]\n      when 'text.summarize'\n        [\n          { name: 'result', label: 'Summary' },\n          { name: 'word_count', type: 'integer' }\n        ]\n      when 'text.classify'\n        [\n          { name: 'category', label: 'Selected Category' },\n          { name: 'confidence', type: 'number' }\n        ]\n      # Embedding\n      when 'text.embed'\n        [\n          { name: 'embeddings', type: 'array', of: 'object', properties: [\n            { name: 'values', label: 'Values', type: 'array', of: 'number'}]},\n          # @note PATCH 2025-10-01-A, added properties to ensure output is emitted as expected\n          # @note PATCH 2025-10-01-B, removed erroneous trailing space in scalar type \n          { name: 'vectors', type: 'array', of: 'object', properties: [ \n            { name: 'feature_vector', type: 'array', of: 'number' } ]},\n          { name: 'count', type: 'integer' },\n          { name: 'dimension', type: 'integer' },\n          { name: 'avg_norm', type: 'number' },\n          { name: 'norms', type: 'array', of: 'number' } \n        ]\n      # Vector search\n      when 'vector.upsert_datapoints'\n        [\n          { name: 'ack' }, { name: 'count', type: 'integer' }, { name: 'index' }, { name: 'empty_body', type: 'boolean' }\n        ]\n      when 'vector.find_neighbors'\n        [\n          { name: 'summary', type: 'object', properties: [\n            { name: 'groups', type: 'integer' },\n            { name: 'neighbors', type: 'integer' },\n            { name: 'distance_mean', type: 'number' },\n            { name: 'score_mean', type: 'number' },\n            { name: 'score_max',  type: 'number' },\n            { name: 'confidence_mean', type: 'number' },\n            { name: 'confidence_max',  type: 'number' }\n          ]},\n          { name: 'groups', type: 'array', of: 'object', properties: [\n            { name: 'query_id' },\n            { name: 'stats', type: 'object', properties: [\n              { name: 'neighbor_count', type: 'integer' },\n              { name: 'distance_mean', type: 'number' },\n              { name: 'score_mean',     type: 'number' },\n              { name: 'score_max',      type: 'number' },\n              { name: 'confidence_mean', type: 'number' },\n              { name: 'confidence_max',  type: 'number' }\n            ]},\n            { name: 'neighbors', type: 'array', of: 'object', properties: [\n              { name: 'datapoint_id' },\n              { name: 'distance', type: 'number' },\n              { name: 'score',    type: 'number' },\n              { name: 'confidence', type: 'number' },\n              { name: 'datapoint', type: 'object' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        [\n          { name: 'datapoints', type: 'array', of: 'object', properties: [\n            { name: 'datapoint_id' },\n            { name: 'feature_vector', type: 'array', of: 'number' },\n            { name: 'restricts', type: 'array', of: 'object' },\n            { name: 'numeric_restricts', type: 'array', of: 'object' },\n            { name: 'crowding_tag', type: 'object' },\n            { name: 'embedding_metadata', type: 'object' }\n          ] }\n        ]\n      # Multimodal\n      when 'multimodal.analyze'\n        [{ name: 'result', label: 'Analysis' }]\n      else\n        [{ n"}
{"id":"n_7493453b9263ee8a","kind":"method","name":"list_publisher_models","fqname":"connector.Vertex AI/methods.methods/method.list_publisher_models","loc":{"line":2839,"column":27,"length":758,"begin":125086,"end":125844},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"beta1)\n    list_publisher_models: lambda do |connection, publisher: 'google'|\n      ver = connection['version'].to_s.strip\n      ver = ver.empty? ? 'v1' : ver\n      cache_key = \"pub_models:#{publisher}:#{ver}\"   # <â€” include version in key\n\n      if (cached = call('memo_get', cache_key))\n        return cached\n      end\n\n      url = call('build_endpoint_url', connection, { 'family' => 'publisher_models', 'publisher' => publisher }, {})\n      resp = call('http_request', connection, method: 'GET', url: url,\n                  headers: call('build_headers', connection),\n                  retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429, 500, 502, 503, 504] })\n\n      models = (resp['publisherModels'] || [])\n      call('memo_put', cache_key"}
{"id":"n_db87fa15e249d65e","kind":"method","name":"memo_store","fqname":"connector.Vertex AI/methods.methods/method.memo_store","loc":{"line":2858,"column":16,"length":25,"begin":125863,"end":125888},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"   models\n    end,\n\n    m"}
{"id":"n_922c8848841618cf","kind":"method","name":"memo_get","fqname":"connector.Vertex AI/methods.methods/method.memo_get","loc":{"line":2860,"column":14,"length":178,"begin":125905,"end":126083},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" { @__memo ||= {} },\n\n    memo_get: lambda do |key|\n      item = call('memo_store')[key]\n      return nil unless item\n      exp = item['exp']\n      return nil if exp && Time.now."}
{"id":"n_c41f9c08ea43f3c3","kind":"method","name":"memo_put","fqname":"connector.Vertex AI/methods.methods/method.memo_put","loc":{"line":2868,"column":14,"length":145,"begin":126100,"end":126245},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"item['val']\n    end,\n\n    memo_put: lambda do |key, val, ttl=nil|\n      call('memo_store')[key] = { 'val' => val, 'exp' => (ttl ? Time.now.to_i +"}
{"id":"n_459d64eed23df3ca","kind":"method","name":"normalize_find_neighbors","fqname":"connector.Vertex AI/methods.methods/method.normalize_find_neighbors","loc":{"line":2874,"column":30,"length":2416,"begin":126354,"end":128770},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"shape\n    normalize_find_neighbors: lambda do |resp, input|\n      groups_raw = Array(resp['nearestNeighbors'])\n      metric     = input['distance_metric']\n      norm_type  = input['feature_norm_type']\n      include_stats = input.key?('include_stats') ? !!input['include_stats'] : true\n\n      groups = groups_raw.map do |nn|\n        neighbors = Array(nn['neighbors']).map do |n|\n          dist = n['distance']\n          did  = n.dig('datapoint', 'datapointId')\n          {\n            'datapoint_id' => did,\n            'distance'     => dist,\n            # Legacy score: normalized from distance (cosine heuristic)\n            'score'        => call('transform_data', input: dist, from_format: 'distance', to_format: 'similarity'),\n            # New: mathematically valid confidence when possible\n            'confidence'   => call('confidence_from_distance', dist, metric, norm_type),\n            'datapoint'    => n['datapoint']\n          }.compact\n        end\n\n        stats =\n          if include_stats\n            {\n              'neighbor_count'   => neighbors.length,\n              'distance_mean'    => call('safe_mean', neighbors.map { |z| z['distance'] }),\n              'score_mean'       => call('safe_mean', neighbors.map { |z| z['score'] }),\n              'score_max'        => (neighbors.map { |z| z['score'] }.compact.max),\n              'confidence_mean'  => call('safe_mean', neighbors.map { |z| z['confidence'] }),\n              'confidence_max'   => (neighbors.map { |z| z['confidence'] }.compact.max)\n            }.compact\n          end\n\n        {\n          'query_id'  => nn['id'],\n          'stats'     => stats,\n          'neighbors' => neighbors\n        }.compact\n      end\n\n      # Top-level summary if desired\n      summary =\n        if include_stats\n          flat = groups.flat_map { |g| g['neighbors'] || [] }\n          {\n            'groups'          => groups.length,\n            'neighbors'       => flat.length,\n            'distance_mean'   => call('safe_mean', flat.map { |z| z['distance'] }),\n            'score_mean'      => call('safe_mean', flat.map { |z| z['score'] }),\n            'score_max'       => (flat.map { |z| z['score'] }.compact.max),\n            'confidence_mean' => call('safe_mean', flat.map { |z| z['confidence'] }),\n            'confidence_max'  => (flat.map { |z| z['confidence'] }.compact.max)\n          }.compact\n        end\n\n      { 'summary' => summary, "}
{"id":"n_1a3db9579288aabc","kind":"method","name":"normalize_http_error","fqname":"connector.Vertex AI/methods.methods/method.normalize_http_error","loc":{"line":2932,"column":26,"length":1256,"begin":128799,"end":130055},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"    end,\n\n    normalize_http_error: lambda do |connection, code:, body:, headers:, message:, url:, corr_id:, attempt:, duration_ms:|\n      parsed = {}\n      if body.is_a?(Hash)\n        parsed = body\n      else\n        begin\n          parsed = JSON.parse(body.to_s)\n        rescue\n          parsed = {}\n        end\n      end\n\n      gerr    = parsed['error'].is_a?(Hash) ? parsed['error'] : {}\n      status  = gerr['status']\n      summary = (gerr['message'] || message || body.to_s).to_s.strip[0, 300] # compact\n      hint    = call('error_hint', connection, code, status)\n\n      remote_id = nil\n      if headers\n        remote_id = headers['x-request-id'] ||\n                    headers['x-cloud-trace-context'] ||\n                    headers['x-guploader-uploadid']\n      end\n\n      {\n        'code'              => code.to_i,\n        'status'            => status,\n        'summary'           => summary,\n        'hint'              => hint,\n        'retryable'         => call('retryable_http_code', code),\n        'retry_after_s'     => call('parse_retry_after', headers),\n        'correlation_id'    => corr_id,\n        'remote_request_id' => remote_id,\n        'attempt'           => attempt,\n        'duration_ms'       => duration_ms,\n        'url' "}
{"id":"n_390bd043b12321b7","kind":"method","name":"normalize_read_index_datapoints","fqname":"connector.Vertex AI/methods.methods/method.normalize_read_index_datapoints","loc":{"line":2971,"column":37,"length":704,"begin":130095,"end":130799},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"   normalize_read_index_datapoints: lambda do |resp, _input|\n      # Expected Vertex shape: { \"datapoints\": [ { \"datapointId\": \"...\", \"featureVector\": [...],\n      #   \"restricts\": [...], \"numericRestricts\": [...], \"crowdingTag\": {...}, \"embeddingMetadata\": {...} } ] }\n      dps = Array(resp['datapoints']).map do |d|\n        {\n          'datapoint_id'      => d['datapointId'] || d['id'],\n          'feature_vector'    => Array(d['featureVector']).map(&:to_f),\n          'restricts'         => d['restricts'],\n          'numeric_restricts' => d['numericRestricts'],\n          'crowding_tag'      => d['crowdingTag'],\n          'embedding_metadata'=> d['embeddingMetadata']\n        }.compact\n      end\n "}
{"id":"n_8c91b21d7ef44653","kind":"method","name":"normalize_safety_settings","fqname":"connector.Vertex AI/methods.methods/method.normalize_safety_settings","loc":{"line":2988,"column":31,"length":1080,"begin":130865,"end":131945},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ings\n    normalize_safety_settings: lambda do |input|\n      # Accepts either the new array shape or the legacy hash; returns array\n      if input.is_a?(Array)\n        # non-destructive copy with only supported keys\n        return input.map do |r|\n          {\n            'category'  => r['category']  || r[:category],\n            'threshold' => r['threshold'] || r[:threshold],\n            'method'    => r['method']    || r[:method]\n          }.compact\n        end\n      end\n\n      # Legacy object: { harassment: 'BLOCK_...', hate_speech: 'BLOCK_...', ... }\n      if input.is_a?(Hash)\n        map = {\n          'harassment'          => 'HARM_CATEGORY_HARASSMENT',\n          'hate_speech'         => 'HARM_CATEGORY_HATE_SPEECH',\n          'sexually_explicit'   => 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n          'dangerous_content'   => 'HARM_CATEGORY_DANGEROUS_CONTENT'\n        }\n        return input.each_with_object([]) do |(k, v), arr|\n          next if v.nil? || v.to_s.strip.empty?\n          cat = map[k.to_s]\n          arr << { 'category' => cat, 'threshold' => v } if cat\n   "}
{"id":"n_5096942698d58393","kind":"method","name":"parse_retry_after","fqname":"connector.Vertex AI/methods.methods/method.parse_retry_after","loc":{"line":3021,"column":23,"length":257,"begin":131995,"end":132252},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"RETRY HELPER\n    parse_retry_after: lambda do |headers|\n      return nil unless headers\n      ra = headers['Retry-After'] || headers['retry-after']\n      return nil if ra.nil? || ra.to_s.strip.empty?\n      # integer seconds only (safe & simple)\n      ra.to_"}
{"id":"n_56d620eb5198952b","kind":"method","name":"qualify_resource","fqname":"connector.Vertex AI/methods.methods/method.qualify_resource","loc":{"line":3030,"column":22,"length":415,"begin":132357,"end":132772},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" caller input\n    qualify_resource: lambda do |connection, type, value|\n      return value if value.to_s.start_with?('projects/')\n      project = connection['project']\n      region  = connection['region']\n      case type.to_s\n      when 'index'          then \"projects/#{project}/locations/#{region}/indexes/#{value}\"\n      when 'index_endpoint' then \"projects/#{project}/locations/#{region}/indexEndpoints/#{value}"}
{"id":"n_9a2ec3495dfd9ed2","kind":"method","name":"resolve_model_version","fqname":"connector.Vertex AI/methods.methods/method.resolve_model_version","loc":{"line":3043,"column":27,"length":613,"begin":132936,"end":133549},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"on style\n    resolve_model_version: lambda do |connection, short|\n      return short if short.to_s.match?(/(-\\d{3,}|@\\d{3,})$/)\n\n      cache_key = \"model_resolve:#{short}\"\n      if (cached = call('memo_get', cache_key)); return cached; end\n\n      ids = Array(call('list_publisher_models', connection))\n              .map { |m| (m['name'] || '').split('/').last }\n              .select { |id| id.start_with?(\"#{short}-\") || id.start_with?(\"#{short}@\") }\n\n      latest = ids.max_by { |id| id[/[-@](\\d+)$/, 1].to_i }\n      chosen = latest || short  # fall back to alias if no numeric\n      call('memo_put', cache_key"}
{"id":"n_a34a2384d8963081","kind":"method","name":"retryable_http_code","fqname":"connector.Vertex AI/methods.methods/method.retryable_http_code","loc":{"line":3060,"column":25,"length":78,"begin":133596,"end":133674},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"TRY HELPER\n    retryable_http_code: lambda { |code|\n      [408, 429, 500, 502,"}
{"id":"n_04795dcea6e17a00","kind":"method","name":"safe_mean","fqname":"connector.Vertex AI/methods.methods/method.safe_mean","loc":{"line":3064,"column":15,"length":120,"begin":133692,"end":133812},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"?(code.to_i)\n    },\n\n    safe_mean: lambda do |arr|\n      xs = Array(arr).compact\n      return nil if xs.empty?\n      xs"}
{"id":"n_29a9c281083943f9","kind":"method","name":"select_model","fqname":"connector.Vertex AI/methods.methods/method.select_model","loc":{"line":3071,"column":18,"length":2188,"begin":133861,"end":136049},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"l selection logic\n    select_model: lambda do |behavior_def, cfg, input|\n      # 0) Respect explicit model in input\n      # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n      if call('value_present', input['model']) || call('value_present', input['model_override'])\n        return call('value_present', input['model']) ? input['model'] : input['model_override']\n      end\n\n      mode      = (input['model_mode'] || cfg.dig(:models, :mode) || 'auto').to_s\n      strategy  = (cfg.dig(:models, :strategy) || 'balanced').to_s\n      supported = Array(behavior_def[:supported_models]).compact\n      default   = cfg.dig(:models, :default)\n\n      # Prefer an item if supported, else first supported, else default\n      prefer = lambda do |*candidates|\n        # Choose the first candidate that is in 'supported'; else first supported; else default\n        c = candidates.flatten.compact.find { |m| supported.include?(m) }\n        c || supported.first || default\n      end\n\n      case mode\n      when 'connection'\n        # Only honor connection default if it's supported by this behavior\n        return default if supported.include?(default)\n        return supported.first || default\n      when 'explicit'\n        # If user chose 'explicit' but didn't supply a model, pick a safe supported default\n        return supported.first || default\n      else # 'auto'\n        if behavior_def[:capability].to_s == 'embedding'\n          case strategy\n          when 'cost'        then prefer.call('textembedding-gecko', 'text-embedding-005', 'text-embedding-004')\n          when 'performance' then prefer.call('gemini-embedding-001', 'text-embedding-005', 'textembedding-gecko', 'text-embedding-004')\n          else                    prefer.call('text-embedding-005', 'gemini-embedding-001', 'textembedding-gecko', 'text-embedding-004')\n          end\n        else\n          case strategy\n          when 'cost'        then prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n          when 'performance' then prefer.call('gemini-1.5-pro',   'gemini-1.5-flash')\n          else                    prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n       "}
{"id":"n_e206a19f2b6e8cd0","kind":"method","name":"telemetry_envelope_fields","fqname":"connector.Vertex AI/methods.methods/method.telemetry_envelope_fields","loc":{"line":3116,"column":31,"length":325,"begin":136131,"end":136456},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" ===\n    telemetry_envelope_fields: lambda do\n      [\n        { name: 'success', type: 'boolean' },\n        { name: 'timestamp', type: 'datetime' },\n        { name: 'metadata', type: 'object', properties: [\n          { name: 'operation' }, { name: 'model' }\n        ]},\n        { name: 'trace', type: 'object', properties: ca"}
{"id":"n_28b5a4f546a28f4f","kind":"method","name":"trace_fields","fqname":"connector.Vertex AI/methods.methods/method.trace_fields","loc":{"line":3126,"column":18,"length":525,"begin":136476,"end":137001},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"\n      ]\n    end,\n    trace_fields: lambda do\n      [\n        { name: 'correlation_id' },\n        { name: 'duration_ms', type: 'integer' },\n        { name: 'attempt', type: 'integer' },\n        { name: 'http_status', type: 'integer' },\n        { name: 'remote_request_id' }, \n        { name: 'rate_limit', type: 'object', properties: [\n          { name: 'rpm', type: 'integer' },\n          { name: 'count', type: 'integer' },\n          { name: 'reset_in_s', type: 'integer' },\n          { name: 'window_started_at', type: 'da"}
{"id":"n_c63923f62e968f33","kind":"method","name":"to_query","fqname":"connector.Vertex AI/methods.methods/method.to_query","loc":{"line":3143,"column":14,"length":579,"begin":137060,"end":137639},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"ry schema helpers===\n\n    to_query: lambda do |params|\n      encode = lambda do |s|\n        # RFC3986 unreserved: ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n        s.to_s.bytes.map { |b|\n          if (48..57).cover?(b) || (65..90).cover?(b) || (97..122).cover?(b) || [45,46,95,126].include?(b)\n            b.chr\n          else\n            \"%%%02X\" % b\n          end\n        }.join\n      end\n\n      params.flat_map do |k, v|\n        key = encode.call(k)\n        if v.is_a?(Array)\n          v.map { |e| \"#{key}=#{encode.call(e)}\" }\n        else\n          \"#{key}=#{encode.call(v)}\"\n   "}
{"id":"n_a4e7fd6a15c44f51","kind":"method","name":"value_present","fqname":"connector.Vertex AI/methods.methods/method.value_present","loc":{"line":3166,"column":19,"length":174,"begin":137725,"end":137899},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"eated as absent)\n    value_present: lambda do |v|\n      return false if v.nil?\n      return false if v.is_a?(String) && v.strip.empty?\n      return false if v.respond_to?(:em"}
{"id":"n_09b095a563e872cc","kind":"method","name":"vector_search_base","fqname":"connector.Vertex AI/methods.methods/method.vector_search_base","loc":{"line":3174,"column":24,"length":653,"begin":138011,"end":138664},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":"n provided.\n    vector_search_base: lambda do |connection, input|\n      host = (input['endpoint_host'] || connection['vector_search_endpoint']).to_s.strip\n      v = connection['version']\n      version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n\n      if host.empty?\n        # Fallback to regional API host (works for admin ops; query should use public vdb host)\n        \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n      elsif host.include?('vdb.vertexai.goog')\n        \"https://#{host}/#{version}\"\n      else\n        # Allow passing a full https://... custom host\n        host = host.sub(%r{\\Ahttps?://}i, '')\n        \"https://#"}
{"id":"n_fc47d3c26be29eec","kind":"method","name":"wrap_embeddings_vectors_v1","fqname":"connector.Vertex AI/methods.methods/method.wrap_embeddings_vectors_v1","loc":{"line":3193,"column":32,"length":787,"begin":138855,"end":139642},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" fx\n    wrap_embeddings_vectors_v1: lambda do |response, input|\n      raw = if response.is_a?(Hash) && response.key?('result')\n        response['result']\n      else\n        response\n      end\n\n      arr = Array(raw).map { |v| Array(v).map(&:to_f) }\n      norms = arr.map { |v| Math.sqrt(v.reduce(0.0) { |s, x| s + (x.to_f * x.to_f) }) }\n      dim   = arr.first ? arr.first.length : nil\n\n      out = {\n        'embeddings' => arr.map { |v| { 'values' => v } },      # <-- new, pillâ€‘friendly\n        'vectors'    => arr.map { |v| { 'feature_vector' => v } },\n        'count'      => arr.length,\n        'dimension'  => dim,\n        'norms'      => norms,\n        'avg_norm'   => call('safe_mean', norms)\n      }.compact\n\n      out['_trace'] = response['_trace'] if response.is_a?(Hash) &"}
{"id":"n_b1794f2e8f430e96","kind":"method","name":"wrap_embeddings_vectors","fqname":"connector.Vertex AI/methods.methods/method.wrap_embeddings_vectors","loc":{"line":3218,"column":29,"length":1884,"begin":139713,"end":141597},"file":null,"keys":[],"http":{"verbs":[],"endpoints":[]},"text":" testing\n    wrap_embeddings_vectors: lambda do |response, input|\n      # Extract raw embeddings from response\n      raw = if response.is_a?(Hash) && response.key?('result')\n        response['result']\n      else\n        response\n      end\n      \n      # Ensure we have an array of embeddings\n      embeddings_array = []\n      \n      if raw.nil? || (raw.is_a?(Array) && raw.empty?)\n        # Return empty structure if no embeddings\n        return {\n          'embeddings' => [],\n          'vectors'    => [],\n          'count'      => 0,\n          'dimension'  => 0,\n          'norms'      => [],\n          'avg_norm'   => 0\n        }.merge(response.is_a?(Hash) && response['_trace'] ? { '_trace' => response['_trace'] } : {})\n      end\n      \n      # Normalize to array of arrays\n      if raw.is_a?(Array)\n        if raw.first.is_a?(Numeric)\n          # Single embedding as flat array\n          embeddings_array = [raw]\n        else\n          # Multiple embeddings\n          embeddings_array = raw\n        end\n      else\n        # Unexpected format - wrap in array\n        embeddings_array = [Array(raw)]\n      end\n      \n      # Convert to floats and calculate norms\n      arr = embeddings_array.map { |v| \n        Array(v).map { |x| x.to_f rescue 0.0 }  # Safe conversion with fallback\n      }.reject { |v| v.empty? }\n      \n      norms = arr.map { |v| \n        Math.sqrt(v.reduce(0.0) { |s, x| s + (x * x) })\n      }\n      \n      dim = arr.first ? arr.first.length : 0\n      \n      out = {\n        'embeddings' => arr.map { |v| { 'values' => v } },\n        'vectors'    => arr.map { |v| { 'feature_vector' => v } },\n        'count'      => arr.length,\n        'dimension'  => dim,\n        'norms'      => norms,\n        'avg_norm'   => call('safe_mean', norms) || 0\n      }\n      \n      # Preserve trace if present\n      out['_trace'] = response['_trace'] if response.is_a?(Hash) &"}
{"id":"n_93381d7943e64a02","kind":"object_definitions","name":"object_definitions","fqname":"connector.Vertex AI/object_definitions.object_definitions","loc":{"line":3452,"column":22,"length":1145,"begin":147249,"end":148394},"file":null,"keys":[],"http":{},"text":"=================\n  object_definitions: {\n    generation_config: {\n      fields: lambda do |connection|\n        [\n          { name: 'temperature', type: 'number', hint: 'Controls randomness (0-1)', group: 'Generation options' },\n          { name: 'max_tokens', type: 'integer', hint: 'Maximum response length', group: 'Generation options' },\n          { name: 'top_p', type: 'number', hint: 'Nucleus sampling', group: 'Generation options' },\n          { name: 'top_k', type: 'integer', hint: 'Top-k sampling' , group: 'Generation options' },\n          { name: 'stop_sequences', type: 'array', of: 'string', hint: 'Stop generation at these sequences', group: 'Generation options' }\n        ]\n      end\n    },\n    safety_settings: {\n      fields: lambda do |_connection|\n        [\n          { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n          { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n          { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true,\n            hint: 'Optional; defaults to model b"}
{"id":"n_71a5bd1994c5e13b","kind":"object_definition","name":"generation_config","fqname":"connector.Vertex AI/object_definitions.object_definitions/object_definition.generation_config","loc":{"line":3453,"column":23,"length":641,"begin":147274,"end":147915},"file":null,"keys":[],"http":{},"text":"t_definitions: {\n    generation_config: {\n      fields: lambda do |connection|\n        [\n          { name: 'temperature', type: 'number', hint: 'Controls randomness (0-1)', group: 'Generation options' },\n          { name: 'max_tokens', type: 'integer', hint: 'Maximum response length', group: 'Generation options' },\n          { name: 'top_p', type: 'number', hint: 'Nucleus sampling', group: 'Generation options' },\n          { name: 'top_k', type: 'integer', hint: 'Top-k sampling' , group: 'Generation options' },\n          { name: 'stop_sequences', type: 'array', of: 'string', hint: 'Stop generation at these sequences', group: 'Generat"}
{"id":"n_b1e0d892cd66d17b","kind":"object_definition","name":"safety_settings","fqname":"connector.Vertex AI/object_definitions.object_definitions/object_definition.safety_settings","loc":{"line":3464,"column":21,"length":452,"begin":147938,"end":148390},"file":null,"keys":[],"http":{},"text":"]\n      end\n    },\n    safety_settings: {\n      fields: lambda do |_connection|\n        [\n          { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n          { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n          { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true,\n            hint: 'Optional; defaults to mod"}
{"id":"n_44aa1fb45b95b3e7","kind":"actions","name":"actions","fqname":"connector.Vertex AI/actions.actions","loc":{"line":249,"column":11,"length":26745,"begin":10633,"end":37378},"file":null,"keys":[],"http":{},"text":"ns: {\n\n    # ------ UNIVERSAL ACTIONS ----------------------------------\n    # Batch Operation\n    batch_operation: {\n      title: 'UNIVERSAL - Batch AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'batchable_behaviors', optional: false },\n        { name: 'batch_strategy', label: 'Batch Strategy', control_type: 'select', default: 'count', options: [['By Count', 'count'], ['By Token Limit', 'tokens']] },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens â‰ˆ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n      end,\n      # SAMPLE\n      sample_output: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n      end\n    },\n    # Vertex Operation\n    vertex_operation: {\n      title: 'UNIVERSAL - Vertex AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'available_behaviors', optional: false, extends_schema: true,\n          hint: 'Select the AI operation to perform' },\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select',\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          default: 'auto', optional: false, sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.'\n        },\n        # @note PATCH 2025-10-01-D removed ngIf, field behavior is configuration driven\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', sticky: true, \n          optional: true, extends_schema: true, pick_list: 'models_dynamic_for_behavior',  pick_list_params: { behavior: 'behavior' },\n          toggle_hint: 'Select from list', toggle_field: {\n            name: 'model', label: 'Model (custom id)',\n            type: 'string', control_type: 'text',\n            optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n          control_type: 'checkbox', group: 'Model & tuning',\n          hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        behavior = cfg['behavior']\n        behavior ? call('get_behavior_input_fields', behavior, cfg['advanced_config'], cfg) : []\n      end,\n      # OUTPUT\n      output_fields: lambda do |_object_definitions, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('telemetry_envelope_fields') + (call('get_behavior_output_fields', cfg['behavior']) || [])\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior     = config_fields['behavior']\n        user_config  = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input   = call('deep_copy', input) # do NOT mutate Workatoâ€™s input\n\n        # Leave advanced fields in safe_input; pipeline reads only what it needs\n        call('execute_behavior', connection, behavior, safe_input, user_config)\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, config_fields|\n        behavior = (config_fields.is_a?(Hash) ? config_fields : {})['behavior']\n        case behavior\n        when 'text.generate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.generate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hello world.\" }\n        when 'text.translate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.translate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hola mundo.\" }\n        when 'text.summarize'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.summarize\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Concise summary.\", \"word_count\"=>2 }\n        when 'text.classify'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.classify\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"category\"=>\"Support\", \"confidence\"=>0.98 }\n        when 'text.embed'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.embed\", \"model\"=>\"text-embedding-004\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"embeddings\"=>[[0.01,0.02,0.03]] }\n        when 'multimodal.analyze'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"multimodal.analyze\", \"model\"=>\"gemini-1.5-pro\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"The image shows a tabby cat on a desk.\" }\n        else\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"unknown\", \"model\"=>\"gemini-1.5-flash\" } }\n        end\n      end\n    },\n\n    # ------ THIN WRAPPERS --------------------------------------\n    # --- Index discovery\n    discover_index_config: {\n      title: 'VECTOR SEARCH - Discover index configuration',\n      description: 'Reads IndexEndpoint and Index to determine distance metrics and feature normalization',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        [\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false }\n        ]\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'index', label: 'Index resource' },\n          { name: 'distance_metric' },\n          { name: 'feature_norm_type' }\n        ]\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input|\n        safe = call('deep_copy', input)\n        disc = call('discover_index_config', connection, safe)\n        call('enrich_response',\n          response: disc,\n          metadata: { 'operation' => 'vector.discover_config', 'model' => 'n/a' }\n        )\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n          \"metadata\"=>{ \"operation\"=>\"vector.discover_config\", \"model\"=>\"n/a\" },\n          \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>12, \"attempt\"=>1 },\n          \"index\"=>\"projects/.../locations/us-central1/indexes/123\",\n          \"distance_metric\"=>\"COSINE_DISTANCE\",\n          \"feature_norm_type\"=>\"UNIT_L2_NORM\"\n        }\n      end\n    },\n    # --- Text\n    classify_text: {\n      title: 'AI - Classify Text',\n      description: 'Classify text into one of the provided categories',\n\n      # CONFIG\n      config_fields: [\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        # @note PATCH 2025-10-01-D removed ngIf, field behavior is configuration driven\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select',\n          sticky: true, optional: true, extends_schema: true, pick_list: 'models_dynamic_for_behavior',\n          toggle_hint: 'Select from list', toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }},\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.classify', cfg['advanced_config'], cfg)\n      end,\n\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.classify')\n      end,\n\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input)\n        call('execute_behavior', connection, 'text.classify', safe, user_cfg)\n      end,\n\n      # SAMPLE \n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"text.classify\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"category\"  => \"Support\",\n          \"confidence\"=> 0.98\n        }\n      end\n    },\n    generate_text: {\n      title: 'AI - Generate Text',\n      description: 'Gemini text generation',\n\n      # CONFIG\n      config_fields: [\n        { name: 'prompt_mode', label: 'Prompt mode', control_type: 'select', default: 'simple', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Simple (text prompt)',       'simple'],\n            ['Structured (contents array)', 'contents'],\n            ['Raw JSON payload',           'raw_json']\n          ],\n          hint: 'Structured modes let you pass a pre-built Vertex request (useful with RAG).' },\n\n        # --- Standardized model selector (unified with universal op) ---\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)',  'explicit'],\n            ['Use connection default',         'connection']\n          ],\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        # @note PATCH 2025-10-01-D removed ngIf, field behavior is configuration driven\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', sticky: true, optional: true, extends_schema: true,\n          pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: 'text.generate' }, toggle_hint: 'Select from list', \n          toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning',\n          hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg       = config_fields.is_a?(Hash) ? config_fields : {}\n        mode      = (cfg['prompt_mode'] || 'simple').to_s\n        show_adv  = !!cfg['advanced_config']\n\n        case mode\n        when 'contents'\n          fields = [\n            # Prompt structure\n            { name: 'contents', label: 'Contents', type: 'array', of: 'ob"}
{"id":"n_d147c2a163b2bb83","kind":"action","name":"batch_operation","fqname":"connector.Vertex AI/actions.actions/action.batch_operation","loc":{"line":253,"column":21,"length":4210,"begin":10745,"end":14955},"file":null,"keys":["config_fields","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"on: {\n      title: 'UNIVERSAL - Batch AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'batchable_behaviors', optional: false },\n        { name: 'batch_strategy', label: 'Batch Strategy', control_type: 'select', default: 'count', options: [['By Count', 'count'], ['By Token Limit', 'tokens']] },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens â‰ˆ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n      end,\n      # SAMPLE\n      sample_output: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n      end"}
{"id":"n_92c7f30afd71dba6","kind":"action","name":"vertex_operation","fqname":"connector.Vertex AI/actions.actions/action.vertex_operation","loc":{"line":355,"column":22,"length":5118,"begin":15002,"end":20120},"file":null,"keys":["config_fields","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"tion: {\n      title: 'UNIVERSAL - Vertex AI Operation',\n      # CONFIG\n      config_fields: [\n        { name: 'behavior', label: 'Operation Type', control_type: 'select', pick_list: 'available_behaviors', optional: false, extends_schema: true,\n          hint: 'Select the AI operation to perform' },\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select',\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          default: 'auto', optional: false, sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.'\n        },\n        # @note PATCH 2025-10-01-D removed ngIf, field behavior is configuration driven\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', sticky: true, \n          optional: true, extends_schema: true, pick_list: 'models_dynamic_for_behavior',  pick_list_params: { behavior: 'behavior' },\n          toggle_hint: 'Select from list', toggle_field: {\n            name: 'model', label: 'Model (custom id)',\n            type: 'string', control_type: 'text',\n            optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n          control_type: 'checkbox', group: 'Model & tuning',\n          hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        behavior = cfg['behavior']\n        behavior ? call('get_behavior_input_fields', behavior, cfg['advanced_config'], cfg) : []\n      end,\n      # OUTPUT\n      output_fields: lambda do |_object_definitions, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('telemetry_envelope_fields') + (call('get_behavior_output_fields', cfg['behavior']) || [])\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior     = config_fields['behavior']\n        user_config  = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input   = call('deep_copy', input) # do NOT mutate Workatoâ€™s input\n\n        # Leave advanced fields in safe_input; pipeline reads only what it needs\n        call('execute_behavior', connection, behavior, safe_input, user_config)\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, config_fields|\n        behavior = (config_fields.is_a?(Hash) ? config_fields : {})['behavior']\n        case behavior\n        when 'text.generate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.generate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hello world.\" }\n        when 'text.translate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.translate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hola mundo.\" }\n        when 'text.summarize'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.summarize\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Concise summary.\", \"word_count\"=>2 }\n        when 'text.classify'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.classify\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"category\"=>\"Support\", \"confidence\"=>0.98 }\n        when 'text.embed'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.embed\", \"model\"=>\"text-embedding-004\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"embeddings\"=>[[0.01,0.02,0.03]] }\n        when 'multimodal.analyze'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"multimodal.analyze\", \"model\"=>\"gemini-1.5-pro\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"The image shows a tabby cat on a desk.\" }\n        else\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"unknown\", \"model\"=>\"gemini-1.5-flash\" } }\n        end\n      e"}
{"id":"n_7533d161968112b3","kind":"action","name":"discover_index_config","fqname":"connector.Vertex AI/actions.actions/action.discover_index_config","loc":{"line":444,"column":27,"length":1610,"begin":20242,"end":21852},"file":null,"keys":["description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"config: {\n      title: 'VECTOR SEARCH - Discover index configuration',\n      description: 'Reads IndexEndpoint and Index to determine distance metrics and feature normalization',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        [\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false }\n        ]\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'index', label: 'Index resource' },\n          { name: 'distance_metric' },\n          { name: 'feature_norm_type' }\n        ]\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input|\n        safe = call('deep_copy', input)\n        disc = call('discover_index_config', connection, safe)\n        call('enrich_response',\n          response: disc,\n          metadata: { 'operation' => 'vector.discover_config', 'model' => 'n/a' }\n        )\n      end,\n      # SAMPLE\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n          \"metadata\"=>{ \"operation\"=>\"vector.discover_config\", \"model\"=>\"n/a\" },\n          \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>12, \"attempt\"=>1 },\n          \"index\"=>\"projects/.../locations/us-central1/indexes/123\",\n          \"distance_metric\"=>\"COSINE_DISTANCE\",\n          \"feature_norm_type\"=>\"UNIT_L2_NORM\"\n        }\n      e"}
{"id":"n_eecaaabaf36357ce","kind":"action","name":"classify_text","fqname":"connector.Vertex AI/actions.actions/action.classify_text","loc":{"line":484,"column":19,"length":2645,"begin":21888,"end":24533},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"y_text: {\n      title: 'AI - Classify Text',\n      description: 'Classify text into one of the provided categories',\n\n      # CONFIG\n      config_fields: [\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          sticky: true, extends_schema: true, # forces input_fields to re-render when changed\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        # @note PATCH 2025-10-01-D removed ngIf, field behavior is configuration driven\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select',\n          sticky: true, optional: true, extends_schema: true, pick_list: 'models_dynamic_for_behavior',\n          toggle_hint: 'Select from list', toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }},\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.classify', cfg['advanced_config'], cfg)\n      end,\n\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.classify')\n      end,\n\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input)\n        call('execute_behavior', connection, 'text.classify', safe, user_cfg)\n      end,\n\n      # SAMPLE \n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"text.classify\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"category\"  => \"Support\",\n          \"confidence\"=> 0.98\n        }\n      e"}
{"id":"n_fcdab0af46f7bbb0","kind":"action","name":"generate_text","fqname":"connector.Vertex AI/actions.actions/action.generate_text","loc":{"line":535,"column":19,"length":6542,"begin":24554,"end":31096},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"e_text: {\n      title: 'AI - Generate Text',\n      description: 'Gemini text generation',\n\n      # CONFIG\n      config_fields: [\n        { name: 'prompt_mode', label: 'Prompt mode', control_type: 'select', default: 'simple', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Simple (text prompt)',       'simple'],\n            ['Structured (contents array)', 'contents'],\n            ['Raw JSON payload',           'raw_json']\n          ],\n          hint: 'Structured modes let you pass a pre-built Vertex request (useful with RAG).' },\n\n        # --- Standardized model selector (unified with universal op) ---\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false, sticky: true, extends_schema: true,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)',  'explicit'],\n            ['Use connection default',         'connection']\n          ],\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.' },\n        # @note PATCH 2025-10-01-D removed ngIf, field behavior is configuration driven\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', sticky: true, optional: true, extends_schema: true,\n          pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: 'text.generate' }, toggle_hint: 'Select from list', \n          toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning',\n          hint: 'Resolves alias (e.g., gemini-1.5-pro) to highest numeric rev at runtime.' },\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg       = config_fields.is_a?(Hash) ? config_fields : {}\n        mode      = (cfg['prompt_mode'] || 'simple').to_s\n        show_adv  = !!cfg['advanced_config']\n\n        case mode\n        when 'contents'\n          fields = [\n            # Prompt structure\n            { name: 'contents', label: 'Contents', type: 'array', of: 'object', group: 'Prompt structure', optional: false,\n              properties: [\n                { name: 'role', label: 'Role', control_type: 'select',\n                  options: [['User','user'], ['Model','model']], optional: true },\n                { name: 'parts', label: 'Parts', type: 'array', of: 'object', properties: [\n                  { name: 'text',        label: 'Text' },\n                  { name: 'inline_data', label: 'Inline data', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'data',      label: 'Base64 data', control_type: 'text-area' }\n                  ]},\n                  { name: 'file_data',   label: 'File data (URI)', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'file_uri',  label: 'File URI' }\n                  ]}\n                ]}\n              ]\n            }\n          ]\n          if show_adv\n            fields += [\n              { name: 'system', label: 'System instruction', control_type: 'text-area', group: 'Advanced' },\n              { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n                properties: [\n                  { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                  { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                  { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n                ]\n              },\n              { name: 'response_mime_type', label: 'Response MIME type', group: 'Advanced',\n                hint: 'e.g., application/json for JSON mode' },\n              { name: 'response_schema', label: 'Response schema (object)', type: 'object', group: 'Advanced',\n                hint: 'When set, a compatible response_mime_type is required' },\n              { name: 'temperature', label: 'Temperature', type: 'number', group: 'Advanced', hint: '0.0 to 1.0' },\n              { name: 'max_tokens',  label: 'Max Tokens',  type: 'integer', group: 'Advanced' },\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        when 'raw_json'\n          fields = [\n            { name: 'payload_json', label: 'Full request JSON', control_type: 'text-area',\n              optional: false, group: 'Prompt (raw JSON)',\n              hint: 'Paste the entire models.generateContent request body including contents[].' }\n          ]\n          if show_adv\n            fields += [\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        else # 'simple'\n          call('get_behavior_input_fields', 'text.generate', show_adv, cfg)\n        end\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.generate')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg   = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input = call('deep_copy', input)\n        # Make prompt mode visible to the pipeline selector without mutating recipe input\n        safe_input['prompt_mode'] = config_fields['prompt_mode'] || 'simple'\n        call('execute_behavior', connection, 'text.generate', safe_input, user_cfg)\n      end,\n      # SAMPLE OUT\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\" => true, \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\" => { \"operation\" => \"text.generate\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\" => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"result\" => \"Hello world.\"\n        }\n      e"}
{"id":"n_a2583dc40c2af269","kind":"action","name":"find_neighbors","fqname":"connector.Vertex AI/actions.actions/action.find_neighbors","loc":{"line":655,"column":20,"length":690,"begin":31142,"end":31832},"file":null,"keys":["description","execute","input_fields","output_fields","title"],"http":{"verbs":[],"endpoints":[]},"text":"ghbors: {\n      title: 'VECTOR SEARCH - Find nearest neighbors',\n      description: 'Query a deployed Vector Search index',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.find_neighbors', true)\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.find_neighbors')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        safe = call('deep_copy', input)\n        call('execute_behavior', connection, 'vector.find_neighbors', safe)\n      e"}
{"id":"n_62f562d1e9b48fae","kind":"action","name":"read_index_datapoints","fqname":"connector.Vertex AI/actions.actions/action.read_index_datapoints","loc":{"line":672,"column":27,"length":1883,"begin":31861,"end":33744},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"points: {\n      title: 'VECTOR SEARCH - Read datapoints (vectors) by ID',\n      description: 'Fetch stored vectors and metadata for specific datapoint IDs from a deployed index',\n      # CONFIG\n      config_fields: [\n        { name: 'id_source', label: 'ID source', control_type: 'select', optional: false,\n          options: [\n            ['Auto (accept any)', 'auto'],\n            ['Manual IDs',        'manual'],\n            ['Neighbors array',   'neighbors'],\n            ['kâ€‘NN groups',       'groups']\n          ],\n          default: 'auto', sticky: true, extends_schema: true,\n          hint: 'Controls which input fields are shown for datapoint IDs.'\n        }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, cfg|\n        call('get_behavior_input_fields', 'vector.read_datapoints', true, cfg)\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.read_datapoints')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, cfg|\n        safe = call('deep_copy', input)\n        # Pass the chosen mode to the behavior without mutating the original input\n        safe['id_source'] = cfg['id_source'] if cfg['id_source']\n        call('execute_behavior', connection, 'vector.read_datapoints', safe)\n      end,\n      # SAMPLE OUT\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"vector.read_datapoints\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 12, \"attempt\" => 1 },\n          \"datapoints\"=> [\n            { \"datapoint_id\" => \"dp_000001\", \"feature_vector\" => [0.01, 0.02, 0.03] }\n          ]\n        }\n     "}
{"id":"n_0cf81029740fe18e","kind":"action","name":"upsert_index_datapoints","fqname":"connector.Vertex AI/actions.actions/action.upsert_index_datapoints","loc":{"line":716,"column":29,"length":713,"begin":33775,"end":34488},"file":null,"keys":["description","execute","input_fields","output_fields","title"],"http":{"verbs":[],"endpoints":[]},"text":"tapoints: {\n      title: 'VECTOR SEARCH - Upsert index datapoints',\n      description: 'Add or update datapoints in a Vector Search index',\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.upsert_datapoints', true)\n      end,\n      # OUTPUT\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.upsert_datapoints')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        safe = call('deep_copy', input)\n        call('execute_behavior', connection, 'vector.upsert_datapoints', safe)\n     "}
{"id":"n_e7d49a58d3c77f2f","kind":"action","name":"generate_embeddings","fqname":"connector.Vertex AI/actions.actions/action.generate_embeddings","loc":{"line":734,"column":25,"length":2838,"begin":34536,"end":37374},"file":null,"keys":["config_fields","description","execute","input_fields","output_fields","sample_output","title"],"http":{"verbs":[],"endpoints":[]},"text":"beddings: {\n      title: 'VECTOR SEARCH - Generate embeddings',\n      description: 'Create dense embeddings for text',\n      # CONFIG\n      config_fields: [\n        { name: 'model_mode', label: 'Model selection', group: 'Model & tuning', control_type: 'select', default: 'auto', optional: false,\n          options: [\n            ['Auto (use connection strategy)', 'auto'],\n            ['Explicit (choose model below)', 'explicit'],\n            ['Use connection default',        'connection'] ],\n          hint: 'Switch to Explicit to pick an exact Vertex model for this step.', sticky: true, extends_schema: true },\n        { name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, extends_schema: true,\n          pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: 'text.embed' }, toggle_hint: 'Select from list', \n          toggle_field: {  name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' } },\n        { name: 'lock_model_revision', label: 'Lock to latest numbered revision', control_type: 'checkbox', group: 'Model & tuning'},\n        { name: 'advanced_config', label: 'Show Advanced Configuration', control_type: 'checkbox', extends_schema: true, optional: true, default: false }\n      ],\n      # INPUT\n      input_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.embed', cfg['advanced_config'], cfg)\n      end,\n      # OUTPUT\n      # @note PATCH 2025-10-01-A routed output fields to get_behavior_output_fields\n      output_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.embed')\n      end,\n      # EXECUTE\n      execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input) # one copy at the action boundary\n        call('execute_behavior', connection, 'text.embed', safe, user_cfg)\n      end,\n      # SAMPLE OUT\n      sample_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"text.embed\", \"model\" => \"text-embedding-005@latest\" },\n          \"trace\"     => { \"correlation_id\" => \"abc123\", \"duration_ms\" => 21, \"attempt\" => 1 },\n          \"embeddings\"=> [ { \"values\" => [0.01, 0.02, 0.03] } ],\n          \"vectors\"   => [ { \"feature_vector\" => [0.01, 0.02, 0.03] } ],\n          \"count\"     => 1,\n          \"dimension\" => 768,\n          \"avg_norm\"  => 1.0,\n          \"norms\"     => [1.0]\n        }\n     "}
{"id":"n_d33cf8ca3eaec9b1","kind":"triggers","name":"triggers","fqname":"connector.Vertex AI/triggers.triggers","loc":{"line":3479,"column":12,"length":2,"begin":148552,"end":148554},"file":null,"keys":[],"http":{},"text":"=="}
{"id":"n_820de3b11de4a776","kind":"pick_lists","name":"pick_lists","fqname":"connector.Vertex AI/pick_lists.pick_lists","loc":{"line":3285,"column":14,"length":5307,"begin":141764,"end":147071},"file":null,"keys":[],"http":{},"text":"=======================\n  pick_lists: {\n\n    all_models: lambda do |connection|\n      [\n        ['Gemini 1.5 Flash', 'gemini-1.5-flash'],\n        ['Gemini 1.5 Pro', 'gemini-1.5-pro'],\n        ['Gemini Embedding 001',  'gemini-embedding-001'],\n        ['Text Embedding 005',    'text-embedding-005'],\n        ['Text Embedding 004',    'text-embedding-004'],\n        ['Text Embedding Gecko', 'textembedding-gecko']\n      ]\n    end,\n    available_behaviors: lambda do |connection|\n      behaviors = call('behavior_registry')\n      behaviors.map do |key, config|\n        [config[:description], key]\n      end.sort_by { |label, _| label }\n    end,\n    batchable_behaviors: lambda do |connection|\n      behaviors = call('behavior_registry')\n      behaviors.select { |_, config| \n        config[:features]&.include?('batching') \n      }.map { |key, config|\n        [config[:description], key]\n      }\n    end,\n    # @note PATCH 2025-10-01-D completed embedding task types\n    embedding_tasks: lambda do |_connection|\n      [\n        ['Document Retrieval', 'RETRIEVAL_DOCUMENT'],\n        ['Query Retrieval', 'RETRIEVAL_QUERY'],\n        ['Semantic Similarity', 'SEMANTIC_SIMILARITY'],\n        ['Classification', 'CLASSIFICATION'],\n        ['Clustering', 'CLUSTERING'],\n        ['Question Answering (query side)', 'QUESTION_ANSWERING'],\n        ['Fact Verification (query side)',  'FACT_VERIFICATION'],\n        ['Code Retrieval (Java/Python)',    'CODE_RETRIEVAL_QUERY']\n      ]\n    end,\n\n    gcp_regions: lambda do |connection|\n      [\n        ['US Central 1', 'us-central1'],\n        ['US East 1', 'us-east1'],\n        ['US East 4', 'us-east4'],\n        ['US West 1', 'us-west1'],\n        ['US West 4', 'us-west4']\n      ]\n    end,\n\n    languages: lambda do |connection|\n      [\n        ['Auto-detect', 'auto'],\n        ['English', 'en'],\n        ['Spanish', 'es'],\n        ['French', 'fr'],\n        ['German', 'de'],\n        ['Italian', 'it'],\n        ['Portuguese', 'pt'],\n        ['Japanese', 'ja'],\n        ['Korean', 'ko'],\n        ['Chinese (Simplified)', 'zh-CN'],\n        ['Chinese (Traditional)', 'zh-TW']\n      ]\n    end,\n\n    models_for_behavior: lambda do |connection, input = {}|\n      behavior = input['behavior']\n      defn = call('behavior_registry')[behavior]\n\n      if defn && defn[:supported_models]\n        defn[:supported_models].map do |model|\n          [model.split('-').map!(&:capitalize).join(' '), model]\n        end\n      else\n        []\n      end\n    end,\n\n    # @note PATCH 2025-10-01-A exposed gemini-embedding-* in dynamic pick list when behavior is embeddings\n    models_dynamic_for_behavior: lambda do |connection, input = {}|\n      behavior = input['behavior']\n      prefixes = if behavior.to_s == 'text.embed'\n        ['text-embedding-', 'textembedding-', 'gemini-embedding-']\n      else\n        ['gemini-'] # last\n      end\n      items = []\n      begin\n        items = call('list_publisher_models', connection)\n          .map { |m| id = (m['name'] || '').split('/').last; [m['displayName'] || id, id] }\n          .select { |_label, id| prefixes.any? { |p| id.start_with?(p) } }\n          .sort_by { |_label, id| - (id[/(\\d+)$/, 1].to_i) } # still works for hyphen & @ suffixes\n      rescue\n        items = [] # fall through to fallback\n      end\n\n      if items.empty?\n        # Minimal, safe fallback to keep the UI usable before a connection is fully ready.\n        items = if prefixes.first == 'gemini-'\n          [['Gemini 1.5 Flash', 'gemini-1.5-flash'], ['Gemini 1.5 Pro', 'gemini-1.5-pro']]\n        else\n          [\n            ['Text Embedding 005',     'text-embedding-005'],\n            ['Text Embedding 004',     'text-embedding-004'],\n            ['Gemini Embedding 001',   'gemini-embedding-001'],\n            ['Text Embedding Gecko',   'textembedding-gecko']\n          ]\n        end\n      end\n      items\n    end,\n\n    safety_categories: lambda do |_connection|\n      [\n        ['Harassment',           'HARM_CATEGORY_HARASSMENT'],\n        ['Hate speech',          'HARM_CATEGORY_HATE_SPEECH'],\n        ['Sexually explicit',    'HARM_CATEGORY_SEXUALLY_EXPLICIT'],\n        ['Dangerous content',    'HARM_CATEGORY_DANGEROUS_CONTENT']\n      ]\n    end,\n\n    safety_levels: lambda do |_connection|\n      [\n        ['Block none',   'BLOCK_NONE'],\n        ['Block low',    'BLOCK_LOW'],\n        ['Block medium', 'BLOCK_MEDIUM'],\n        ['Block high',   'BLOCK_HIGH']\n      ]\n    end,\n\n    safety_thresholds: lambda do |_connection|\n      [\n        ['Block none',              'BLOCK_NONE'],\n        ['Block only high',         'BLOCK_ONLY_HIGH'],\n        ['Block medium and above',  'BLOCK_MEDIUM_AND_ABOVE'],\n        ['Block low and above',     'BLOCK_LOW_AND_ABOVE']\n      ]\n    end,\n\n    safety_methods: lambda do |_connection|\n      [\n        ['By severity',    'SEVERITY'],\n        ['By probability', 'PROBABILITY']\n      ]\n    end,\n  \n    vector_distance_metrics: lambda do |_|\n      [\n        ['Cosine distance (1 - cos_sim)', 'COSINE_DISTANCE'],\n        ['Dot-product distance (âˆ’dot)',   'DOT_PRODUCT_DISTANCE'],\n        ['Squared L2 (Euclidean^2)',      'SQUARED_L2_DISTANCE'],\n        ['L1 (Manhattan)',                'L1_DISTANCE']\n      ]\n    end,\n\n    vector_feature_norm_types: lambda do |_|\n      [\n        ['Unit L2 norm', 'UNIT_L2_NORM'],\n        ['No"}
{"id":"lam:0ec79456810b30cd","kind":"lambda","name":"test","fqname":"connector#test","loc":{"line":224,"column":8,"length":758,"begin":9669,"end":10427},"file":"connector.rb","text":": lambda do |connection|\n    project = connection['project']\n    region  = connection['region']\n\n    # 1) Token + API enablement (global catalog)\n    call('list_publisher_models', connection) # raises normalized errors\n\n    # 2) Regional reachability / permissions\n    host = (region.to_s == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n    url = call('build_endpoint_url', connection,\n      { 'custom_path' => \"https://#{host}/#{connection['version']}/projects/{project}/locations/{region}/endpoints\" },\n      {}\n    )\n    call('http_request', connection, method: 'GET', url: url, headers: call('build_headers', connection))\n\n    true\n  rescue => e\n    # Keep the normalized, compact message asâ€‘is\n    error(e.message)\n "}
{"id":"lam:e3a702ec83ae52f6","kind":"lambda","name":"method","fqname":"method:build_payload","loc":{"line":791,"column":19,"length":9158,"begin":37647,"end":46805},"file":"connector.rb","text":"_payload: lambda do |template:, variables:, format:|\n      case format\n      \n      # Direct\n      when 'direct'\n        variables\n      # Template\n      when 'template'\n        result = template.dup\n        variables.each { |k, v| result = result.gsub(\"{#{k}}\", v.to_s) }\n        result\n      # Vertex prompt\n      when 'vertex_prompt'\n        payload = {\n          'contents' => [{\n            'role'  => 'user',\n            'parts' => [{ 'text' => call('apply_template', template, variables) }]\n          }],\n          'generationConfig' => call('build_generation_config', variables)\n        }.compact\n\n        # Variables\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        # @note PATCH 2025-10-01-C post-normalized safety settings \n        sys = variables['system']\n        payload['systemInstruction'] = { 'parts' => [{ 'text' => sys }] } if sys && !sys.to_s.strip.empty?\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        if call('value_present', variables['response_mime_type']) || call('value_present', variables['response_schema'])\n          gc = (payload['generationConfig'] ||= {})\n          gc['responseMimeType'] = variables['response_mime_type'] if call('value_present', variables['response_mime_type'])\n          gc['responseSchema']   = variables['response_schema']     if call('value_present', variables['response_schema'])\n        end\n\n        payload['labels'] = variables['labels'] if call('value_present', variables['labels'])\n        payload\n\n      when 'vertex_contents'\n        contents = Array(variables['contents']).map do |c|\n          role = c['role'] || c[:role] || 'user'\n          parts = Array(c['parts']).map do |p|\n            if p['text'] || p[:text]\n              { 'text' => p['text'] || p[:text] }\n            elsif p['inline_data'] || p[:inline_data] || p['inlineData']\n              src = p['inline_data'] || p[:inline_data] || p['inlineData']\n              { 'inlineData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'data'     => src['data'] || src[:data]\n                }.compact\n              }\n            elsif p['file_data'] || p[:file_data] || p['fileData']\n              src = p['file_data'] || p[:file_data] || p['fileData']\n              { 'fileData' => {\n                  'mimeType' => src['mime_type'] || src[:mime_type] || src['mimeType'],\n                  'fileUri'  => src['file_uri']  || src[:file_uri]  || src['fileUri']\n                }.compact\n              }\n            else\n              {} # ignored\n            end\n          end.compact\n\n          { 'role' => role, 'parts' => parts }\n        end\n\n        payload = {\n          'contents'         => contents,\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        # Variables\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        # @note PATCH 2025-10-01-C post-normalized safety settings \n        if call('value_present', variables['system'])\n          payload['systemInstruction'] = { 'parts' => [{ 'text' => variables['system'] }] }\n        end\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        gc = (payload['generationConfig'] ||= {})\n        gc['responseMimeType'] = variables['response_mime_type'] if call('value_present', variables['response_mime_type'])\n        gc['responseSchema']   = variables['response_schema']     if call('value_present', variables['response_schema'])\n\n        payload['labels'] = variables['labels'] if call('value_present', variables['labels'])\n        payload\n      when 'vertex_passthrough'\n        src = variables['payload'] || variables['payload_json'] || variables['fully_formed'] || variables['request_json']\n        obj =\n          if src.is_a?(String)\n            begin\n              JSON.parse(src)\n            rescue\n              corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n              error(\"Invalid payload_json (must be valid JSON object). [corr_id=#{corr}]\")\n            end\n          else\n            src\n          end\n        error('payload_json must be a JSON object') unless obj.is_a?(Hash)\n        obj\n\n      # Embedding\n      when 'embedding'\n        # Normalize and filter\n        texts = Array(variables['texts']).map { |t| t.to_s.strip }.reject(&:empty?)\n        error('No non-empty texts provided') if texts.empty?\n\n        # Enforce model-aware limits\n        model_id = variables['model'].to_s\n        max_per_request =\n          if model_id.start_with?('text-embedding-005') then 100\n          else 100 # safe default for older models as well\n          end\n        if texts.length > max_per_request\n          corr = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          error(\"Too many texts for a single request (#{texts.length} > #{max_per_request}). Split the batch or use Batch AI Operation. [corr_id=#{corr}]\")\n        end\n\n        # Task/type normalization\n        task_type = variables['task_type'] || 'RETRIEVAL_DOCUMENT'\n        include_title = (task_type == 'RETRIEVAL_DOCUMENT')\n\n        # Build instances\n        body = {\n          'instances' => texts.map { |text|\n            inst = { 'content' => text, 'task_type' => task_type }\n            inst['title'] = variables['title'] if include_title && variables['title']\n            inst\n          }\n        }\n\n        # Parameters\n        params = {}\n        supports_dimensionality = model_id.start_with?('text-embedding-005')\n        supports_auto_truncate  = supports_dimensionality || model_id.start_with?('textembedding-gecko')\n\n        if supports_auto_truncate && call('value_present', variables['auto_truncate'])\n          params['autoTruncate'] = variables['auto_truncate']\n        end\n        if supports_dimensionality && call('value_present', variables['output_dimensionality'])\n          params['outputDimensionality'] = variables['output_dimensionality']\n        end\n        body['parameters'] = params unless params.empty?\n\n        body\n\n      # Vector search\n      when 'find_neighbors'\n        queries = Array(variables['queries']).map do |q|\n          dp =\n            if q['feature_vector']\n              { 'featureVector' => Array(q['feature_vector']).map(&:to_f) }\n            elsif q['vector'] # alias\n              { 'featureVector' => Array(q['vector']).map(&:to_f) }\n            elsif q['datapoint_id']\n              { 'datapointId' => q['datapoint_id'] }\n            else\n              {}\n            end\n\n          {\n            'datapoint'         => dp,\n            'neighborCount'     => (q['neighbor_count'] || variables['neighbor_count'] || 10).to_i,\n            'restricts'         => q['restricts'],\n            'numericRestricts'  => q['numeric_restricts'] # keep input snake_case; map to camel here\n          }.compact\n        end\n\n        {\n          'deployedIndexId'     => variables['deployed_index_id'],\n          'queries'             => queries,\n          'returnFullDatapoint' => variables['return_full_datapoint']\n        }.compact\n\n      when 'upsert_datapoints'\n        datapoints =\n          if Array(variables['datapoints']).any?\n            Array(variables['datapoints']).map do |d|\n              {\n                'datapointId'      => d['datapoint_id'] || d['id'],\n                'featureVector'    => Array(d['feature_vector'] || d['vector']).map(&:to_f),\n                'sparseEmbedding'  => d['sparse_embedding'],\n                'restricts'        => d['restricts'],\n                'numericRestricts' => d['numeric_restricts'],\n                'crowdingTag'      => d['crowding_tag'],\n                'embeddingMetadata'=> d['embedding_metadata']\n              }.compact\n            end\n          elsif Array(variables['embeddings']).any?\n            call('coerce_embeddings_to_datapoints', variables)\n          else\n            []\n          end\n\n        { 'datapoints' => datapoints }\n      when 'read_index_datapoints'\n        ids = call('extract_ids_for_read', variables)\n        { 'deployedIndexId' => variables['deployed_index_id'], 'ids' => ids }\n      # Multimodal\n      when 'multimodal'\n        parts = []\n        parts << { 'text' => variables['text'] } if variables['text']\n        if variables['images']\n          variables['images'].each do |img|\n            parts << { 'inlineData' => { 'mimeType' => img['mime_type'] || 'image/jpeg', 'data' => img['data'] } }\n          end\n        end\n\n        payload = {\n          'contents' => [{ 'role' => 'user', 'parts' => parts }],\n          'generationConfig' => call('build_generation_config', variables)\n        }\n\n        norm = call('normalize_safety_settings', variables['safety_settings'])\n        payload['safetySettings'] = norm unless norm.nil? || (norm.respond_to?(:empty?) && norm.empty?)\n\n        payload\n        \n      else\n        variables\n      e"}
{"id":"lam:50a63ee24e50b740","kind":"lambda","name":"method","fqname":"method:enrich_response","loc":{"line":1019,"column":21,"length":1193,"begin":46935,"end":48128},"file":"connector.rb","text":"response: lambda do |response:, metadata: {}|\n      base  = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'result' => response }\n      trace = base.delete('_trace') || (base['result'].is_a?(Hash) ? base['result'].delete('_trace') : nil)\n\n      # Preserve success if caller provided it; otherwise assume true\n      success = base.key?('success') ? base['success'] : true\n\n      # Build a uniform trace object (always present)\n      trace_hash  = trace.is_a?(Hash) ? trace : {}\n      final_trace = {\n        'correlation_id'    => trace_hash['correlation_id'] || SecureRandom.hex(8),\n        'duration_ms'       => (trace_hash['duration_ms'] || 0).to_i,\n        'attempt'           => (trace_hash['attempt'] || 1).to_i,\n        'http_status'       => trace_hash['http_status'],\n        'remote_request_id' => trace_hash['remote_request_id'],\n        'rate_limit'        => trace_hash['rate_limit']\n      }.compact\n\n      base.merge(\n        'success'   => success,\n        'timestamp' => base['timestamp'] || Time.now.utc.iso8601,\n        'metadata'  => { 'operation' => metadata['operation'], 'model' => metadata['model'] }.compact,\n        'trace'     => final_trace\n      ).compa"}
{"id":"lam:a051a2d57abea0b7","kind":"lambda","name":"method","fqname":"method:extract_response","loc":{"line":1046,"column":22,"length":2238,"begin":48183,"end":50421},"file":"connector.rb","text":"response: lambda do |data:, path: nil, format: 'raw'|\n      case format\n      # RAW\n      when 'raw' then data\n      # JSON_FIELD\n      when 'json_field'\n        return data unless path\n        path.split('.').reduce(data) { |acc, seg| acc.is_a?(Array) && seg =~ /^\\d+$/ ? acc[seg.to_i] : (acc || {})[seg] }\n      # VERTEX_TEXT\n      when 'vertex_text'\n        parts = data.dig('candidates', 0, 'content', 'parts') || []\n        text  = parts.select { |p| p['text'] }.map { |p| p['text'] }.join\n        text.empty? ? data.dig('predictions', 0, 'content').to_s : text\n      \n      # VERTEX_JSON\n      when 'vertex_json'\n        raw = (data.dig('candidates', 0, 'content', 'parts') || []).map { |p| p['text'] }.compact.join\n        return {} if raw.nil? || raw.empty?\n        m = raw.match(/```(?:json)?\\s*(\\{.*?\\})\\s*```/m) || raw.match(/\\{.*\\}/m)\n        m ? (JSON.parse(m[1] || m[0]) rescue {}) : {}\n\n      # EMBEDDINGS  \n      when 'embeddings'\n        # Normalize all known Vertex shapes to an array of Float arrays\n        preds = Array(data['predictions'])\n\n        vectors = preds.map do |p|\n          next p if p.is_a?(Array) && p.all? { |x| x.is_a?(Numeric) } # raw numeric array\n\n          next unless p.is_a?(Hash)\n          v = nil\n\n          emb = p['embeddings'] || p['embedding']\n\n          # Preferred: { \"embeddings\": { \"values\": [...] } }\n          if emb.is_a?(Hash) && emb['values'].is_a?(Array)\n            v = emb['values']\n\n          # Sometimes: { \"embeddings\": [ { \"values\": [...] } ] }\n          elsif emb.is_a?(Array) && emb.first.is_a?(Hash) && emb.first['values'].is_a?(Array)\n            v = emb.first['values']\n\n          # Legacy: { \"embeddings\": [ ...numbers... ] } OR { \"embedding\": [ ...numbers... ] }\n          elsif emb.is_a?(Array) && emb.first.is_a?(Numeric)\n            v = emb\n\n          # Fallbacks occasionally seen in older/experimental endpoints\n          elsif p['denseEmbedding'].is_a?(Array)\n            v = p['denseEmbedding']\n          elsif p['values'].is_a?(Array)\n            v = p['values']\n          end\n\n          # Only accept a clean numeric vector\n          (v.is_a?(Array) && v.all? { |x| x.is_a?(Numeric) }) ? v : nil\n        end.compact\n\n        vectors\n      else data\n      e"}
{"id":"lam:8966ec35f3327563","kind":"lambda","name":"method","fqname":"method:http_request","loc":{"line":1109,"column":18,"length":3959,"begin":50475,"end":54434},"file":"connector.rb","text":"_request: lambda do |connection, method:, url:, payload: nil, headers: {}, retry_config: {}, request_format: 'json'|\n      max_attempts = (retry_config['max_attempts'] || retry_config['max_retries'] || 3).to_i\n      base_backoff = (retry_config['backoff'] || 1.0).to_f\n      retry_on     = Array(retry_config['retry_on'] || [408, 429, 500, 502, 503, 504]).map(&:to_i)\n      do_not_retry = Array(retry_config['do_not_retry']).map(&:to_i)\n\n      attempt = 0\n      last_error = nil\n\n      while attempt < max_attempts\n        attempt += 1\n        begin\n          hdrs = (headers || {}).dup\n          corr = hdrs['X-Correlation-Id'] ||= \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n          started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n          last_error = nil\n\n          req = case method.to_s.upcase\n                when 'GET'    then get(url)\n                when 'POST'   then post(url, payload)\n                when 'PUT'    then put(url, payload)\n                when 'DELETE' then delete(url)\n                else error(\"Unsupported HTTP method: #{method}\")\n                end\n\n          # Respect application/x-www-form-urlencoded when requested\n          if request_format.to_s == 'form'\n            hdrs['Content-Type'] ||= 'application/x-www-form-urlencoded'\n            req = req.request_format_www_form_urlencoded\n          end\n\n          response =\n            req.headers(hdrs)\n              .after_error_response(/.*/) { |code, body, rheaders, message|\n                 dur_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n                 err = call('normalize_http_error',\n                            connection,\n                            code: code, body: body, headers: (rheaders || {}),\n                            message: message, url: url, corr_id: corr, attempt: attempt, duration_ms: dur_ms)\n                 last_error = err\n                 error(call('format_user_error', err))\n               }\n              .after_response { |code, body, rheaders|\n                # Always return a Hash payload with HTTP metadata, even when the API returns a raw string/bytes.\n                payload = body.is_a?(Hash) ? body : { 'raw' => body }\n                payload['_http'] = { 'status' => code, 'headers' => rheaders }\n                payload\n              }\n\n          duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n          out = response.is_a?(Hash) ? JSON.parse(JSON.dump(response)) : { 'raw' => response }\n\n          # Extract a useful Google/Vertex request id when present.\n          hdrs_out = (out.dig('_http', 'headers') || {})\n          rid = hdrs_out['x-request-id'] || hdrs_out['x-cloud-trace-context'] || hdrs_out['x-guploader-uploadid']\n\n          out['_trace'] = {\n            'correlation_id'    => corr,\n            'duration_ms'       => duration_ms,\n            'attempt'           => attempt,\n            'http_status'       => out.dig('_http','status'),\n            'remote_request_id' => rid\n          }.compact\n\n          return out\n\n        rescue => _e\n          code = last_error ? last_error['code'].to_i : 0\n          retryable = last_error ? last_error['retryable'] : retry_on.include?(code)\n          retryable &&= !do_not_retry.include?(code)\n\n          # Break if not retryable or out of attempts\n          break unless retryable && attempt < max_attempts\n\n          # Retry-After (seconds) takes precedence when present\n          delay =\n            if last_error && last_error['retry_after_s'].to_i > 0\n              last_error['retry_after_s'].to_i\n            else\n              # exp backoff with small jitter\n              (base_backoff * (2 ** (attempt - 1))).to_f + rand * 0.25\n            end\n\n          sleep(delay)\n        end\n      end\n\n      # Exhausted: bubble the last normalized message if present\n      msg = last_error ? call('format_user_error', last_error) : 'HTTP request failed'\n      error(ms"}
{"id":"lam:53cc3d747e319b4c","kind":"lambda","name":"method","fqname":"method:transform_data","loc":{"line":1202,"column":20,"length":1056,"begin":54487,"end":55543},"file":"connector.rb","text":"orm_data: lambda do |input:, from_format:, to_format:, connection: nil|\n      case \"#{from_format}_to_#{to_format}\"\n      when 'url_to_base64'\n        # Use centralized http_request for retries and telemetry\n        resp = call('http_request', connection, method: 'GET', url: input, headers: {})\n        raw  = resp['raw'] || resp.to_s\n        require 'base64'\n        Base64.strict_encode64(raw.to_s)\n      when 'base64_to_bytes'\n        require 'base64'\n        Base64.decode64(input.to_s)\n      when 'language_code_to_name'\n        names = {\n          'en'=>'English','es'=>'Spanish','fr'=>'French','de'=>'German','it'=>'Italian','pt'=>'Portuguese',\n          'ja'=>'Japanese','ko'=>'Korean','zh-CN'=>'Chinese (Simplified)','zh-TW'=>'Chinese (Traditional)'\n        }\n        return 'auto-detected' if input == 'auto'\n        names[input] || input\n      when 'categories_to_text'\n        input.map { |c| \"#{c['name']}: #{c['description']}\" }.join(\"\\n\")\n      when 'distance_to_similarity'\n        1.0 - (input.to_f / 2.0)\n      else\n        input\n      e"}
{"id":"lam:02b5b63a7ca65e38","kind":"lambda","name":"method","fqname":"method:validate_input","loc":{"line":1230,"column":20,"length":4327,"begin":55597,"end":59924},"file":"connector.rb","text":"te_input: lambda do |data:, schema: [], constraints: []|\n      errors = []\n      \n      # Schema validation\n      schema.each do |field|\n        field_name = field['name']\n        field_value = data[field_name]\n        \n        # Required check\n        # @note PATCH 2025-10-01-C updated to treat [] {} as missing for required fields\n        if field['required'] && !call('value_present', field_value)\n          errors << \"#{field_name} is required\"\n        end\n        \n        # Length validation\n        if field['max_length'] && field_value.to_s.length > field['max_length']\n          errors << \"#{field_name} exceeds maximum length of #{field['max_length']}\"\n        end\n        \n        # Pattern validation\n        if field['pattern'] && field_value && !field_value.match?(Regexp.new(field['pattern']))\n          errors << \"#{field_name} format is invalid\"\n        end\n      end\n      \n      # Constraint validation\n      constraints.each do |constraint|\n        ctype = (constraint['type'] || constraint[:type]).to_s\n\n        case ctype\n        when 'min_value'\n          value = data[(constraint['field'] || constraint[:field]).to_s].to_f\n          if value < constraint['value'].to_f\n            errors << \"#{constraint['field'] || constraint[:field]} must be at least #{constraint['value']}\"\n          end\n\n        when 'max_items'\n          field = (constraint['field'] || constraint[:field]).to_s\n          items = data[field] || []\n          if Array(items).size > constraint['value'].to_i\n            errors << \"#{field} cannot exceed #{constraint['value']} items\"\n          end\n\n        # XOR/ONE-OF across fields (root or per-item scope)\n        when 'xor', 'one_of'\n          scope   = (constraint['scope'] || constraint[:scope]).to_s # e.g., 'queries[]' or ''\n          fields  = Array(constraint['fields'] || constraint[:fields]).map(&:to_s)\n          aliases = (constraint['aliases'] || constraint[:aliases] || {}) # { 'feature_vector' => ['vector'] }\n          exactly_one = (ctype == 'xor') || (constraint['exactly_one'] == true)\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            count = 0\n            fields.each do |f|\n              keys = [f] + Array(aliases[f] || aliases[f.to_sym]).map(&:to_s)\n              present = keys.any? { |k| call('value_present', ctx[k]) }\n              count += 1 if present\n            end\n\n            if exactly_one\n              if count != 1\n                display = fields.map { |f|\n                  al = Array(aliases[f] || aliases[f.to_sym])\n                  al.any? ? \"#{f} (alias: #{al.join(', ')})\" : f\n                }.join(', ')\n                errors << \"#{label}: exactly one of #{display} must be provided\"\n              end\n            else\n              if count < 1\n                errors << \"#{label}: at least one of #{fields.join(', ')} must be provided\"\n              end\n            end\n          end\n\n        # Conditional required with root-level fallback and optional default\n        # Example: each queries[].neighbor_count is optional if top-level neighbor_count is present,\n        # or if a default is defined; else required.\n        when 'fallback_required', 'conditional_required'\n          scope    = (constraint['scope'] || constraint[:scope]).to_s       # e.g., 'queries[]'\n          field    = (constraint['field'] || constraint[:field]).to_s       # e.g., 'neighbor_count'\n          fallback = (constraint['fallback_to_root'] || constraint[:fallback_to_root]).to_s # e.g., 'neighbor_count'\n          default_ok = constraint.key?('default_if_absent') || constraint.key?(:default_if_absent)\n\n          root_has_fallback = fallback.empty? ? false : call('value_present', data[fallback])\n\n          call('each_in_scope', data, scope).each do |ctx, label|\n            item_has = call('value_present', ctx[field])\n            unless item_has || root_has_fallback || default_ok\n              if fallback.empty?\n                errors << \"#{label}.#{field} is required\"\n              else\n                errors << \"#{label}.#{field} is required when top-level #{fallback} is not provided\"\n              end\n            end\n          end\n\n        else\n          # unknown constraint type: ignore silently (forward-compatible)\n        end\n      end\n      \n      error(errors.join('; ')) if errors.any?\n      tr"}
{"id":"lam:6b41fce340f0885a","kind":"lambda","name":"method","fqname":"method:with_resilience","loc":{"line":1335,"column":21,"length":2187,"begin":59977,"end":62164},"file":"connector.rb","text":"silience: lambda do |operation:, config: {}, task: {}, connection: nil, &blk|\n      # Rate limiting (per-job) â€” always initialize and use a unique name\n      rate_limit_info = nil\n      if config['rate_limit']\n        rate_limit_info = call('check_rate_limit', operation, config['rate_limit'])\n      end\n\n      circuit_key   = \"circuit_#{operation}\"\n      circuit_state = call('memo_get', circuit_key) || { 'failures' => 0 }\n      error(\"Circuit breaker open for #{operation}. Too many recent failures.\") if circuit_state['failures'] >= 5\n\n      begin\n        result =\n          if blk\n            # Instrument the block path so trace is still present\n            corr    = \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\"\n            started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n            raw     = blk.call\n            dur_ms  = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n            out     = raw.is_a?(Hash) ? JSON.parse(JSON.dump(raw)) : { 'result' => raw }\n            out['_trace'] ||= {}\n            out['_trace'].merge!({ 'correlation_id' => corr, 'duration_ms' => dur_ms, 'attempt' => 1 })\n            out\n          else\n            error('with_resilience requires a task hash with url/method') unless task.is_a?(Hash) && task['url']\n\n            call('http_request',\n              connection,\n              method:       (task['method'] || 'GET'),\n              url:          task['url'],\n              payload:      task['payload'],\n              headers:      (task['headers'] || {}),\n              retry_config: (task['retry_config'] || {})\n            )\n          end\n\n        # Attach rate-limit counters to trace if present (guarded)\n        if rate_limit_info && result.is_a?(Hash)\n          result['_trace'] ||= {}\n          result['_trace']['rate_limit'] = rate_limit_info\n        end\n\n        # Reset circuit on success\n        call('memo_put', circuit_key, { 'failures' => 0 }, 300)\n        result\n\n      rescue => e\n        circuit_state['failures'] += 1\n        call('memo_put', circuit_key, circuit_state, 300)\n        # Keep normalized messages intact; do not blanket-retry non-retryables here\n        raise e\n     "}
{"id":"lam:b2980738d9a0b097","kind":"lambda","name":"method","fqname":"method:execute_pipeline","loc":{"line":1391,"column":22,"length":3528,"begin":62255,"end":65783},"file":"connector.rb","text":"e_pipeline: lambda do |connection, operation, input, config|\n      # Recursion guard\n      @pipeline_depth ||= 0\n      @pipeline_depth += 1\n      error(\"Pipeline recursion detected!\") if @pipeline_depth > 3\n      local = input\n\n      # 1. Validate\n      if config['validate']\n        call('validate_input',\n          data:         local,\n          schema:       config['validate']['schema'] || [],\n          constraints:  config['validate']['constraints'] || []\n        )\n      end\n      \n      # 2. Transform input\n      if config['transform_input']\n        config['transform_input'].each do |field, transform|\n          if local[field]\n            local[field] = call('transform_data',\n              input:        local[field],\n              from_format:  transform['from'],\n              to_format:    transform['to'],\n              connection:   connection\n            )\n          end\n        end\n      end\n\n      # -- Ensure selected model from ops config is visible to URL builder\n      local['model'] = config['model'] unless call('value_present', local['model'])\n\n      # 3. Build payload\n      payload = if config['payload']\n        call('build_payload',\n          template:   config['payload']['template'] || '',\n          variables:  local.merge('system' => config['payload']['system']),\n          format:     config['payload']['format'] || 'direct'\n        )\n      else\n        local\n      end\n      \n      # 4. Build URL\n      endpoint  = config['endpoint'] || {}\n      url       = call('build_endpoint_url', connection, endpoint, local)\n      \n      # 5. Execute with resilience\n      response = call('with_resilience',\n        operation:  operation,\n        config:     (config['resilience'] || {}),\n        task: {\n          'method'       => endpoint['method'] || 'POST',\n          'url'          => url,\n          'payload'      => payload,\n          'headers'      => call('build_headers', connection),\n          'retry_config' => (config.dig('resilience', 'retry') || {})\n        }, connection: connection\n      )\n      \n      trace_from_response = (response.is_a?(Hash) ? response['_trace'] : nil)\n\n      # 6. Extract response\n      extracted = if config['extract']\n        call('extract_response',\n          data:   response,\n          path:   config['extract']['path'],\n          format: config['extract']['format'] || 'raw'\n        )\n      else\n        response\n      end\n      \n      # 6.5 Attach trace ASAP so post_process can preserve/propagate it\n      if trace_from_response\n        if extracted.is_a?(Hash)\n          extracted['_trace'] ||= {}\n          extracted['_trace'].merge!(trace_from_response)\n        else\n          extracted = { 'result' => extracted, '_trace' => trace_from_response }\n        end\n      end\n\n      # 7. Post-process\n      if config['post_process']\n        extracted = call(config['post_process'], extracted, local)\n      end\n\n      # 7.5 Ensure trace still present after post_process (if function dropped it)\n      if trace_from_response\n        if extracted.is_a?(Hash)\n          extracted['_trace'] ||= {}\n          # Preserve any trace the post-processor may have added; don't overwrite it\n          extracted['_trace'].merge!(trace_from_response) { |_k, old, _new| old }\n        else\n          extracted = { 'result' => extracted, '_trace' => trace_from_response }\n        end\n      end\n      \n      # 8. Enrich\n      call('enrich_response',\n        response: extracted,\n        metadata: { 'operation' => operation, 'model' => config['model'] || local['model'] }\n   "}
{"id":"lam:87e0a3cbb67faed1","kind":"lambda","name":"method","fqname":"method:behavior_registry","loc":{"line":1501,"column":23,"length":10076,"begin":65929,"end":76005},"file":"connector.rb","text":"r_registry: lambda do\n      {\n        # Text Operations\n        'text.generate' => {\n          description: 'Generate text from a prompt',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['streaming', 'caching'],\n          config_template: {\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => '{prompt}',\n              'system' => nil\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        'text.translate' => {\n          description: 'Translate text between languages',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true, 'max_length' => 10000 },\n                { 'name' => 'target_language', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'source_language' => { 'from' => 'language_code', 'to' => 'name' },\n              'target_language' => { 'from' => 'language_code', 'to' => 'name' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Translate the following text from {source_language} to {target_language}. Return only the translation:\\n\\n{text}',\n              'system' => 'You are a professional translator. Maintain tone and context.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent',\n              'method' => 'POST'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          },\n          defaults: {\n            'temperature' => 0.3,\n            'max_tokens' => 2048\n          }\n        },\n        'text.summarize' => {\n          description: 'Summarize text content',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'max_words', 'required' => false }\n              ]\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Summarize the following text in {max_words} words:\\n\\n{text}',\n              'system' => 'You are an expert at creating clear, concise summaries.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            },\n            'post_process' => 'add_word_count'\n          },\n          defaults: {\n            'temperature' => 0.5,\n            'max_words' => 200\n          }\n        },\n        'text.classify' => {\n          description: 'Classify text into categories',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-flash', 'gemini-1.5-pro'],\n          features: ['caching', 'batching'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'text', 'required' => true },\n                { 'name' => 'categories', 'required' => true }\n              ]\n            },\n            'transform_input' => {\n              'categories' => { 'from' => 'categories', 'to' => 'text' }\n            },\n            'payload' => {\n              'format' => 'vertex_prompt',\n              'template' => 'Classify this text into one of these categories:\\n{categories}\\n\\nText: {text}\\n\\nRespond with JSON: {\"category\": \"name\", \"confidence\": 0.0-1.0}',\n              'system' => 'You are a classification expert. Always return valid JSON.'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_json'\n            }\n          },\n          defaults: {\n            'temperature' => 0.1\n          }\n        },\n\n        # Embedding Operations\n        'text.embed' => {\n          description: 'Generate text embeddings',\n          capability: 'embedding',\n          supported_models: ['text-embedding-005', 'text-embedding-004', 'textembedding-gecko', 'gemini-embedding-001'],\n          features: ['batching', 'caching'],\n          # @note PATCH 2025-10-01-D aligned constraints.max_items.value with that of API\n          config_template: {\n            'validate' => {\n              'schema' => [ { 'name' => 'texts', 'required' => true } ],\n              'constraints' => [ { 'type' => 'max_items', 'field' => 'texts', 'value' => 100 } ]\n            },\n            'payload' => { 'format' => 'embedding' },\n            'endpoint' => { 'path' => ':predict', 'method' => 'POST' },\n            'extract' => { 'format' => 'embeddings' },\n            'post_process' => 'wrap_embeddings_vectors'\n          }\n        },\n        # Multimodal Operations\n        'multimodal.analyze' => {\n          description: 'Analyze images with text prompts',\n          capability: 'generation',\n          supported_models: ['gemini-1.5-pro', 'gemini-1.5-flash'],\n          features: ['streaming'],\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'prompt', 'required' => true },\n                { 'name' => 'images', 'required' => true }\n              ]\n            },\n            'payload' => {\n              'format' => 'multimodal'\n            },\n            'endpoint' => {\n              'path' => ':generateContent'\n            },\n            'extract' => {\n              'format' => 'vertex_text'\n            }\n          }\n        },\n        # Vector Operations\n        'vector.upsert_datapoints' => {\n          description: 'Upsert datapoints into a Vector Search index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index', 'required' => true }\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['datapoints', 'embeddings'] }\n              ]\n            },\n            'payload'  => { 'format' => 'upsert_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_indexes',\n              'path'   => ':upsertDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' }, # empty body on success\n            'post_process' => 'add_upsert_ack'\n          }\n        },\n        'vector.find_neighbors' => {\n          description: 'Find nearest neighbors from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'queries',           'required' => true },\n                { 'name' => 'distance_metric' },        # optional\n                { 'name' => 'feature_norm_type' },      # optional\n                { 'name' => 'include_stats' }           # optional\n              ],\n              'constraints' => [\n                # Exactly one locator per query: vector OR datapoint_id\n                {\n                  'type'   => 'xor',\n                  'scope'  => 'queries[]',\n                  'fields' => ['feature_vector', 'datapoint_id'],\n                  'aliases'=> { 'feature_vector' => ['vector'] } # honor your alias\n                },\n                # If a query omits neighbor_count, allow top-level neighbor_count or the internal default (10)\n                {\n                  'type'               => 'fallback_required',\n                  'scope'              => 'queries[]',\n                  'field'              => 'neighbor_count',\n                  'fallback_to_root'   => 'neighbor_count',\n                  'default_if_absent'  => 10  # matches your payload fallback\n                }\n              ]\n            },\n            'payload'  => { 'format' => 'find_neighbors' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':findNeighbors',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_find_neighbors'\n          }\n        },\n        'vector.read_datapoints' => {\n          description: 'Read datapoints (vectors) by ID from a deployed index',\n          capability: 'vector',\n          supported_models: [], # not model-driven\n          config_template: {\n            'validate' => {\n              'schema' => [\n                { 'name' => 'index_endpoint',    'required' => true },\n                { 'name' => 'deployed_index_id', 'required' => true },\n                { 'name' => 'ids' },        # manual ids\n                { 'name' => 'groups' },     # from find_neighbors (normalized)\n                { 'name' => 'neighbors' }   # flattened neighbors\n              ],\n              'constraints' => [\n                { 'type' => 'one_of', 'fields' => ['ids', 'groups', 'neighbors'] },\n                { 'type' => 'max_items', 'field' => 'ids', 'value' => 1000 }\n              ]\n            },\n            'payload'  => { 'format' => 'read_index_datapoints' },\n            'endpoint' => {\n              'family' => 'vector_index_endpoints',\n              'path'   => ':readIndexDatapoints',\n              'method' => 'POST'\n            },\n            'extract'  => { 'format' => 'raw' },\n            'post_process' => 'normalize_read_index_datapoints'\n          }\n        }     \n   "}
{"id":"lam:4cf5d3cd7adc8e43","kind":"lambda","name":"method","fqname":"method:configuration_registry","loc":{"line":1760,"column":28,"length":1079,"begin":76092,"end":77171},"file":"connector.rb","text":"n_registry: lambda do |connection, user_config|\n      {\n        # Model selection\n        models: {\n          default: user_config['model'] || connection['default_model'] || 'gemini-1.5-flash',\n          strategy: connection['optimization_mode'] || 'balanced',\n          mode: user_config['model_mode'] || 'auto'\n        },\n        \n        # Generation settings\n        generation: {\n          temperature: user_config['temperature'],\n          max_tokens: user_config['max_tokens'],\n          top_p: user_config['top_p'],\n          top_k: user_config['top_k']\n        }.compact,\n        \n        # Features\n        features: {\n          caching: {\n            enabled: connection['enable_caching'] != false,\n            ttl: user_config['cache_ttl'] || 300\n          },\n          logging: {\n            enabled: connection['enable_logging'] == true\n          }\n        },\n        \n        # Execution\n        execution: {\n          retry: {\n            max_attempts: 3,\n            backoff: 1.0\n          },\n          rate_limit: {\n            rpm: 60\n          }\n        }\n   "}
{"id":"lam:d016fd190a2e2d40","kind":"lambda","name":"method","fqname":"method:execute_behavior","loc":{"line":1802,"column":22,"length":3631,"begin":77253,"end":80884},"file":"connector.rb","text":"e_behavior: lambda do |connection, behavior, input, user_config = {}|\n      behavior_def = call('behavior_registry')[behavior] or error(\"Unknown behavior: #{behavior}\")\n      local_input = input # @note PATCH 2025-10-03-A assume input deep copied at action boundary\n\n      # Apply defaults without side effects\n      if behavior_def[:defaults]\n        behavior_def[:defaults].each { |k, v| local_input[k] = local_input.key?(k) ? local_input[k] : v }\n      end\n\n      # Bring model-selection keys into local_input\n      %w[model model_mode lock_model_revision].each do |k|\n        if user_config.key?(k) && !user_config[k].nil?\n          local_input[k] = user_config[k]\n        end\n      end\n\n      cfg = call('configuration_registry', connection, user_config)\n      operation_config = JSON.parse(JSON.dump(behavior_def[:config_template] || {}))\n\n      # Bring generation settings into the local input\n      if cfg[:generation]\n        cfg[:generation].each { |k, v| local_input[k] = v unless v.nil? }\n      end\n\n      operation_config['model'] = call('select_model', behavior_def, cfg, local_input)\n\n      # Force correct model if needed\n      if behavior_def[:supported_models].any? && !behavior_def[:supported_models].include?(operation_config['model'])\n        scrubbed = call('deep_copy', local_input) # @note deep_copy here, but also deleting - should resolve over alloc\n        scrubbed.delete('model')\n        scrubbed.delete('model_override')\n        operation_config['model'] = call('select_model', behavior_def, cfg, scrubbed.merge('model_mode' => 'auto'))\n      end\n\n      operation_config['resilience'] = cfg[:execution]\n\n      # Embedding\n      # Guard against global region for embeddings\n      if behavior == 'text.embed' && connection['region'].to_s == 'global'\n        error(\"Embeddings are typically not served from the 'global' location. Choose a concrete region like 'us-central1'.\")\n      end\n\n      if behavior == 'text.embed' && connection['enable_logging'] == true\n        # Log what we're about to execute\n        debug_info = {\n          'input_texts' => local_input['texts'],\n          'model' => operation_config['model'],\n          'config' => operation_config\n        }\n        \n        # Execute pipeline WITH debugging\n        result = call('execute_pipeline', connection, behavior, local_input, operation_config)\n        \n        # Check if result has actual data\n        if result.is_a?(Hash)\n          debug_result = call('deep_copy', result)\n\n          result['trace'] ||= {}\n          result['trace']['debug'] = {\n            'input'  => debug_info,\n            'output' => debug_result\n          }\n        end\n        \n        return result\n      else\n        # Normal execution for non-embedding behaviors\n        result = call('execute_pipeline', connection, behavior, local_input, operation_config)\n      end\n\n      # Add model selection trace\n      selection_mode = (local_input['model_mode'] || cfg.dig(:models, :mode) || 'auto').to_s\n      strategy = (cfg.dig(:models, :strategy) || 'balanced').to_s\n      explicit_in = user_config['model']\n\n      if result.is_a?(Hash) && result['trace'].is_a?(Hash)\n        result['trace']['model_selection'] = {\n          'mode' => selection_mode,\n          'strategy' => strategy,\n          'explicit_model' => explicit_in,\n          'effective_model' => operation_config['model']\n        }.compact\n      end\n\n      # Cache if enabled\n      if cfg[:features][:caching][:enabled]\n        cache_key = \"vertex_#{behavior}_#{local_input.to_json.hash}\"\n        call('memo_put', cache_key, result, cfg[:features][:caching][:ttl] || 300)\n      end\n\n      re"}
{"id":"lam:5b8553b978f2ce5e","kind":"lambda","name":"method","fqname":"method:debug_embedding_response","loc":{"line":1897,"column":30,"length":673,"begin":80996,"end":81669},"file":"connector.rb","text":"g_response: lambda do |data|\n      return unless ENV['VERTEX_DEBUG'] == 'true'  # Only log when debugging enabled\n      \n      if data && data['predictions']\n        pred = data['predictions'].first\n        structure = if pred.is_a?(Hash)\n          pred.keys.join(', ')\n        else\n          pred.class.name\n        end\n        puts \"DEBUG: Embedding response structure - predictions[0] keys: #{structure}\"\n        \n        if pred.is_a?(Hash) && pred['embeddings']\n          emb_structure = pred['embeddings'].is_a?(Hash) ? pred['embeddings'].keys.join(', ') : pred['embeddings'].class.name\n          puts \"DEBUG: embeddings structure: #{emb_structure}\"\n        end\n     "}
{"id":"lam:e281441a3d216925","kind":"lambda","name":"method","fqname":"method:add_upsert_ack","loc":{"line":1916,"column":20,"length":320,"begin":81721,"end":82041},"file":"connector.rb","text":"upsert_ack: lambda do |response, input|\n      # response is empty on success; return a useful ack\n      out = {\n        'ack'         => 'upserted',\n        'count'       => Array(input['datapoints']).size,\n        'index'       => input['index'],\n        'empty_body'  => (response.nil? || response == {})\n      }\n     "}
{"id":"lam:a542a3f22036f00d","kind":"lambda","name":"method","fqname":"method:add_word_count","loc":{"line":1927,"column":20,"length":290,"begin":82064,"end":82354},"file":"connector.rb","text":"word_count: lambda do |response, input|\n      if response.is_a?(String)\n        { \n          'result' => response,\n          'word_count' => response.split.size\n        }\n      else\n        {\n          'result' => response,\n          'word_count' => response.to_s.split.size\n        }\n     "}
{"id":"lam:8c2af734d3f01192","kind":"lambda","name":"method","fqname":"method:apply_template","loc":{"line":1942,"column":20,"length":238,"begin":82408,"end":82646},"file":"connector.rb","text":"y_template: lambda do |template, variables|\n      return template unless template && variables\n      \n      result = template.dup\n      variables.each do |key, value|\n        result = result.gsub(\"{#{key}}\", value.to_s)\n      end\n      re"}
{"id":"lam:e8c7d0437f787332","kind":"lambda","name":"method","fqname":"method:approx_token_count","loc":{"line":1953,"column":24,"length":107,"begin":82708,"end":82815},"file":"connector.rb","text":"oken_count: lambda do |text|\n      # Fast, side-effect-free approximation\n      ((text.to_s.length) / 4.0)."}
{"id":"lam:51e8e2f6a8bb9d99","kind":"lambda","name":"method","fqname":"method:augment_vector_context","loc":{"line":1958,"column":28,"length":769,"begin":82846,"end":83615},"file":"connector.rb","text":"or_context: lambda do |connection, local_input|\n      # @note PATCH 2025-10-03-A assume input deep copied at action boundary\n      out = local_input\n      need_metric = !call('value_present', out['distance_metric'])\n      need_norm   = !call('value_present', out['feature_norm_type'])\n      return out unless need_metric || need_norm\n\n      # Only attempt admin discovery if the connection is allowed\n      return out unless connection['allow_admin_discovery'] == true\n\n      begin\n        disc = call('discover_index_config', connection, out)\n        out['distance_metric']   ||= disc['distance_metric']\n        out['feature_norm_type'] ||= disc['feature_norm_type']\n      rescue\n        # Softâ€‘fail; confidence will be nil but neighbors still returned\n      end\n   "}
{"id":"lam:13ff11e07e6ed1f9","kind":"lambda","name":"method","fqname":"method:build_endpoint_url","loc":{"line":1979,"column":24,"length":3200,"begin":83667,"end":86867},"file":"connector.rb","text":"endpoint_url: lambda do |connection, endpoint_config, input|\n      v = connection['version']\n      api_version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n      region = connection['region']\n      base_regional = \"https://#{region}-aiplatform.googleapis.com/#{api_version}\"\n\n      family = endpoint_config['family']\n\n      case family\n      # PUBLISHER MODELS\n      when 'publisher_models'\n        api_version = (connection['version'].to_s.strip.empty? ? 'v1' : connection['version'])\n        publisher   = endpoint_config['publisher'] || 'google'\n        \"https://aiplatform.googleapis.com/#{api_version}/publishers/#{publisher}/models\"\n      # VECTOR INDEXES\n      when 'vector_indexes' # admin/data-plane ops on Index resources\n        index = call('qualify_resource', connection, 'index', input['index'] || endpoint_config['index'])\n        \"#{base_regional}/#{index}#{endpoint_config['path']}\" # e.g., ':upsertDatapoints'\n      # VECTOR INDEX ENDPOINTS\n      when 'vector_index_endpoints' # query via MatchService or admin reads\n        base =\n          if endpoint_config['admin'] == true\n            v = connection['version']; version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n            \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n          else\n            call('vector_search_base', connection, input) # uses vdb host when provided\n          end\n        ie = call('qualify_resource', connection, 'index_endpoint',\n                  input['index_endpoint'] || endpoint_config['index_endpoint'])\n        \"#{base}/#{ie}#{endpoint_config['path']}\" # e.g., ':findNeighbors' or ''\n\n\n      else\n        base_host = (region == 'global') ? 'aiplatform.googleapis.com' : \"#{region}-aiplatform.googleapis.com\"\n        base_url  = \"https://#{base_host}/#{api_version}\"\n\n        # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n        model = call('value_present', input['model']) ? input['model'] : (connection['default_model'] || 'gemini-1.5-flash')\n        model_id = model.to_s\n\n        # Honor lock model revision input flag\n        lock_rev = input['lock_model_revision'] == true || endpoint_config['require_version'] == true\n        if lock_rev && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n        # Only resolve to a numeric version when explicitly requested by endpoint config\n        if endpoint_config['require_version'] == true && !model_id.match?(/-\\d{3,}$/)\n          model_id = call('resolve_model_version', connection, model_id)\n        end\n\n        model_path = \"projects/#{connection['project']}/locations/#{region}/publishers/google/models/#{model_id}\"\n\n        # If the user supplies a custom path, replace the the critical elements with those from the connection\n        if endpoint_config['custom_path']\n          endpoint_config['custom_path']\n            .gsub('{project}',  connection['project'])\n            .gsub('{region}',   region)\n            .gsub('{endpoint}', connection['vector_search_endpoint'] || '')\n        else\n          \"#{base_url}/#{model_path}#{endpoint_config['path'] || ':generateContent'}\"\n        end\n   "}
{"id":"lam:85f117fdf5487dd8","kind":"lambda","name":"method","fqname":"method:build_generation_config","loc":{"line":2044,"column":29,"length":559,"begin":86991,"end":87550},"file":"connector.rb","text":"ation_config: lambda do |vars|\n      {\n        'temperature'     => call('value_present', vars['temperature']) ? vars['temperature'] : 0.7,\n        'maxOutputTokens' => call('value_present', vars['max_tokens'])  ? vars['max_tokens']  : 2048,\n        'topP'            => call('value_present', vars['top_p'])       ? vars['top_p']       : 0.95,\n        'topK'            => call('value_present', vars['top_k'])       ? vars['top_k']       : 40,\n        'stopSequences'   => call('value_present', vars['stop_sequences']) ? vars['stop_sequences'] : nil\n      }.c"}
{"id":"lam:2d0f9c56e3e45a6a","kind":"lambda","name":"method","fqname":"method:build_headers","loc":{"line":2055,"column":19,"length":147,"begin":87600,"end":87747},"file":"connector.rb","text":"uild_headers: lambda do |connection|\n      {\n        'Content-Type' => 'application/json',\n        'X-Goog-User-Project' => connection['project']\n "}
{"id":"lam:898278f414e4727e","kind":"lambda","name":"method","fqname":"method:check_rate_limit","loc":{"line":2063,"column":22,"length":661,"begin":87792,"end":88453},"file":"connector.rb","text":"k_rate_limit: lambda do |operation, limits|\n      rpm  = (limits['rpm'] || limits[:rpm]).to_i\n      window_id     = Time.now.to_i / 60\n      window_start  = window_id * 60\n      key           = \"rate_#{operation}_#{window_id}\"\n\n      count = call('memo_get', key) || 0\n      error(\"Rate limit exceeded for #{operation}. Please wait before retrying.\") if count >= rpm\n\n      new_count = count + 1\n      reset_in  = (window_start + 60) - Time.now.to_i\n      reset_in  = 60 if reset_in <= 0\n\n      call('memo_put', key, new_count, reset_in)\n\n      { 'rpm' => rpm, 'count' => new_count, 'reset_in_s' => reset_in, 'window_started_at' => Time.at(window_start).utc.iso"}
{"id":"lam:ea5e0e2a00b63f5b","kind":"lambda","name":"method","fqname":"method:chunk_by_tokens","loc":{"line":2081,"column":21,"length":1696,"begin":88477,"end":90173},"file":"connector.rb","text":"nk_by_tokens: lambda do |items:, token_ceiling:, max_items:, max_body_bytes: nil|\n      token_cap = token_ceiling.to_i\n      token_cap = 8000 if token_cap <= 0 # conservative fallback if not provided\n      max_items = (max_items || 100).to_i\n      max_items = 1 if max_items <= 0\n      max_body  = max_body_bytes ? max_body_bytes.to_i : nil\n\n      batches   = []\n      oversized = []\n\n      current       = []\n      current_tokens= 0\n      current_bytes = 0\n\n      # crude but steady overheads so we donâ€™t undercount request size\n      per_item_overhead = 64\n      base_overhead     = 512\n\n      items.each do |item|\n        txt = item['text'].to_s\n        t   = call('approx_token_count', txt)\n        b   = txt.bytesize + per_item_overhead\n\n        # single-item guards\n        if t > token_cap\n          oversized << { 'item' => item, 'reason' => \"estimated tokens #{t} exceed ceiling #{token_cap}\" }\n          next\n        end\n        if max_body && (b + base_overhead) > max_body\n          oversized << { 'item' => item, 'reason' => \"approx body bytes #{b + base_overhead} exceed limit #{max_body}\" }\n          next\n        end\n\n        # would adding this item break any limit?\n        if !current.empty? &&\n          (current_tokens + t > token_cap ||\n            current.length + 1 > max_items ||\n            (max_body && current_bytes + b + base_overhead > max_body))\n          batches << current\n          current        = []\n          current_tokens = 0\n          current_bytes  = 0\n        end\n\n        current << item\n        current_tokens += t\n        current_bytes  += b\n      end\n\n      batches << current unless current.empty?\n\n      { 'batches' => batches, 'oversized' => ove"}
{"id":"lam:7ac81384849315e1","kind":"lambda","name":"method","fqname":"method:coerce_embeddings_to_datapoints","loc":{"line":2136,"column":37,"length":1241,"begin":90277,"end":91518},"file":"connector.rb","text":"_to_datapoints: lambda do |vars|\n      embeddings = Array(vars['embeddings'])\n      error('No embeddings provided') if embeddings.empty?\n\n      ids     = Array(vars['datapoint_ids'])\n      prefix  = (vars['datapoint_id_prefix'] || 'dp_').to_s\n      start   = (vars['start_index'] || 1).to_i\n      pad_to  = (vars['pad_to'] || 6).to_i\n\n      if ids.empty?\n        ids = embeddings.each_index.map { |i| \"#{prefix}#{(start + i).to_s.rjust(pad_to, '0')}\" }\n      elsif ids.length != embeddings.length\n        error(\"datapoint_ids length (#{ids.length}) must match embeddings length (#{embeddings.length})\")\n      end\n\n      common_restricts        = vars['common_restricts']\n      common_numeric          = vars['common_numeric_restricts']\n      common_crowding_tag     = vars['common_crowding_tag']\n      common_embedding_meta   = vars['embedding_metadata']\n\n      embeddings.each_with_index.map do |vec, i|\n        {\n          'datapointId'       => ids[i],\n          'featureVector'     => Array(vec).map(&:to_f),\n          'restricts'         => common_restricts,\n          'numericRestricts'  => common_numeric,\n          'crowdingTag'       => common_crowding_tag,\n          'embeddingMetadata' => common_embedding_meta\n        }.compact\n "}
{"id":"lam:8176d023fb9ba437","kind":"lambda","name":"method","fqname":"method:coerce_kwargs","loc":{"line":2168,"column":19,"length":886,"begin":91540,"end":92426},"file":"connector.rb","text":" coerce_kwargs: lambda do |*args, **kwargs|\n      # Non-destructive copies\n      positional = args.dup\n      kw = kwargs.dup\n\n      # If caller passed a trailing Hash, treat it as kwargs (merged with explicit kwargs)\n      if positional.last.is_a?(Hash)\n        trailing = positional.pop\n        # deep copy to avoid side-effects\n        trailing_copy = JSON.parse(JSON.dump(trailing)) rescue trailing.dup\n        trailing_sym  = trailing_copy.each_with_object({}) do |(k, v), acc|\n          key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n          acc[key] = v\n        end\n        # Explicit kwargs take precedence\n        kw = trailing_sym.merge(kw) { |_key, left, right| right }\n      end\n\n      # Ensure symbolized keys for kwargs\n      kw = kw.each_with_object({}) do |(k, v), acc|\n        key = (k.respond_to?(:to_sym) ? k.to_sym : k)\n        acc[key] = v\n      end\n\n      [positio"}
{"id":"lam:a15941fc8589617b","kind":"lambda","name":"method","fqname":"method:confidence_from_distance","loc":{"line":2195,"column":30,"length":615,"begin":92463,"end":93078},"file":"connector.rb","text":"_from_distance: lambda do |distance, metric, feature_norm_type|\n      return nil unless distance\n      m = metric.to_s\n      case m\n      when 'COSINE_DISTANCE'\n        # distance = 1 - cos_sim  => confidence = (1 + cos_sim)/2 = 1 - distance/2\n        c = 1.0 - (distance.to_f / 2.0)\n        [[c, 0.0].max, 1.0].min\n      when 'DOT_PRODUCT_DISTANCE'\n        # distance = -dot; if vectors were UNIT_L2_NORM, dot âˆˆ [-1,1] ~ cos_sim\n        if feature_norm_type.to_s == 'UNIT_L2_NORM'\n          dot = -distance.to_f\n          c = 0.5 * (1.0 + dot)\n          [[c, 0.0].max, 1.0].min\n        end\n      else\n        nil"}
{"id":"lam:67c6801f4b33054a","kind":"lambda","name":"method","fqname":"method:deep_copy","loc":{"line":2216,"column":15,"length":43,"begin":93126,"end":93169},"file":"connector.rb","text":"ct\n    deep_copy: lambda { |obj| JSON.parse"}
{"id":"lam:d12bc82e862815d6","kind":"lambda","name":"method","fqname":"method:discover_index_config","loc":{"line":2218,"column":27,"length":1500,"begin":93199,"end":94699},"file":"connector.rb","text":"ver_index_config: lambda do |connection, input|\n      ep = call('qualify_resource', connection, 'index_endpoint', input['index_endpoint'])\n      dep_id = input['deployed_index_id'].to_s\n      return {} if ep.to_s.empty? || dep_id.empty?\n\n      cache_key = \"idxcfg:#{ep}:#{dep_id}\"\n      if (hit = call('memo_get', cache_key)); return hit; end\n\n      # 1) Read IndexEndpoint (admin host)\n      url_ep = call('build_endpoint_url', connection, {\n        'family' => 'vector_index_endpoints', 'index_endpoint' => ep, 'method' => 'GET', 'admin' => true\n      }, input)\n      ep_body = call('http_request', connection, method: 'GET', url: url_ep, headers: call('build_headers', connection))\n      deployed = Array(ep_body['deployedIndexes']).find { |d| d['id'] == dep_id }\n      return {} unless deployed && deployed['index']\n\n      # 2) Read Index (admin host)\n      url_idx = call('build_endpoint_url', connection, {\n        'family' => 'vector_indexes', 'index' => deployed['index'], 'method' => 'GET'\n      }, input)\n      idx_body = call('http_request', connection, method: 'GET', url: url_idx, headers: call('build_headers', connection))\n\n      cfg = idx_body.dig('metadata', 'config') || {}\n      out = {\n        'index'              => deployed['index'],\n        'distance_metric'    => (cfg['distanceMeasureType'] || cfg['distance_measure_type']),\n        'feature_norm_type'  => (cfg['featureNormType']     || cfg['feature_norm_type'])\n      }.compact\n\n      call('memo_put', cache_key, out, 600)"}
{"id":"lam:7eee1c0ff514f494","kind":"lambda","name":"method","fqname":"method:each_in_scope","loc":{"line":2252,"column":19,"length":273,"begin":94793,"end":95066},"file":"connector.rb","text":"   each_in_scope: lambda do |data, scope|\n      s = scope.to_s\n      if s.end_with?('[]')\n        key = s[0..-3] # strip []\n        arr = Array(data[key]) # safe\n        arr.each_with_index.map { |item, idx| [item || {}, \"#{key}[#{idx}]\"] }\n      else\n        [[data, '$']]"}
{"id":"lam:9eb4e4598ceb3771","kind":"lambda","name":"method","fqname":"method:error_hint","loc":{"line":2264,"column":16,"length":565,"begin":95104,"end":95669},"file":"connector.rb","text":"R\n    error_hint: lambda do |connection, code, status|\n      c = code.to_i\n      case c\n      when 401\n        # keep small + actionable\n        'Unauthorized. Reâ€‘authenticate; then check project/region, API enablement, and roles.'\n      when 403\n        'Forbidden. Check project/region, API enablement, and roles.'\n      when 404\n        'Not found. Check project/region (feature/model availability) and the resource id.'\n      when 429\n        'Rate limit/quota. Reduce request rate or increase quota. Will honor Retryâ€‘After when present.'\n      else\n       "}
{"id":"lam:eea2827b25415398","kind":"lambda","name":"method","fqname":"method:extract_ids_for_read","loc":{"line":2281,"column":26,"length":984,"begin":95698,"end":96682},"file":"connector.rb","text":"extract_ids_for_read: lambda do |vars|\n      mode = (vars['id_source'] || 'auto').to_s\n      pick = lambda do |source|\n        case source\n        when 'manual'\n          Array(vars['ids']).compact\n        when 'neighbors'\n          Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact\n        when 'groups'\n          Array(vars['groups'])\n            .flat_map { |g| Array(g['neighbors']) }\n            .map { |n| n['datapoint_id'] }.compact\n        else # auto: prefer manual â†’ neighbors â†’ groups\n          ids = Array(vars['ids']).compact\n          ids = Array(vars['neighbors']).map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids = Array(vars['groups']).flat_map { |g| Array(g['neighbors']) }.map { |n| n['datapoint_id'] }.compact if ids.empty?\n          ids\n        end\n      end\n\n      ids = pick.call(mode).map(&:to_s)\n      ids = ids.uniq if vars['unique'] != false\n      error('No datapoint IDs provided or derivable from neighbors/groups') if id"}
{"id":"lam:cb4cae105f1b3541","kind":"lambda","name":"method","fqname":"method:extract_user_config","loc":{"line":2308,"column":25,"length":1634,"begin":96750,"end":98384},"file":"connector.rb","text":"\n    extract_user_config: lambda do |input, cfg_enabled = false, config_ctx = {}|\n      cfg = {}\n      config_ctx ||= {}\n\n      # Prefer config_fields values; fall back to input (back-compat)\n      mode = (config_ctx['model_mode'] || input['model_mode'] || '').to_s\n      #explicit_model = config_ctx['model'] || input['model'] || input['model_override']\n      # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n      explicit_model = call('value_present', input['model']) ? input['model'] : config_ctx['model'] || input['model_override']\n\n      case mode\n      when 'explicit'\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      when 'connection', 'auto', ''\n        # no-op; use selection logic defaults\n      else\n        # unknown mode: treat as legacy explicit if model present\n        cfg['model'] = explicit_model if call('value_present', explicit_model)\n      end\n\n      cfg['model_mode']          = mode unless mode.empty?\n      # After (prefer config_fields, fall back to input for completeness)\n      if config_ctx.key?('lock_model_revision')\n        cfg['lock_model_revision'] = config_ctx['lock_model_revision']\n      elsif input.key?('lock_model_revision')\n        cfg['lock_model_revision'] = input['lock_model_revision']\n      end\n\n      # Advanced tuning (unchanged)\n      if cfg_enabled\n        cfg['temperature'] = input['temperature'] if input.key?('temperature')\n        cfg['max_tokens']  = input['max_tokens']  if input.key?('max_tokens')\n        cfg['cache_ttl']   = input['cache_ttl']   if input.key?('cache_ttl')\n      end\n"}
{"id":"lam:01df9abdf6636a34","kind":"lambda","name":"method","fqname":"method:execute_batch_behavior","loc":{"line":2347,"column":28,"length":2729,"begin":98439,"end":101168},"file":"connector.rb","text":"  execute_batch_behavior: lambda do |connection, behavior, items, batch_size, strategy, options = {}|\n      results = []\n      errors = []\n      total_processed = 0\n\n      # @note PATCH 2025-10-03-A assume input deep copied at action boundary\n      local_items = Array(items)\n      \n      # 1) Build batches according to strategy\n      batches =\n        if strategy.to_s == 'tokens'\n          chunk = call('chunk_by_tokens',\n            items: local_items,\n            token_ceiling: (options['token_ceiling'] || options[:token_ceiling]),\n            max_items: (options['max_items_per_batch'] || options[:max_items_per_batch] || 100),\n            max_body_bytes: (options['max_body_bytes'] || options[:max_body_bytes])\n          )\n          # surface oversize items as per-batch errors (unchanged error shape: batch + error)\n          Array(chunk['oversized']).each do |o|\n            errors << { 'batch' => [o['item']], 'error' => \"Skipped item: #{o['reason']}\" }\n          end\n          chunk['batches'] || []\n        else\n          size  = (batch_size || 10).to_i\n          limit = (options['max_items_per_batch'] || options[:max_items_per_batch] || size).to_i\n          size  = [[size, limit].min, 1].max\n          local_items.each_slice(size).to_a\n        end\n\n      # 2) Execute batches\n      batches.each do |batch|\n        begin\n          if behavior.include?('embed')\n            texts = batch.map { |item| item['text'] }\n\n            payload = { 'texts' => texts }\n            unique_tasks = batch.map { |i| i['task_type'] }.compact.uniq\n            payload['task_type'] = unique_tasks.first if unique_tasks.length == 1\n\n            batch_result = call('execute_behavior', connection, behavior, payload)\n\n            # For embeddings, API is truly batchable: one result per batch (keep prior shape)\n            results.concat([batch_result])\n            total_processed += batch.length\n\n          else\n            # Non-embeddings: execute per-item so partial failures are surfaced\n            batch.each do |item|\n              begin\n                item_result = call('execute_behavior', connection, behavior, item)\n                results << item_result\n                total_processed += 1\n              rescue => e\n                errors << { 'batch' => [item], 'error' => e.message }\n              end\n            end\n          end\n\n        rescue => e\n          # catastrophic batch failure (network, quota, etc.)\n          errors << { 'batch' => batch, 'error' => e.message }\n        end\n      end\n      \n      {\n        'success'         => errors.empty?,\n        'results'         => results,\n        'errors'          => errors,\n        'total_processed' => total_processed,\n        'total_errors'    => err"}
{"id":"lam:1e6b0393952b84c7","kind":"lambda","name":"method","fqname":"method:format_user_error","loc":{"line":2421,"column":23,"length":400,"begin":101215,"end":101615},"file":"connector.rb","text":"ER\n    format_user_error: lambda do |err|\n      base = \"Vertex AI error #{err['code']}\"\n      base += \" #{err['status']}\" if err['status']\n      head = \"#{base}: #{err['summary']}\"\n      tags = [\"corr_id=#{err['correlation_id']}\"]\n      tags << \"remote_id=#{err['remote_request_id']}\" if err['remote_request_id']\n      msg = \"#{head} [#{tags.join(' ')}]\"\n      msg += \" â€” Hint: #{err['hint']}\" if e"}
{"id":"lam:0fd2672c3fb0993a","kind":"lambda","name":"method","fqname":"method:get_behavior_input_fields","loc":{"line":2433,"column":31,"length":19718,"begin":101693,"end":121411},"file":"connector.rb","text":" get_behavior_input_fields: lambda do |behavior, show_advanced, ui_cfg = {}|\n      show_advanced = !!show_advanced\n      ui_cfg ||= {}\n      explicit      = (ui_cfg['model_mode'] == 'explicit')\n      legacy_mode   = !ui_cfg.key?('model_mode')\n      include_model = false\n\n      behavior_def = call('behavior_registry')[behavior]\n      return [] unless behavior_def\n      \n      # Map behavior to input fields\n      case behavior\n      when 'text.generate'\n        fields = [\n          { name: 'prompt', label: 'Prompt', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.translate'\n        fields = [\n          { name: 'text', label: 'Text to Translate', control_type: 'text-area', optional: false },\n          { name: 'target_language', label: 'Target Language', control_type: 'select', pick_list: 'languages', optional: false },\n          { name: 'source_language', label: 'Source Language', control_type: 'select', pick_list: 'languages', optional: true, hint: 'Leave blank for auto-detection' }\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.summarize'\n        fields = [\n          { name: 'text', label: 'Text to Summarize', control_type: 'text-area', optional: false },\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.classify'\n        fields = [\n          { name: 'text', label: 'Text to Classify', control_type: 'text-area', optional: false },\n          { name: 'categories', label: 'Categories', type: 'array', of: 'object', properties: [\n            { name: 'name', label: 'Category Name' },\n            { name: 'description', label: 'Description' }\n          ]}\n        ]\n        if include_model\n          fields << {\n            name: 'model', label: 'Model', group: 'Model & tuning', control_type: 'select', optional: true, sticky: true,\n            pick_list: 'models_dynamic_for_behavior', pick_list_params: { behavior: behavior }, toggle_hint: 'Select from list',\n            toggle_field: { name: 'model', label: 'Model (custom id)', type: 'string', control_type: 'text', optional: true, toggle_hint: 'Provide custom value' }\n          }\n          fields << {\n            name: 'lock_model_revision', label: 'Lock to latest numbered revision',\n            control_type: 'checkbox', group: 'Model & tuning',\n            hint: 'Resolves alias (e.g., gemini-1.5-pro) to current highest revision at runtime.'\n          }\n        end\n        if show_advanced\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n        end\n        fields\n      when 'text.embed'\n        fields = [\n          { name: 'texts', label: 'Texts to Embed', type: 'array', of: 'string', optional: false },\n          { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks',  optional: true, hint: 'Helps the model optimize embeddings for your use case.' },\n          { name: 'title', label: 'Title (for documents)', optional: true, hint: 'Used only with task_type = RETRIEVAL_DOCUMENT.' }\n        ]\n        # Advanced embedding controls\n        if show_advanced\n          fields += [\n            { name: 'output_dimensionality', label: 'Output dimensionality', type: 'integer', group: 'Advanced',\n              hint: 'Truncate embedding size to this dimension (e.g., 256/512/768/3072 depending on model).' },\n            { name: 'auto_truncate', label: 'Auto truncate long inputs', control_type: 'checkbox', default: true, group: 'Advanced',\n              hint: 'Set false to error on over-limit inputs instead of silent truncation.' }\n          ]\n          fields += [\n            { name: 'system', label: 'System instruction', control_type: 'text-area', optional: true, group: 'Advanced', hint: 'Optional system prompt to guide the model' },\n            { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n              properties: [\n                { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n              ]}\n          ]\n         end\n         fields\n      when 'vector.upsert_datapoints'\n        fields = [\n          # Target\n          { name: 'index', label: 'Index', group: 'Target', hint: 'Index resource or ID (e.g., projects/.../indexes/IDX or just IDX)', optional: false },\n          # Source: from embeddings (recommended path from Generate embeddings)\n          { name: 'embeddings', label: 'Embeddings', group: 'Source (from embeddings)', type: 'array', of: 'array', optional: true,\n            hint: 'Map from Generate embeddings â†’ vectors or embeddings' },\n          { name: 'datapoint_ids', label: 'Datapoint IDs', group: 'Source (from embeddings)', type: 'array', of: 'string', \n            optional: true, hint: 'Optional; if omitted, IDs are auto-generated' },\n          { name: 'datapoint_id_prefix', label: 'Auto ID prefix', group: 'Source (from embeddings)', optional: true, default: 'dp_' },\n          { name: 'start_index', label: 'Starting index (1-based)', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 1 },\n          { name: 'pad_to', label: 'Pad IDs to N digits', group: 'Source (from embeddings)', type: 'integer', optional: true, default: 6 },\n\n          # Datapoint defaults applied to all when using embeddings\n          { name: 'common_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n          ]},\n          { name: 'common_numeric_restricts', group: 'Datapoint defaults', type: 'array', of: 'object', properties: [\n            { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n            { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n          ]},\n          { name: 'common_crowding_tag', group: 'Datapoint defaults', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n          { name: 'embedding_metadata', group: 'Datapoint defaults', type: 'object' },\n\n          # Advanced: provide full datapoints directly (legacy / power-user)\n          { name: 'datapoints', label: 'Datapoints (advanced)', group: 'Provide full datapoints',\n            type: 'array', of: 'object', optional: true, properties: [\n              { name: 'datapoint_id', label: 'Datapoint ID', optional: false },\n              { name: 'feature_vector', label: 'Feature vector', type: 'array', of: 'number', optional: false },\n              { name: 'restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n              ]},\n              { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n                { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' },\n                { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n              ]},\n              { name: 'crowding_tag', type: 'object', properties: [{ name: 'crowdingAttribute' }] },\n              { name: 'embedding_metadata', type: 'object' }\n            ]}\n        ]\n      when 'vector.find_neighbors'\n        fields = [\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Overrides connection host just for this call (e.g. <hash>....vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', hint: 'Resource or ID (e.g. projects/.../indexEndpoints/IEP or IEP)', optional: false, group: 'Target' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n          { name: 'neighbor_count', label: 'Neighbors per query', type: 'integer', default: 10, group: 'Query' },\n          { name: 'return_full_datapoint', label: 'Return full datapoint', control_type: 'checkbox', group: 'Query' },\n\n          # NEW: scoring & aggregates\n          { name: 'distance_metric', label: 'Index distance metric', control_type: 'select',\n            pick_list: 'vector_distance_metrics', optional: true, group: 'Scoring & aggregates',\n            hint: 'Set if you want valid confidence scores. For DOT_PRODUCT, set Feature normalization to UNIT_L2_NORM.' },\n          { name: 'feature_norm_type', label: 'Feature normalization', control_type: 'select',\n            pick_list: 'vector_feature_norm_types', optional: true, group: 'Scoring & aggregates' },\n          { name: 'include_stats', label: 'Include aggregate stats', control_type: 'checkbox',\n            default: true, optional: true, group: 'Scoring & aggregates' },\n\n          { name: 'queries', label: 'Queries', type: 'array', of: 'object', group: 'Queries', properties: [\n            { name: 'datapoint_id', label: 'Query datapoint ID' },\n            { name: 'feature_vector', label: 'Query vector', type: 'array', of: 'number', hint: 'Use either vector or datapoint_id' },\n            { name: 'neighbor_count', label: 'Override neighbors for this query', type: 'integer' },\n            { name: 'restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'allowList', type: 'array', of: 'string' }, { name: 'denyList', type: 'array', of: 'string' }\n            ]},\n            { name: 'numeric_restricts', type: 'array', of: 'object', properties: [\n              { name: 'namespace' }, { name: 'op' }, { name: 'valueInt' }, { name: 'valueFloat', type: 'number' }, { name: 'valueDouble', type: 'number' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        mode = (ui_cfg['id_source'] || 'auto').to_s\n        fields = [\n          # Target\n          { name: 'endpoint_host', label: 'Public endpoint host (vdb)', hint: 'Optional override (e.g. <hash>.vdb.vertexai.goog)', optional: true, group: 'Target' },\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, group: 'Target', hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false, group: 'Target' },\n        ]\n        # Helper lambdas to append groups\n        add_manual = lambda {\n          fields << { name: 'ids', label: 'Datapoint IDs (manual)',\n                      type: 'array', of: 'string', optional: true, group: 'IDs' }\n        }\n        add_neighbors = lambda {\n          fields << { name: 'neighbors', label: 'kâ€‘NN neighbors (flattened)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [{ name: 'datapoint_id' }] }\n        }\n        add_groups = lambda {\n          fields << { name: 'groups', label: 'kâ€‘NN groups (from Find neighbors)',\n                      optional: true, group: 'Map from Find neighbors',\n                      type: 'array', of: 'object', properties: [\n                        { name: 'neighbors', type: 'array', of: 'object', properties: [{ name: 'datapoint_id"}
{"id":"lam:4c1e9c0406f749cf","kind":"lambda","name":"method","fqname":"method:get_behavior_output_fields","loc":{"line":2751,"column":32,"length":3528,"begin":121483,"end":125011},"file":"connector.rb","text":"s\n    get_behavior_output_fields: lambda do |behavior|\n      case behavior\n      # Text\n      when 'text.generate'\n        [{ name: 'result', label: 'Generated Text' }]\n      when 'text.translate'\n        [\n          { name: 'result', label: 'Translated Text' }\n        ]\n      when 'text.summarize'\n        [\n          { name: 'result', label: 'Summary' },\n          { name: 'word_count', type: 'integer' }\n        ]\n      when 'text.classify'\n        [\n          { name: 'category', label: 'Selected Category' },\n          { name: 'confidence', type: 'number' }\n        ]\n      # Embedding\n      when 'text.embed'\n        [\n          { name: 'embeddings', type: 'array', of: 'object', properties: [\n            { name: 'values', label: 'Values', type: 'array', of: 'number'}]},\n          # @note PATCH 2025-10-01-A, added properties to ensure output is emitted as expected\n          # @note PATCH 2025-10-01-B, removed erroneous trailing space in scalar type \n          { name: 'vectors', type: 'array', of: 'object', properties: [ \n            { name: 'feature_vector', type: 'array', of: 'number' } ]},\n          { name: 'count', type: 'integer' },\n          { name: 'dimension', type: 'integer' },\n          { name: 'avg_norm', type: 'number' },\n          { name: 'norms', type: 'array', of: 'number' } \n        ]\n      # Vector search\n      when 'vector.upsert_datapoints'\n        [\n          { name: 'ack' }, { name: 'count', type: 'integer' }, { name: 'index' }, { name: 'empty_body', type: 'boolean' }\n        ]\n      when 'vector.find_neighbors'\n        [\n          { name: 'summary', type: 'object', properties: [\n            { name: 'groups', type: 'integer' },\n            { name: 'neighbors', type: 'integer' },\n            { name: 'distance_mean', type: 'number' },\n            { name: 'score_mean', type: 'number' },\n            { name: 'score_max',  type: 'number' },\n            { name: 'confidence_mean', type: 'number' },\n            { name: 'confidence_max',  type: 'number' }\n          ]},\n          { name: 'groups', type: 'array', of: 'object', properties: [\n            { name: 'query_id' },\n            { name: 'stats', type: 'object', properties: [\n              { name: 'neighbor_count', type: 'integer' },\n              { name: 'distance_mean', type: 'number' },\n              { name: 'score_mean',     type: 'number' },\n              { name: 'score_max',      type: 'number' },\n              { name: 'confidence_mean', type: 'number' },\n              { name: 'confidence_max',  type: 'number' }\n            ]},\n            { name: 'neighbors', type: 'array', of: 'object', properties: [\n              { name: 'datapoint_id' },\n              { name: 'distance', type: 'number' },\n              { name: 'score',    type: 'number' },\n              { name: 'confidence', type: 'number' },\n              { name: 'datapoint', type: 'object' }\n            ]}\n          ]}\n        ]\n      when 'vector.read_datapoints'\n        [\n          { name: 'datapoints', type: 'array', of: 'object', properties: [\n            { name: 'datapoint_id' },\n            { name: 'feature_vector', type: 'array', of: 'number' },\n            { name: 'restricts', type: 'array', of: 'object' },\n            { name: 'numeric_restricts', type: 'array', of: 'object' },\n            { name: 'crowding_tag', type: 'object' },\n            { name: 'embedding_metadata', type: 'object' }\n          ] }\n        ]\n      # Multimodal\n      when 'multimodal.analyze'\n        [{ name: 'result', label: 'Analysis' }]\n      else\n        [{ n"}
{"id":"lam:453be5929339f7c3","kind":"lambda","name":"method","fqname":"method:list_publisher_models","loc":{"line":2839,"column":27,"length":758,"begin":125086,"end":125844},"file":"connector.rb","text":"beta1)\n    list_publisher_models: lambda do |connection, publisher: 'google'|\n      ver = connection['version'].to_s.strip\n      ver = ver.empty? ? 'v1' : ver\n      cache_key = \"pub_models:#{publisher}:#{ver}\"   # <â€” include version in key\n\n      if (cached = call('memo_get', cache_key))\n        return cached\n      end\n\n      url = call('build_endpoint_url', connection, { 'family' => 'publisher_models', 'publisher' => publisher }, {})\n      resp = call('http_request', connection, method: 'GET', url: url,\n                  headers: call('build_headers', connection),\n                  retry_config: { max_attempts: 3, backoff: 1.0, retry_on: [429, 500, 502, 503, 504] })\n\n      models = (resp['publisherModels'] || [])\n      call('memo_put', cache_key"}
{"id":"lam:ba1e0af79f1abd59","kind":"lambda","name":"method","fqname":"method:memo_store","loc":{"line":2858,"column":16,"length":25,"begin":125863,"end":125888},"file":"connector.rb","text":"   models\n    end,\n\n    m"}
{"id":"lam:e72a7b38e9abef36","kind":"lambda","name":"method","fqname":"method:memo_get","loc":{"line":2860,"column":14,"length":178,"begin":125905,"end":126083},"file":"connector.rb","text":" { @__memo ||= {} },\n\n    memo_get: lambda do |key|\n      item = call('memo_store')[key]\n      return nil unless item\n      exp = item['exp']\n      return nil if exp && Time.now."}
{"id":"lam:31340ac2487ac38d","kind":"lambda","name":"method","fqname":"method:memo_put","loc":{"line":2868,"column":14,"length":145,"begin":126100,"end":126245},"file":"connector.rb","text":"item['val']\n    end,\n\n    memo_put: lambda do |key, val, ttl=nil|\n      call('memo_store')[key] = { 'val' => val, 'exp' => (ttl ? Time.now.to_i +"}
{"id":"lam:fae6b448d0c1b642","kind":"lambda","name":"method","fqname":"method:normalize_find_neighbors","loc":{"line":2874,"column":30,"length":2416,"begin":126354,"end":128770},"file":"connector.rb","text":"shape\n    normalize_find_neighbors: lambda do |resp, input|\n      groups_raw = Array(resp['nearestNeighbors'])\n      metric     = input['distance_metric']\n      norm_type  = input['feature_norm_type']\n      include_stats = input.key?('include_stats') ? !!input['include_stats'] : true\n\n      groups = groups_raw.map do |nn|\n        neighbors = Array(nn['neighbors']).map do |n|\n          dist = n['distance']\n          did  = n.dig('datapoint', 'datapointId')\n          {\n            'datapoint_id' => did,\n            'distance'     => dist,\n            # Legacy score: normalized from distance (cosine heuristic)\n            'score'        => call('transform_data', input: dist, from_format: 'distance', to_format: 'similarity'),\n            # New: mathematically valid confidence when possible\n            'confidence'   => call('confidence_from_distance', dist, metric, norm_type),\n            'datapoint'    => n['datapoint']\n          }.compact\n        end\n\n        stats =\n          if include_stats\n            {\n              'neighbor_count'   => neighbors.length,\n              'distance_mean'    => call('safe_mean', neighbors.map { |z| z['distance'] }),\n              'score_mean'       => call('safe_mean', neighbors.map { |z| z['score'] }),\n              'score_max'        => (neighbors.map { |z| z['score'] }.compact.max),\n              'confidence_mean'  => call('safe_mean', neighbors.map { |z| z['confidence'] }),\n              'confidence_max'   => (neighbors.map { |z| z['confidence'] }.compact.max)\n            }.compact\n          end\n\n        {\n          'query_id'  => nn['id'],\n          'stats'     => stats,\n          'neighbors' => neighbors\n        }.compact\n      end\n\n      # Top-level summary if desired\n      summary =\n        if include_stats\n          flat = groups.flat_map { |g| g['neighbors'] || [] }\n          {\n            'groups'          => groups.length,\n            'neighbors'       => flat.length,\n            'distance_mean'   => call('safe_mean', flat.map { |z| z['distance'] }),\n            'score_mean'      => call('safe_mean', flat.map { |z| z['score'] }),\n            'score_max'       => (flat.map { |z| z['score'] }.compact.max),\n            'confidence_mean' => call('safe_mean', flat.map { |z| z['confidence'] }),\n            'confidence_max'  => (flat.map { |z| z['confidence'] }.compact.max)\n          }.compact\n        end\n\n      { 'summary' => summary, "}
{"id":"lam:c7c6f3fc4797184b","kind":"lambda","name":"method","fqname":"method:normalize_http_error","loc":{"line":2932,"column":26,"length":1256,"begin":128799,"end":130055},"file":"connector.rb","text":"    end,\n\n    normalize_http_error: lambda do |connection, code:, body:, headers:, message:, url:, corr_id:, attempt:, duration_ms:|\n      parsed = {}\n      if body.is_a?(Hash)\n        parsed = body\n      else\n        begin\n          parsed = JSON.parse(body.to_s)\n        rescue\n          parsed = {}\n        end\n      end\n\n      gerr    = parsed['error'].is_a?(Hash) ? parsed['error'] : {}\n      status  = gerr['status']\n      summary = (gerr['message'] || message || body.to_s).to_s.strip[0, 300] # compact\n      hint    = call('error_hint', connection, code, status)\n\n      remote_id = nil\n      if headers\n        remote_id = headers['x-request-id'] ||\n                    headers['x-cloud-trace-context'] ||\n                    headers['x-guploader-uploadid']\n      end\n\n      {\n        'code'              => code.to_i,\n        'status'            => status,\n        'summary'           => summary,\n        'hint'              => hint,\n        'retryable'         => call('retryable_http_code', code),\n        'retry_after_s'     => call('parse_retry_after', headers),\n        'correlation_id'    => corr_id,\n        'remote_request_id' => remote_id,\n        'attempt'           => attempt,\n        'duration_ms'       => duration_ms,\n        'url' "}
{"id":"lam:4257f54664223631","kind":"lambda","name":"method","fqname":"method:normalize_read_index_datapoints","loc":{"line":2971,"column":37,"length":704,"begin":130095,"end":130799},"file":"connector.rb","text":"   normalize_read_index_datapoints: lambda do |resp, _input|\n      # Expected Vertex shape: { \"datapoints\": [ { \"datapointId\": \"...\", \"featureVector\": [...],\n      #   \"restricts\": [...], \"numericRestricts\": [...], \"crowdingTag\": {...}, \"embeddingMetadata\": {...} } ] }\n      dps = Array(resp['datapoints']).map do |d|\n        {\n          'datapoint_id'      => d['datapointId'] || d['id'],\n          'feature_vector'    => Array(d['featureVector']).map(&:to_f),\n          'restricts'         => d['restricts'],\n          'numeric_restricts' => d['numericRestricts'],\n          'crowding_tag'      => d['crowdingTag'],\n          'embedding_metadata'=> d['embeddingMetadata']\n        }.compact\n      end\n "}
{"id":"lam:6ec6adea8727c27a","kind":"lambda","name":"method","fqname":"method:normalize_safety_settings","loc":{"line":2988,"column":31,"length":1080,"begin":130865,"end":131945},"file":"connector.rb","text":"ings\n    normalize_safety_settings: lambda do |input|\n      # Accepts either the new array shape or the legacy hash; returns array\n      if input.is_a?(Array)\n        # non-destructive copy with only supported keys\n        return input.map do |r|\n          {\n            'category'  => r['category']  || r[:category],\n            'threshold' => r['threshold'] || r[:threshold],\n            'method'    => r['method']    || r[:method]\n          }.compact\n        end\n      end\n\n      # Legacy object: { harassment: 'BLOCK_...', hate_speech: 'BLOCK_...', ... }\n      if input.is_a?(Hash)\n        map = {\n          'harassment'          => 'HARM_CATEGORY_HARASSMENT',\n          'hate_speech'         => 'HARM_CATEGORY_HATE_SPEECH',\n          'sexually_explicit'   => 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n          'dangerous_content'   => 'HARM_CATEGORY_DANGEROUS_CONTENT'\n        }\n        return input.each_with_object([]) do |(k, v), arr|\n          next if v.nil? || v.to_s.strip.empty?\n          cat = map[k.to_s]\n          arr << { 'category' => cat, 'threshold' => v } if cat\n   "}
{"id":"lam:035e358495135034","kind":"lambda","name":"method","fqname":"method:parse_retry_after","loc":{"line":3021,"column":23,"length":257,"begin":131995,"end":132252},"file":"connector.rb","text":"RETRY HELPER\n    parse_retry_after: lambda do |headers|\n      return nil unless headers\n      ra = headers['Retry-After'] || headers['retry-after']\n      return nil if ra.nil? || ra.to_s.strip.empty?\n      # integer seconds only (safe & simple)\n      ra.to_"}
{"id":"lam:adbc1b7a6adee3a5","kind":"lambda","name":"method","fqname":"method:qualify_resource","loc":{"line":3030,"column":22,"length":415,"begin":132357,"end":132772},"file":"connector.rb","text":" caller input\n    qualify_resource: lambda do |connection, type, value|\n      return value if value.to_s.start_with?('projects/')\n      project = connection['project']\n      region  = connection['region']\n      case type.to_s\n      when 'index'          then \"projects/#{project}/locations/#{region}/indexes/#{value}\"\n      when 'index_endpoint' then \"projects/#{project}/locations/#{region}/indexEndpoints/#{value}"}
{"id":"lam:f69d6fc0eada40a9","kind":"lambda","name":"method","fqname":"method:resolve_model_version","loc":{"line":3043,"column":27,"length":613,"begin":132936,"end":133549},"file":"connector.rb","text":"on style\n    resolve_model_version: lambda do |connection, short|\n      return short if short.to_s.match?(/(-\\d{3,}|@\\d{3,})$/)\n\n      cache_key = \"model_resolve:#{short}\"\n      if (cached = call('memo_get', cache_key)); return cached; end\n\n      ids = Array(call('list_publisher_models', connection))\n              .map { |m| (m['name'] || '').split('/').last }\n              .select { |id| id.start_with?(\"#{short}-\") || id.start_with?(\"#{short}@\") }\n\n      latest = ids.max_by { |id| id[/[-@](\\d+)$/, 1].to_i }\n      chosen = latest || short  # fall back to alias if no numeric\n      call('memo_put', cache_key"}
{"id":"lam:e608368a5ea0c574","kind":"lambda","name":"method","fqname":"method:retryable_http_code","loc":{"line":3060,"column":25,"length":78,"begin":133596,"end":133674},"file":"connector.rb","text":"TRY HELPER\n    retryable_http_code: lambda { |code|\n      [408, 429, 500, 502,"}
{"id":"lam:4d0243933a608264","kind":"lambda","name":"method","fqname":"method:safe_mean","loc":{"line":3064,"column":15,"length":120,"begin":133692,"end":133812},"file":"connector.rb","text":"?(code.to_i)\n    },\n\n    safe_mean: lambda do |arr|\n      xs = Array(arr).compact\n      return nil if xs.empty?\n      xs"}
{"id":"lam:31eea81eb58244f2","kind":"lambda","name":"method","fqname":"method:select_model","loc":{"line":3071,"column":18,"length":2188,"begin":133861,"end":136049},"file":"connector.rb","text":"l selection logic\n    select_model: lambda do |behavior_def, cfg, input|\n      # 0) Respect explicit model in input\n      # @note PATCH 2025-10-01-C using helper so that nil and empty arrays/hashes are treated as not present\n      if call('value_present', input['model']) || call('value_present', input['model_override'])\n        return call('value_present', input['model']) ? input['model'] : input['model_override']\n      end\n\n      mode      = (input['model_mode'] || cfg.dig(:models, :mode) || 'auto').to_s\n      strategy  = (cfg.dig(:models, :strategy) || 'balanced').to_s\n      supported = Array(behavior_def[:supported_models]).compact\n      default   = cfg.dig(:models, :default)\n\n      # Prefer an item if supported, else first supported, else default\n      prefer = lambda do |*candidates|\n        # Choose the first candidate that is in 'supported'; else first supported; else default\n        c = candidates.flatten.compact.find { |m| supported.include?(m) }\n        c || supported.first || default\n      end\n\n      case mode\n      when 'connection'\n        # Only honor connection default if it's supported by this behavior\n        return default if supported.include?(default)\n        return supported.first || default\n      when 'explicit'\n        # If user chose 'explicit' but didn't supply a model, pick a safe supported default\n        return supported.first || default\n      else # 'auto'\n        if behavior_def[:capability].to_s == 'embedding'\n          case strategy\n          when 'cost'        then prefer.call('textembedding-gecko', 'text-embedding-005', 'text-embedding-004')\n          when 'performance' then prefer.call('gemini-embedding-001', 'text-embedding-005', 'textembedding-gecko', 'text-embedding-004')\n          else                    prefer.call('text-embedding-005', 'gemini-embedding-001', 'textembedding-gecko', 'text-embedding-004')\n          end\n        else\n          case strategy\n          when 'cost'        then prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n          when 'performance' then prefer.call('gemini-1.5-pro',   'gemini-1.5-flash')\n          else                    prefer.call('gemini-1.5-flash', 'gemini-1.5-pro')\n       "}
{"id":"lam:9e6ebf166b57687c","kind":"lambda","name":"method","fqname":"method:telemetry_envelope_fields","loc":{"line":3116,"column":31,"length":325,"begin":136131,"end":136456},"file":"connector.rb","text":" ===\n    telemetry_envelope_fields: lambda do\n      [\n        { name: 'success', type: 'boolean' },\n        { name: 'timestamp', type: 'datetime' },\n        { name: 'metadata', type: 'object', properties: [\n          { name: 'operation' }, { name: 'model' }\n        ]},\n        { name: 'trace', type: 'object', properties: ca"}
{"id":"lam:1476fda426a9ca93","kind":"lambda","name":"method","fqname":"method:trace_fields","loc":{"line":3126,"column":18,"length":525,"begin":136476,"end":137001},"file":"connector.rb","text":"\n      ]\n    end,\n    trace_fields: lambda do\n      [\n        { name: 'correlation_id' },\n        { name: 'duration_ms', type: 'integer' },\n        { name: 'attempt', type: 'integer' },\n        { name: 'http_status', type: 'integer' },\n        { name: 'remote_request_id' }, \n        { name: 'rate_limit', type: 'object', properties: [\n          { name: 'rpm', type: 'integer' },\n          { name: 'count', type: 'integer' },\n          { name: 'reset_in_s', type: 'integer' },\n          { name: 'window_started_at', type: 'da"}
{"id":"lam:346fe00a129ed882","kind":"lambda","name":"method","fqname":"method:to_query","loc":{"line":3143,"column":14,"length":579,"begin":137060,"end":137639},"file":"connector.rb","text":"ry schema helpers===\n\n    to_query: lambda do |params|\n      encode = lambda do |s|\n        # RFC3986 unreserved: ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n        s.to_s.bytes.map { |b|\n          if (48..57).cover?(b) || (65..90).cover?(b) || (97..122).cover?(b) || [45,46,95,126].include?(b)\n            b.chr\n          else\n            \"%%%02X\" % b\n          end\n        }.join\n      end\n\n      params.flat_map do |k, v|\n        key = encode.call(k)\n        if v.is_a?(Array)\n          v.map { |e| \"#{key}=#{encode.call(e)}\" }\n        else\n          \"#{key}=#{encode.call(v)}\"\n   "}
{"id":"lam:cc85260b6fccd3de","kind":"lambda","name":"method","fqname":"method:value_present","loc":{"line":3166,"column":19,"length":174,"begin":137725,"end":137899},"file":"connector.rb","text":"eated as absent)\n    value_present: lambda do |v|\n      return false if v.nil?\n      return false if v.is_a?(String) && v.strip.empty?\n      return false if v.respond_to?(:em"}
{"id":"lam:da791663000b935e","kind":"lambda","name":"method","fqname":"method:vector_search_base","loc":{"line":3174,"column":24,"length":653,"begin":138011,"end":138664},"file":"connector.rb","text":"n provided.\n    vector_search_base: lambda do |connection, input|\n      host = (input['endpoint_host'] || connection['vector_search_endpoint']).to_s.strip\n      v = connection['version']\n      version = (v && !v.to_s.strip.empty?) ? v : 'v1'\n\n      if host.empty?\n        # Fallback to regional API host (works for admin ops; query should use public vdb host)\n        \"https://#{connection['region']}-aiplatform.googleapis.com/#{version}\"\n      elsif host.include?('vdb.vertexai.goog')\n        \"https://#{host}/#{version}\"\n      else\n        # Allow passing a full https://... custom host\n        host = host.sub(%r{\\Ahttps?://}i, '')\n        \"https://#"}
{"id":"lam:02761672050f9ad7","kind":"lambda","name":"method","fqname":"method:wrap_embeddings_vectors_v1","loc":{"line":3193,"column":32,"length":787,"begin":138855,"end":139642},"file":"connector.rb","text":" fx\n    wrap_embeddings_vectors_v1: lambda do |response, input|\n      raw = if response.is_a?(Hash) && response.key?('result')\n        response['result']\n      else\n        response\n      end\n\n      arr = Array(raw).map { |v| Array(v).map(&:to_f) }\n      norms = arr.map { |v| Math.sqrt(v.reduce(0.0) { |s, x| s + (x.to_f * x.to_f) }) }\n      dim   = arr.first ? arr.first.length : nil\n\n      out = {\n        'embeddings' => arr.map { |v| { 'values' => v } },      # <-- new, pillâ€‘friendly\n        'vectors'    => arr.map { |v| { 'feature_vector' => v } },\n        'count'      => arr.length,\n        'dimension'  => dim,\n        'norms'      => norms,\n        'avg_norm'   => call('safe_mean', norms)\n      }.compact\n\n      out['_trace'] = response['_trace'] if response.is_a?(Hash) &"}
{"id":"lam:52d8890df392e5de","kind":"lambda","name":"method","fqname":"method:wrap_embeddings_vectors","loc":{"line":3218,"column":29,"length":1884,"begin":139713,"end":141597},"file":"connector.rb","text":" testing\n    wrap_embeddings_vectors: lambda do |response, input|\n      # Extract raw embeddings from response\n      raw = if response.is_a?(Hash) && response.key?('result')\n        response['result']\n      else\n        response\n      end\n      \n      # Ensure we have an array of embeddings\n      embeddings_array = []\n      \n      if raw.nil? || (raw.is_a?(Array) && raw.empty?)\n        # Return empty structure if no embeddings\n        return {\n          'embeddings' => [],\n          'vectors'    => [],\n          'count'      => 0,\n          'dimension'  => 0,\n          'norms'      => [],\n          'avg_norm'   => 0\n        }.merge(response.is_a?(Hash) && response['_trace'] ? { '_trace' => response['_trace'] } : {})\n      end\n      \n      # Normalize to array of arrays\n      if raw.is_a?(Array)\n        if raw.first.is_a?(Numeric)\n          # Single embedding as flat array\n          embeddings_array = [raw]\n        else\n          # Multiple embeddings\n          embeddings_array = raw\n        end\n      else\n        # Unexpected format - wrap in array\n        embeddings_array = [Array(raw)]\n      end\n      \n      # Convert to floats and calculate norms\n      arr = embeddings_array.map { |v| \n        Array(v).map { |x| x.to_f rescue 0.0 }  # Safe conversion with fallback\n      }.reject { |v| v.empty? }\n      \n      norms = arr.map { |v| \n        Math.sqrt(v.reduce(0.0) { |s, x| s + (x * x) })\n      }\n      \n      dim = arr.first ? arr.first.length : 0\n      \n      out = {\n        'embeddings' => arr.map { |v| { 'values' => v } },\n        'vectors'    => arr.map { |v| { 'feature_vector' => v } },\n        'count'      => arr.length,\n        'dimension'  => dim,\n        'norms'      => norms,\n        'avg_norm'   => call('safe_mean', norms) || 0\n      }\n      \n      # Preserve trace if present\n      out['_trace'] = response['_trace'] if response.is_a?(Hash) &"}
{"id":"lam:5397c573753a72ed","kind":"lambda","name":"input_fields","fqname":"action:batch_operation#input_fields","loc":{"line":262,"column":20,"length":1266,"begin":11326,"end":12592},"file":"connector.rb","text":"ds: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        fields = [\n          { name: 'items', type: 'array', of: 'object', properties: [\n              { name: 'text', label: 'Text', optional: false },\n              { name: 'task_type', label: 'Task Type', control_type: 'select', pick_list: 'embedding_tasks' }\n          ]}\n        ]\n\n        if cfg['batch_strategy'] == 'tokens'\n          fields << { name: 'token_ceiling', label: 'Token ceiling per batch (approx)',\n                      type: 'integer', optional: false,\n                      hint: 'Approximation: tokens â‰ˆ characters/4' }\n        else\n          fields << { name: 'batch_size', type: 'integer', default: 10, hint: 'Items per batch' }\n        end\n\n        if cfg['advanced_config']\n          fields += [\n            { name: 'max_items_per_batch', label: 'Max items per batch', type: 'integer', default: 100, group: 'Advanced',\n              hint: 'Guardrail applied to both strategies' },\n            { name: 'max_body_bytes', label: 'Approx max body size (bytes)', type: 'integer', default: 1000000, group: 'Advanced',\n              hint: 'Guardrail; approximate JSON payload size' }\n          ]\n        end\n\n        fields\n   "}
{"id":"lam:74385af8783f3199","kind":"lambda","name":"execute","fqname":"action:batch_operation#execute","loc":{"line":291,"column":15,"length":1098,"begin":12625,"end":13723},"file":"connector.rb","text":"cute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior = config_fields['behavior']\n        started = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n\n        inner = call('execute_batch_behavior',\n          connection,\n          behavior,\n          call('deep_copy', input['items']), # don't mutate caller input\n          input['batch_size'],\n          config_fields['batch_strategy'],\n          {\n            'token_ceiling'       => input['token_ceiling'],\n            'max_items_per_batch' => input['max_items_per_batch'],\n            'max_body_bytes'      => input['max_body_bytes']\n          }.compact\n        )\n\n        # Telemetry\n        duration_ms = ((Process.clock_gettime(Process::CLOCK_MONOTONIC) - started) * 1000).round\n        envelope_trace = { 'correlation_id' => \"#{Time.now.utc.to_i}-#{SecureRandom.hex(6)}\", 'duration_ms' => duration_ms, 'attempt' => 1 }\n\n        call('enrich_response',\n          response: inner.merge('_trace' => envelope_trace),\n          metadata: { 'operation' => \"batch.#{behavior}\", 'model' => 'n/a' }\n        )\n   "}
{"id":"lam:2d8fd2f42e7c462b","kind":"lambda","name":"output_fields","fqname":"action:batch_operation#output_fields","loc":{"line":318,"column":21,"length":439,"begin":13761,"end":14200},"file":"connector.rb","text":"elds: lambda do |_obj, _conn, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'results', type: 'array', of: 'object' },\n          { name: 'errors',  type: 'array', of: 'object', properties: [\n            { name: 'batch', type: 'array', of: 'object' },\n            { name: 'error' }\n          ]},\n          { name: 'total_processed', type: 'integer' },\n          { name: 'total_errors', type: 'integer' }\n        ]\n   "}
{"id":"lam:995f51aa9024f5cc","kind":"lambda","name":"sample_output","fqname":"action:batch_operation#sample_output","loc":{"line":330,"column":21,"length":711,"begin":14238,"end":14949},"file":"connector.rb","text":"tput: lambda do |_conn, cfg|\n        op = cfg['behavior'] || 'unknown'\n        base = {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"batch.#{op}\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"results\"   => [],\n          \"errors\"    => [],\n          \"total_processed\" => 0,\n          \"total_errors\"    => 0\n        }\n        if op == 'text.embed'\n          base.merge(\n            \"results\"=>[\n              { \"embeddings\"=>[[0.01,0.02],[0.03,0.04]] }\n            ],\n            \"total_processed\"=>2\n          )\n        else\n          base\n        end\n   "}
{"id":"lam:0ea6f9f5722321fb","kind":"lambda","name":"input_fields","fqname":"action:vertex_operation#input_fields","loc":{"line":382,"column":20,"length":252,"begin":16816,"end":17068},"file":"connector.rb","text":"elds: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        behavior = cfg['behavior']\n        behavior ? call('get_behavior_input_fields', behavior, cfg['advanced_config'], cfg) : []\n   "}
{"id":"lam:d652b0e58794432d","kind":"lambda","name":"execute","fqname":"action:vertex_operation#execute","loc":{"line":393,"column":15,"length":477,"begin":17373,"end":17850},"file":"connector.rb","text":"cute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        behavior     = config_fields['behavior']\n        user_config  = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input   = call('deep_copy', input) # do NOT mutate Workatoâ€™s input\n\n        # Leave advanced fields in safe_input; pipeline reads only what it needs\n        call('execute_behavior', connection, behavior, safe_input, user_config)\n "}
{"id":"lam:556c7e47bd5e860a","kind":"lambda","name":"output_fields","fqname":"action:vertex_operation#output_fields","loc":{"line":388,"column":21,"length":234,"begin":17106,"end":17340},"file":"connector.rb","text":"elds: lambda do |_object_definitions, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('telemetry_envelope_fields') + (call('get_behavior_output_fields', cfg['behavior']) || [])\n   "}
{"id":"lam:8dccf4cd5388f263","kind":"lambda","name":"sample_output","fqname":"action:vertex_operation#sample_output","loc":{"line":402,"column":21,"length":2226,"begin":17888,"end":20114},"file":"connector.rb","text":"output: lambda do |_connection, config_fields|\n        behavior = (config_fields.is_a?(Hash) ? config_fields : {})['behavior']\n        case behavior\n        when 'text.generate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.generate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hello world.\" }\n        when 'text.translate'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.translate\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Hola mundo.\" }\n        when 'text.summarize'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.summarize\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"Concise summary.\", \"word_count\"=>2 }\n        when 'text.classify'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.classify\", \"model\"=>\"gemini-1.5-flash\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"category\"=>\"Support\", \"confidence\"=>0.98 }\n        when 'text.embed'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"text.embed\", \"model\"=>\"text-embedding-004\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"embeddings\"=>[[0.01,0.02,0.03]] }\n        when 'multimodal.analyze'\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"multimodal.analyze\", \"model\"=>\"gemini-1.5-pro\" },\n            \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>42, \"attempt\"=>1 },\n            \"result\"=>\"The image shows a tabby cat on a desk.\" }\n        else\n          { \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n            \"metadata\"=>{ \"operation\"=>\"unknown\", \"model\"=>\"gemini-1.5-flash\" } }\n        end\n "}
{"id":"lam:503e7d306c0a9fa5","kind":"lambda","name":"input_fields","fqname":"action:discover_index_config#input_fields","loc":{"line":448,"column":20,"length":311,"begin":20447,"end":20758},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        [\n          { name: 'index_endpoint', label: 'Index Endpoint', optional: false, hint: 'Resource or ID (e.g., projects/.../indexEndpoints/IEP or just IEP)' },\n          { name: 'deployed_index_id', label: 'Deployed Index ID', optional: false }\n        ]\n "}
{"id":"lam:5694ff106572d7f9","kind":"lambda","name":"execute","fqname":"action:discover_index_config#execute","loc":{"line":463,"column":15,"length":292,"begin":21068,"end":21360},"file":"connector.rb","text":"xecute: lambda do |connection, input|\n        safe = call('deep_copy', input)\n        disc = call('discover_index_config', connection, safe)\n        call('enrich_response',\n          response: disc,\n          metadata: { 'operation' => 'vector.discover_config', 'model' => 'n/a' }\n        )\n "}
{"id":"lam:d4f787cc6edfb7be","kind":"lambda","name":"output_fields","fqname":"action:discover_index_config#output_fields","loc":{"line":455,"column":21,"length":239,"begin":20796,"end":21035},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + [\n          { name: 'index', label: 'Index resource' },\n          { name: 'distance_metric' },\n          { name: 'feature_norm_type' }\n        ]\n "}
{"id":"lam:a12d507e596103ea","kind":"lambda","name":"sample_output","fqname":"action:discover_index_config#sample_output","loc":{"line":472,"column":21,"length":448,"begin":21398,"end":21846},"file":"connector.rb","text":"output: lambda do |_connection, _cfg|\n        {\n          \"success\"=>true, \"timestamp\"=>\"2025-01-01T00:00:00Z\",\n          \"metadata\"=>{ \"operation\"=>\"vector.discover_config\", \"model\"=>\"n/a\" },\n          \"trace\"=>{ \"correlation_id\"=>\"abc\", \"duration_ms\"=>12, \"attempt\"=>1 },\n          \"index\"=>\"projects/.../locations/us-central1/indexes/123\",\n          \"distance_metric\"=>\"COSINE_DISTANCE\",\n          \"feature_norm_type\"=>\"UNIT_L2_NORM\"\n        }\n "}
{"id":"lam:c469666a89f66af2","kind":"lambda","name":"input_fields","fqname":"action:classify_text#input_fields","loc":{"line":506,"column":20,"length":208,"begin":23364,"end":23572},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.classify', cfg['advanced_config'], cfg)\n "}
{"id":"lam:5f4b3ac847a0f206","kind":"lambda","name":"execute","fqname":"action:classify_text#execute","loc":{"line":517,"column":15,"length":304,"begin":23791,"end":24095},"file":"connector.rb","text":"xecute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input)\n        call('execute_behavior', connection, 'text.classify', safe, user_cfg)\n "}
{"id":"lam:938ec4ee8c61ce14","kind":"lambda","name":"output_fields","fqname":"action:classify_text#output_fields","loc":{"line":512,"column":21,"length":146,"begin":23611,"end":23757},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.classify')\n "}
{"id":"lam:64fe558ac7f06ce7","kind":"lambda","name":"sample_output","fqname":"action:classify_text#sample_output","loc":{"line":524,"column":21,"length":392,"begin":24135,"end":24527},"file":"connector.rb","text":"output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"text.classify\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"category\"  => \"Support\",\n          \"confidence\"=> 0.98\n        }\n "}
{"id":"lam:e5fb3c5b2276c63d","kind":"lambda","name":"input_fields","fqname":"action:generate_text#input_fields","loc":{"line":567,"column":20,"length":3383,"begin":26621,"end":30004},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg       = config_fields.is_a?(Hash) ? config_fields : {}\n        mode      = (cfg['prompt_mode'] || 'simple').to_s\n        show_adv  = !!cfg['advanced_config']\n\n        case mode\n        when 'contents'\n          fields = [\n            # Prompt structure\n            { name: 'contents', label: 'Contents', type: 'array', of: 'object', group: 'Prompt structure', optional: false,\n              properties: [\n                { name: 'role', label: 'Role', control_type: 'select',\n                  options: [['User','user'], ['Model','model']], optional: true },\n                { name: 'parts', label: 'Parts', type: 'array', of: 'object', properties: [\n                  { name: 'text',        label: 'Text' },\n                  { name: 'inline_data', label: 'Inline data', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'data',      label: 'Base64 data', control_type: 'text-area' }\n                  ]},\n                  { name: 'file_data',   label: 'File data (URI)', type: 'object', properties: [\n                    { name: 'mime_type', label: 'MIME type' },\n                    { name: 'file_uri',  label: 'File URI' }\n                  ]}\n                ]}\n              ]\n            }\n          ]\n          if show_adv\n            fields += [\n              { name: 'system', label: 'System instruction', control_type: 'text-area', group: 'Advanced' },\n              { name: 'safety_settings', label: 'Safety settings', type: 'array', of: 'object', group: 'Advanced',\n                properties: [\n                  { name: 'category',  control_type: 'select', pick_list: 'safety_categories',  optional: false },\n                  { name: 'threshold', control_type: 'select', pick_list: 'safety_thresholds',  optional: false },\n                  { name: 'method',    control_type: 'select', pick_list: 'safety_methods',     optional: true }\n                ]\n              },\n              { name: 'response_mime_type', label: 'Response MIME type', group: 'Advanced',\n                hint: 'e.g., application/json for JSON mode' },\n              { name: 'response_schema', label: 'Response schema (object)', type: 'object', group: 'Advanced',\n                hint: 'When set, a compatible response_mime_type is required' },\n              { name: 'temperature', label: 'Temperature', type: 'number', group: 'Advanced', hint: '0.0 to 1.0' },\n              { name: 'max_tokens',  label: 'Max Tokens',  type: 'integer', group: 'Advanced' },\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        when 'raw_json'\n          fields = [\n            { name: 'payload_json', label: 'Full request JSON', control_type: 'text-area',\n              optional: false, group: 'Prompt (raw JSON)',\n              hint: 'Paste the entire models.generateContent request body including contents[].' }\n          ]\n          if show_adv\n            fields += [\n              { name: 'cache_ttl',   label: 'Cache TTL (seconds)', type: 'integer', group: 'Advanced', default: 300 }\n            ]\n          end\n          fields\n\n        else # 'simple'\n          call('get_behavior_input_fields', 'text.generate', show_adv, cfg)\n        end\n "}
{"id":"lam:0e543074a0b23e41","kind":"lambda","name":"execute","fqname":"action:generate_text#execute","loc":{"line":637,"column":15,"length":481,"begin":30221,"end":30702},"file":"connector.rb","text":"xecute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg   = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe_input = call('deep_copy', input)\n        # Make prompt mode visible to the pipeline selector without mutating recipe input\n        safe_input['prompt_mode'] = config_fields['prompt_mode'] || 'simple'\n        call('execute_behavior', connection, 'text.generate', safe_input, user_cfg)\n "}
{"id":"lam:675a3639824b0e94","kind":"lambda","name":"output_fields","fqname":"action:generate_text#output_fields","loc":{"line":633,"column":21,"length":146,"begin":30042,"end":30188},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.generate')\n "}
{"id":"lam:569c2a7e01cfdb46","kind":"lambda","name":"sample_output","fqname":"action:generate_text#sample_output","loc":{"line":645,"column":21,"length":346,"begin":30744,"end":31090},"file":"connector.rb","text":"output: lambda do |_connection, _cfg|\n        {\n          \"success\" => true, \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\" => { \"operation\" => \"text.generate\", \"model\" => \"gemini-1.5-flash-002\" },\n          \"trace\" => { \"correlation_id\" => \"abc\", \"duration_ms\" => 42, \"attempt\" => 1 },\n          \"result\" => \"Hello world.\"\n        }\n "}
{"id":"lam:aa81966c4ec8b327","kind":"lambda","name":"input_fields","fqname":"action:find_neighbors#input_fields","loc":{"line":659,"column":20,"length":123,"begin":31292,"end":31415},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.find_neighbors', true)\n "}
{"id":"lam:6246b03b454dcd59","kind":"lambda","name":"execute","fqname":"action:find_neighbors#execute","loc":{"line":667,"column":15,"length":186,"begin":31640,"end":31826},"file":"connector.rb","text":"xecute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        safe = call('deep_copy', input)\n        call('execute_behavior', connection, 'vector.find_neighbors', safe)\n "}
{"id":"lam:6de171a686bc3026","kind":"lambda","name":"output_fields","fqname":"action:find_neighbors#output_fields","loc":{"line":663,"column":21,"length":154,"begin":31453,"end":31607},"file":"connector.rb","text":"fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.find_neighbors')\n "}
{"id":"lam:b7e3bd2932a83491","kind":"lambda","name":"input_fields","fqname":"action:read_index_datapoints#input_fields","loc":{"line":689,"column":20,"length":128,"begin":32565,"end":32693},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, cfg|\n        call('get_behavior_input_fields', 'vector.read_datapoints', true, cfg)"}
{"id":"lam:41c65bb72fad0e25","kind":"lambda","name":"execute","fqname":"action:read_index_datapoints#execute","loc":{"line":697,"column":15,"length":334,"begin":32919,"end":33253},"file":"connector.rb","text":" execute: lambda do |connection, input, _in_schema, _out_schema, cfg|\n        safe = call('deep_copy', input)\n        # Pass the chosen mode to the behavior without mutating the original input\n        safe['id_source'] = cfg['id_source'] if cfg['id_source']\n        call('execute_behavior', connection, 'vector.read_datapoints', safe)"}
{"id":"lam:91608659e76c90ad","kind":"lambda","name":"output_fields","fqname":"action:read_index_datapoints#output_fields","loc":{"line":693,"column":21,"length":155,"begin":32731,"end":32886},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.read_datapoints')"}
{"id":"lam:7fdb5b3888f5438d","kind":"lambda","name":"sample_output","fqname":"action:read_index_datapoints#sample_output","loc":{"line":704,"column":21,"length":443,"begin":33295,"end":33738},"file":"connector.rb","text":"e_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => Time.now.utc.iso8601,\n          \"metadata\"  => { \"operation\" => \"vector.read_datapoints\", \"model\" => \"n/a\" },\n          \"trace\"     => { \"correlation_id\" => \"abc\", \"duration_ms\" => 12, \"attempt\" => 1 },\n          \"datapoints\"=> [\n            { \"datapoint_id\" => \"dp_000001\", \"feature_vector\" => [0.01, 0.02, 0.03] }\n          ]\n        }"}
{"id":"lam:f17dca4d85f2c610","kind":"lambda","name":"input_fields","fqname":"action:upsert_index_datapoints#input_fields","loc":{"line":720,"column":20,"length":126,"begin":33939,"end":34065},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('get_behavior_input_fields', 'vector.upsert_datapoints', true)"}
{"id":"lam:fd11264cbb8fc80e","kind":"lambda","name":"execute","fqname":"action:upsert_index_datapoints#execute","loc":{"line":728,"column":15,"length":189,"begin":34293,"end":34482},"file":"connector.rb","text":" execute: lambda do |connection, input, _in_schema, _out_schema, _cfg|\n        safe = call('deep_copy', input)\n        call('execute_behavior', connection, 'vector.upsert_datapoints', safe)"}
{"id":"lam:7e8fd89ddb18bbe4","kind":"lambda","name":"output_fields","fqname":"action:upsert_index_datapoints#output_fields","loc":{"line":724,"column":21,"length":157,"begin":34103,"end":34260},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'vector.upsert_datapoints')"}
{"id":"lam:5474177ee2090b39","kind":"lambda","name":"input_fields","fqname":"action:generate_embeddings#input_fields","loc":{"line":752,"column":20,"length":205,"begin":35898,"end":36103},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, config_fields|\n        cfg = config_fields.is_a?(Hash) ? config_fields : {}\n        call('get_behavior_input_fields', 'text.embed', cfg['advanced_config'], cfg)"}
{"id":"lam:d82914cbf577e0db","kind":"lambda","name":"execute","fqname":"action:generate_embeddings#execute","loc":{"line":762,"column":15,"length":335,"begin":36401,"end":36736},"file":"connector.rb","text":" execute: lambda do |connection, input, _in_schema, _out_schema, config_fields|\n        user_cfg = call('extract_user_config', input, config_fields['advanced_config'], config_fields)\n        safe     = call('deep_copy', input) # one copy at the action boundary\n        call('execute_behavior', connection, 'text.embed', safe, user_cfg)"}
{"id":"lam:e5c8e0bc37b51e11","kind":"lambda","name":"output_fields","fqname":"action:generate_embeddings#output_fields","loc":{"line":758,"column":21,"length":143,"begin":36225,"end":36368},"file":"connector.rb","text":"t_fields: lambda do |_obj_defs, _connection, _cfg|\n        call('telemetry_envelope_fields') + call('get_behavior_output_fields', 'text.embed')"}
{"id":"lam:221fbe98b7b27825","kind":"lambda","name":"sample_output","fqname":"action:generate_embeddings#sample_output","loc":{"line":768,"column":21,"length":590,"begin":36778,"end":37368},"file":"connector.rb","text":"e_output: lambda do |_connection, _cfg|\n        {\n          \"success\"   => true,\n          \"timestamp\" => \"2025-01-01T00:00:00Z\",\n          \"metadata\"  => { \"operation\" => \"text.embed\", \"model\" => \"text-embedding-005@latest\" },\n          \"trace\"     => { \"correlation_id\" => \"abc123\", \"duration_ms\" => 21, \"attempt\" => 1 },\n          \"embeddings\"=> [ { \"values\" => [0.01, 0.02, 0.03] } ],\n          \"vectors\"   => [ { \"feature_vector\" => [0.01, 0.02, 0.03] } ],\n          \"count\"     => 1,\n          \"dimension\" => 768,\n          \"avg_norm\"  => 1.0,\n          \"norms\"     => [1.0]\n        }"}
